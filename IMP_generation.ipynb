{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5d1cd6",
   "metadata": {},
   "source": [
    "# IMP Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86572d37",
   "metadata": {},
   "source": [
    "Definition of the CFG in the Lark-style EBNF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d413057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f79df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_grammar = r\"\"\"\n",
    "    ########################\n",
    "    #  Top-level & program #\n",
    "    ########################\n",
    "    ?start: command                       -> program        # one or more commands\n",
    "\n",
    "    ################\n",
    "    #  Commands    #\n",
    "    ################\n",
    "    ?command: simple\n",
    "            | simple \";\" command           -> seq            # right-associative sequencing\n",
    "\n",
    "    ?simple: \"skip\"                        -> skip\n",
    "           | NAME \":=\" aexpr               -> assign\n",
    "           | \"if\" bexpr \"then\" command \"else\" command                -> if\n",
    "           | \"while\" bexpr \"do\" command    -> while\n",
    "           | \"(\" command \")\"                               # optional grouping\n",
    "\n",
    "    ########################\n",
    "    #  Arithmetic grammar  #\n",
    "    ########################\n",
    "    ?aexpr: term\n",
    "          | aexpr \"+\" term                 -> add\n",
    "          | aexpr \"-\" term                 -> sub\n",
    "\n",
    "    ?term: factor\n",
    "         | term \"*\" factor                 -> mul\n",
    "         | term \"/\" factor                 -> div\n",
    "\n",
    "    ?factor: NUMBER                        -> number\n",
    "           | NAME                          -> var\n",
    "           | \"-\" factor                    -> neg\n",
    "           | \"(\" aexpr \")\"                 -> aparen\n",
    "\n",
    "    #######################\n",
    "    #  Boolean grammar    #\n",
    "    #######################\n",
    "    ?bexpr: bterm\n",
    "          | bexpr \"||\" bterm               -> or\n",
    "\n",
    "    ?bterm: bfactor\n",
    "          | bterm \"&&\" bfactor             -> and\n",
    "\n",
    "    ?bfactor: \"true\"                       -> true\n",
    "            | \"false\"                      -> false\n",
    "            | \"!\" bfactor                  -> not\n",
    "            | aexpr relop aexpr            -> rel\n",
    "            | \"(\" bexpr \")\"                -> bparen\n",
    "\n",
    "    relop: \"==\" | \"!=\" | \"<\" | \"<=\" | \">\" | \">=\"\n",
    "\n",
    "    #################\n",
    "    #  Terminals    #\n",
    "    #################\n",
    "    NAME: /[A-Za-z_][A-Za-z0-9_]*/          # identifiers\n",
    "    %import common.NUMBER                   # ints from Lark’s standard library\n",
    "    %import common.WS_INLINE\n",
    "    %import common.WS\n",
    "    %ignore WS_INLINE                       # skip spaces and tabs\n",
    "    %ignore WS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d68b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_grammar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = outlines.models.transformers(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-0.5B\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m generator = outlines.generate.cfg(model, \u001b[43mimp_grammar\u001b[49m)\n\u001b[32m      3\u001b[39m sequence = generator(\u001b[33m\"\u001b[39m\u001b[33mIn the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:\u001b[39m\u001b[33m\"\u001b[39m, max_tokens=\u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(sequence)\n",
      "\u001b[31mNameError\u001b[39m: name 'imp_grammar' is not defined"
     ]
    }
   ],
   "source": [
    "model = outlines.models.transformers(\"Qwen/Qwen2.5-0.5B\")\n",
    "generator = outlines.generate.cfg(model, imp_grammar)\n",
    "sequence = generator(\"In the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:\", max_tokens=50)\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821c592",
   "metadata": {},
   "source": [
    "x:=10; y:=0; while x>0 do y:=y+2; x:=x-1; endwhile:= y; What"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a7abd",
   "metadata": {},
   "source": [
    "No error: endwhile is just a variable name here, we assign the value in y to endwhile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570359f4",
   "metadata": {},
   "source": [
    "# ConstraintLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969ebf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import constraintlm as clm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272e1a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "qwenllm = clm.TransformersLM(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5d3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"In the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:\"]\n",
    "batch = qwenllm.tokenizer(prompt, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc0fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = clm.CLMCFGLogitsProcessor(imp_grammar, model.tokenizer, qwenllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8555039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "['In the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:  \\t(x:= 10;y:=']\n"
     ]
    }
   ],
   "source": [
    "cfg_multinomial = clm.MultinomialSeqSampler(qwenllm, logits_processor=cfg)\n",
    "cons_generated_token_ids = cfg_multinomial.sample(batch.input_ids, max_length=10, top_k=5)\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, cons_generated_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaeeb7",
   "metadata": {},
   "source": [
    "# Python parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb0170",
   "metadata": {},
   "source": [
    "for := x; y:=y+2'\n",
    "\n",
    "This code is valid because we are not checking that variable are assigned, and for is not a keyword here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d2ae7",
   "metadata": {},
   "source": [
    "%import common.NEWLINE   -> _NL\n",
    "%ignore _NL\n",
    "%ignore /[ \\t]+/         // whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987efc5",
   "metadata": {},
   "source": [
    "_NEWLINE: ( /\\r?\\n[\\t ]*/ | COMMENT )+\n",
    "\n",
    "%ignore /[\\t \\f]+/  // WS\n",
    "%ignore /\\\\[\\t \\f]*\\r?\\n/   // LINE_CONT\n",
    "%ignore COMMENT\n",
    "%declare _INDENT _DEDENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be63d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_grammar = r\"\"\"\n",
    "\n",
    "start: file_input\n",
    "\n",
    "single_input: _NEWLINE | simple_stmt | compound_stmt _NEWLINE\n",
    "file_input: (_NEWLINE | stmt)*\n",
    "eval_input: testlist _NEWLINE*\n",
    "\n",
    "decorator: \"@\" dotted_name [ \"(\" [arguments] \")\" ] _NEWLINE\n",
    "decorators: decorator+\n",
    "decorated: decorators (classdef | funcdef | async_funcdef)\n",
    "\n",
    "async_funcdef: \"async\" funcdef\n",
    "funcdef: \"def\" name \"(\" [parameters] \")\" [\"->\" test] \":\" suite\n",
    "\n",
    "parameters: paramvalue (\",\" paramvalue)* [\",\" SLASH (\",\" paramvalue)*] [\",\" [starparams | kwparams]]\n",
    "          | starparams\n",
    "          | kwparams\n",
    "\n",
    "SLASH: \"/\" // Otherwise the it will completely disappear and it will be undisguisable in the result\n",
    "starparams: (starparam | starguard) poststarparams\n",
    "starparam: \"*\" typedparam\n",
    "starguard: \"*\"\n",
    "poststarparams: (\",\" paramvalue)* [\",\" kwparams]\n",
    "kwparams: \"**\" typedparam \",\"?\n",
    "\n",
    "?paramvalue: typedparam (\"=\" test)?\n",
    "?typedparam: name (\":\" test)?\n",
    "\n",
    "\n",
    "lambdef: \"lambda\" [lambda_params] \":\" test\n",
    "lambdef_nocond: \"lambda\" [lambda_params] \":\" test_nocond\n",
    "lambda_params: lambda_paramvalue (\",\" lambda_paramvalue)* [\",\" [lambda_starparams | lambda_kwparams]]\n",
    "          | lambda_starparams\n",
    "          | lambda_kwparams\n",
    "?lambda_paramvalue: name (\"=\" test)?\n",
    "lambda_starparams: \"*\" [name]  (\",\" lambda_paramvalue)* [\",\" [lambda_kwparams]]\n",
    "lambda_kwparams: \"**\" name \",\"?\n",
    "\n",
    "\n",
    "?stmt: simple_stmt | compound_stmt\n",
    "?simple_stmt: small_stmt (\";\" small_stmt)* [\";\"] _NEWLINE\n",
    "?small_stmt: (expr_stmt | assign_stmt | del_stmt | pass_stmt | flow_stmt | import_stmt | global_stmt | nonlocal_stmt | assert_stmt)\n",
    "expr_stmt: testlist_star_expr\n",
    "assign_stmt: annassign | augassign | assign\n",
    "\n",
    "annassign: testlist_star_expr \":\" test [\"=\" test]\n",
    "assign: testlist_star_expr (\"=\" (yield_expr|testlist_star_expr))+\n",
    "augassign: testlist_star_expr augassign_op (yield_expr|testlist)\n",
    "!augassign_op: \"+=\" | \"-=\" | \"*=\" | \"@=\" | \"/=\" | \"%=\" | \"&=\" | \"|=\" | \"^=\" | \"<<=\" | \">>=\" | \"**=\" | \"//=\"\n",
    "?testlist_star_expr: test_or_star_expr\n",
    "                   | test_or_star_expr (\",\" test_or_star_expr)+ \",\"?  -> tuple\n",
    "                   | test_or_star_expr \",\"  -> tuple\n",
    "\n",
    "del_stmt: \"del\" exprlist\n",
    "pass_stmt: \"pass\"\n",
    "?flow_stmt: break_stmt | continue_stmt | return_stmt | raise_stmt | yield_stmt\n",
    "break_stmt: \"break\"\n",
    "continue_stmt: \"continue\"\n",
    "return_stmt: \"return\" [testlist]\n",
    "yield_stmt: yield_expr\n",
    "raise_stmt: \"raise\" [test [\"from\" test]]\n",
    "import_stmt: import_name | import_from\n",
    "import_name: \"import\" dotted_as_names\n",
    "import_from: \"from\" (dots? dotted_name | dots) \"import\" (\"*\" | \"(\" import_as_names \")\" | import_as_names)\n",
    "!dots: \".\"+\n",
    "import_as_name: name [\"as\" name]\n",
    "dotted_as_name: dotted_name [\"as\" name]\n",
    "import_as_names: import_as_name (\",\" import_as_name)* [\",\"]\n",
    "dotted_as_names: dotted_as_name (\",\" dotted_as_name)*\n",
    "dotted_name: name (\".\" name)*\n",
    "global_stmt: \"global\" name (\",\" name)*\n",
    "nonlocal_stmt: \"nonlocal\" name (\",\" name)*\n",
    "assert_stmt: \"assert\" test [\",\" test]\n",
    "\n",
    "?compound_stmt: if_stmt | while_stmt | for_stmt | try_stmt | match_stmt\n",
    "              | with_stmt | funcdef | classdef | decorated | async_stmt\n",
    "async_stmt: \"async\" (funcdef | with_stmt | for_stmt)\n",
    "if_stmt: \"if\" test \":\" suite elifs [\"else\" \":\" suite]\n",
    "elifs: elif_*\n",
    "elif_: \"elif\" test \":\" suite\n",
    "while_stmt: \"while\" test \":\" suite [\"else\" \":\" suite]\n",
    "for_stmt: \"for\" exprlist \"in\" testlist \":\" suite [\"else\" \":\" suite]\n",
    "try_stmt: \"try\" \":\" suite except_clauses [\"else\" \":\" suite] [finally]\n",
    "        | \"try\" \":\" suite finally   -> try_finally\n",
    "finally: \"finally\" \":\" suite\n",
    "except_clauses: except_clause+\n",
    "except_clause: \"except\" [test [\"as\" name]] \":\" suite\n",
    "// NB compile.c makes sure that the default except clause is last\n",
    "\n",
    "\n",
    "with_stmt: \"with\" with_items \":\" suite\n",
    "with_items: with_item (\",\" with_item)*\n",
    "with_item: test [\"as\" name]\n",
    "\n",
    "match_stmt: \"match\" test \":\" _NEWLINE _INDENT case+ _DEDENT\n",
    "\n",
    "case: \"case\" pattern [\"if\" test] \":\" suite\n",
    "\n",
    "?pattern: sequence_item_pattern \",\" _sequence_pattern -> sequence_pattern\n",
    "        | as_pattern\n",
    "?as_pattern: or_pattern (\"as\" NAME)?\n",
    "?or_pattern: closed_pattern (\"|\" closed_pattern)*\n",
    "?closed_pattern: literal_pattern\n",
    "               | NAME -> capture_pattern\n",
    "               | \"_\" -> any_pattern\n",
    "               | attr_pattern\n",
    "               | \"(\" as_pattern \")\"\n",
    "               | \"[\" _sequence_pattern \"]\" -> sequence_pattern\n",
    "               | \"(\" (sequence_item_pattern \",\" _sequence_pattern)? \")\" -> sequence_pattern\n",
    "               | \"{\" (mapping_item_pattern (\",\" mapping_item_pattern)* \",\"?)?\"}\" -> mapping_pattern\n",
    "               | \"{\" (mapping_item_pattern (\",\" mapping_item_pattern)* \",\")? \"**\" NAME \",\"? \"}\" -> mapping_star_pattern\n",
    "               | class_pattern\n",
    "\n",
    "literal_pattern: inner_literal_pattern\n",
    "\n",
    "?inner_literal_pattern: \"None\" -> const_none\n",
    "                      | \"True\" -> const_true\n",
    "                      | \"False\" -> const_false\n",
    "                      | STRING -> string\n",
    "                      | number\n",
    "\n",
    "attr_pattern: NAME (\".\" NAME)+ -> value\n",
    "\n",
    "name_or_attr_pattern: NAME (\".\" NAME)* -> value\n",
    "\n",
    "mapping_item_pattern: (literal_pattern|attr_pattern) \":\" as_pattern\n",
    "\n",
    "_sequence_pattern: (sequence_item_pattern (\",\" sequence_item_pattern)* \",\"?)?\n",
    "?sequence_item_pattern: as_pattern\n",
    "                      | \"*\" NAME -> star_pattern\n",
    "\n",
    "class_pattern: name_or_attr_pattern \"(\" [arguments_pattern \",\"?] \")\"\n",
    "arguments_pattern: pos_arg_pattern [\",\" keyws_arg_pattern]\n",
    "                 | keyws_arg_pattern -> no_pos_arguments\n",
    "\n",
    "pos_arg_pattern: as_pattern (\",\" as_pattern)*\n",
    "keyws_arg_pattern: keyw_arg_pattern (\",\" keyw_arg_pattern)*\n",
    "keyw_arg_pattern: NAME \"=\" as_pattern\n",
    "\n",
    "\n",
    "\n",
    "suite: simple_stmt | _NEWLINE _INDENT stmt+ _DEDENT\n",
    "\n",
    "?test: or_test (\"if\" or_test \"else\" test)?\n",
    "     | lambdef\n",
    "     | assign_expr\n",
    "\n",
    "assign_expr: name \":=\" test\n",
    "\n",
    "?test_nocond: or_test | lambdef_nocond\n",
    "\n",
    "?or_test: and_test (\"or\" and_test)*\n",
    "?and_test: not_test_ (\"and\" not_test_)*\n",
    "?not_test_: \"not\" not_test_ -> not_test\n",
    "         | comparison\n",
    "?comparison: expr (comp_op expr)*\n",
    "star_expr: \"*\" expr\n",
    "\n",
    "?expr: or_expr\n",
    "?or_expr: xor_expr (\"|\" xor_expr)*\n",
    "?xor_expr: and_expr (\"^\" and_expr)*\n",
    "?and_expr: shift_expr (\"&\" shift_expr)*\n",
    "?shift_expr: arith_expr (_shift_op arith_expr)*\n",
    "?arith_expr: term (_add_op term)*\n",
    "?term: factor (_mul_op factor)*\n",
    "?factor: _unary_op factor | power\n",
    "\n",
    "!_unary_op: \"+\"|\"-\"|\"~\"\n",
    "!_add_op: \"+\"|\"-\"\n",
    "!_shift_op: \"<<\"|\">>\"\n",
    "!_mul_op: \"*\"|\"@\"|\"/\"|\"%\"|\"//\"\n",
    "!comp_op: \"<\"|\">\"|\"==\"|\">=\"|\"<=\"|\"<>\"|\"!=\"|\"in\"|\"not\" \"in\"|\"is\"|\"is\" \"not\"\n",
    "\n",
    "?power: await_expr (\"**\" factor)?\n",
    "?await_expr: AWAIT? atom_expr\n",
    "AWAIT: \"await\"\n",
    "\n",
    "?atom_expr: atom_expr \"(\" [arguments] \")\"      -> funccall\n",
    "          | atom_expr \"[\" subscriptlist \"]\"  -> getitem\n",
    "          | atom_expr \".\" name               -> getattr\n",
    "          | atom\n",
    "\n",
    "?atom: \"(\" yield_expr \")\"\n",
    "     | \"(\" _tuple_inner? \")\" -> tuple\n",
    "     | \"(\" comprehension{test_or_star_expr} \")\" -> tuple_comprehension\n",
    "     | \"[\" _exprlist? \"]\"  -> list\n",
    "     | \"[\" comprehension{test_or_star_expr} \"]\"  -> list_comprehension\n",
    "     | \"{\" _dict_exprlist? \"}\" -> dict\n",
    "     | \"{\" comprehension{key_value} \"}\" -> dict_comprehension\n",
    "     | \"{\" _exprlist \"}\" -> set\n",
    "     | \"{\" comprehension{test} \"}\" -> set_comprehension\n",
    "     | name -> var\n",
    "     | number\n",
    "     | string_concat\n",
    "     | \"(\" test \")\"\n",
    "     | \"...\" -> ellipsis\n",
    "     | \"None\"    -> const_none\n",
    "     | \"True\"    -> const_true\n",
    "     | \"False\"   -> const_false\n",
    "\n",
    "\n",
    "?string_concat: string+\n",
    "\n",
    "_tuple_inner: test_or_star_expr ((\",\" test_or_star_expr)+ [\",\"] | \",\")\n",
    "\n",
    "?test_or_star_expr: test\n",
    "                 | star_expr\n",
    "\n",
    "?subscriptlist: subscript\n",
    "              | subscript ((\",\" subscript)+ [\",\"] | \",\") -> subscript_tuple\n",
    "?subscript: test | ([test] \":\" [test] [sliceop]) -> slice\n",
    "sliceop: \":\" [test]\n",
    "?exprlist: (expr|star_expr)\n",
    "         | (expr|star_expr) ((\",\" (expr|star_expr))+ [\",\"]|\",\")\n",
    "?testlist: test | testlist_tuple\n",
    "testlist_tuple: test ((\",\" test)+ [\",\"] | \",\")\n",
    "_dict_exprlist: (key_value | \"**\" expr) (\",\" (key_value | \"**\" expr))* [\",\"]\n",
    "\n",
    "key_value: test \":\"  test\n",
    "\n",
    "_exprlist: test_or_star_expr (\",\"  test_or_star_expr)* [\",\"]\n",
    "\n",
    "classdef: \"class\" name [\"(\" [arguments] \")\"] \":\" suite\n",
    "\n",
    "\n",
    "\n",
    "arguments: argvalue (\",\" argvalue)*  (\",\" [ starargs | kwargs])?\n",
    "         | starargs\n",
    "         | kwargs\n",
    "         | comprehension{test}\n",
    "\n",
    "starargs: stararg (\",\" stararg)* (\",\" argvalue)* [\",\" kwargs]\n",
    "stararg: \"*\" test\n",
    "kwargs: \"**\" test (\",\" argvalue)*\n",
    "\n",
    "?argvalue: test (\"=\" test)?\n",
    "\n",
    "\n",
    "comprehension{comp_result}: comp_result comp_fors [comp_if]\n",
    "comp_fors: comp_for+\n",
    "comp_for: [ASYNC] \"for\" exprlist \"in\" or_test\n",
    "ASYNC: \"async\"\n",
    "?comp_if: \"if\" test_nocond\n",
    "\n",
    "encoding_decl: name\n",
    "\n",
    "yield_expr: \"yield\" [testlist]\n",
    "          | \"yield\" \"from\" test -> yield_from\n",
    "\n",
    "number: DEC_NUMBER | HEX_NUMBER | BIN_NUMBER | OCT_NUMBER | FLOAT_NUMBER | IMAG_NUMBER\n",
    "string: STRING \n",
    "\n",
    "\n",
    "_NEWLINE: ( /\\r?\\n[\\t ]*/ | COMMENT )+\n",
    "\n",
    "%import common.WS_INLINE\n",
    "%ignore WS_INLINE\n",
    "%ignore /\\\\[\\t \\f]*\\r?\\n/   // LINE_CONT\n",
    "%ignore COMMENT\n",
    "%declare _INDENT _DEDENT\n",
    "\n",
    "\n",
    "\n",
    "!name: NAME | \"match\" | \"case\"\n",
    "NAME: /[^\\W\\d]\\w*/\n",
    "COMMENT: /\\#[^\\n]*/\n",
    "\n",
    "// 1) Prefix and escape definitions (still tokens)\n",
    "PREFIX       : /[uUbBfF]?[rR]?/ | /[rR][uUbBfF]/\n",
    "ESCAPED_CHAR : /\\\\./\n",
    "\n",
    "// 2) Short (single-line) strings as tokens\n",
    "STRING_DOUBLE: \"\\\"\" ( ESCAPED_CHAR | /[^\"\\\\]/ )* \"\\\"\"\n",
    "STRING_SINGLE: \"'\"  ( ESCAPED_CHAR | /[^'\\\\]/ )* \"'\"\n",
    "\n",
    "// 3) Expose a parser-rule to combine them\n",
    "STRING: PREFIX? (STRING_DOUBLE | STRING_SINGLE)\n",
    "\n",
    "// 4) Triple-quote markers as tokens\n",
    "TRIPLE_DQ : \"\\\"\\\"\\\"\"\n",
    "TRIPLE_SQ : \"'''\"\n",
    "\n",
    "// 5) The “content” parts as parser-rules\n",
    "long_double_content : ( ESCAPED_CHAR | \"\\\"\\\"\" | /[^\"\\\\]/ )*\n",
    "long_single_content : ( ESCAPED_CHAR | \"''\" | /[^'\\\\]/ )*\n",
    "\n",
    "// 6) The long-string parser-rule\n",
    "long_string : PREFIX? ( TRIPLE_DQ long_double_content TRIPLE_DQ\n",
    "            | TRIPLE_SQ long_single_content TRIPLE_SQ )\n",
    "\n",
    "_SPECIAL_DEC: \"0\"..\"9\"        (\"_\"?  \"0\"..\"9\"                       )*\n",
    "BAD_DEC_NUMBER: \"0\" ( \"_\" | \"0\" )* \"1\"..\"9\" ( \"_\"? \"0\"..\"9\" )*\n",
    "DEC_NUMBER: \"0\" ( \"_\"? \"0\" )*\n",
    "          | \"1\"..\"9\" ( \"_\"? \"0\"..\"9\" )* \n",
    "HEX_NUMBER.2: \"0\" (\"x\" | \"X\") (\"_\"? (\"0\"..\"9\" | \"a\"..\"f\" | \"A\"..\"F\"))+\n",
    "OCT_NUMBER.2: \"0\" (\"o\" | \"O\") (\"_\"?  \"0\"..\"7\"                       )+\n",
    "BIN_NUMBER.2: \"0\" (\"b\" | \"B\") (\"_\"?  \"0\"..\"1\"                       )+\n",
    "\n",
    "_EXP: (\"e\"|\"E\") [\"+\" | \"-\"] _SPECIAL_DEC\n",
    "DECIMAL: \".\" _SPECIAL_DEC | _SPECIAL_DEC \".\" _SPECIAL_DEC?\n",
    "FLOAT_NUMBER.2: _SPECIAL_DEC _EXP | DECIMAL _EXP?\n",
    "IMAG_NUMBER.2: (_SPECIAL_DEC      | FLOAT_NUMBER) (\"J\" | \"j\")\n",
    "\n",
    "// Comma-separated list (with an optional trailing comma)\n",
    "cs_list{item}: item (\",\" item)* \",\"?\n",
    "_cs_list{item}: item (\",\" item)* \",\"?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4041c4c",
   "metadata": {},
   "source": [
    "LONGSTRING, DEC_NUMBER, STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe6a03",
   "metadata": {},
   "source": [
    "- SFT including masking (it means that LCD is needed at inference-time)\n",
    "- SFT w/ \"distillation\" (binary cross entropy: masked probs / unmasked probs) (We get rid of LCD at inference-time)\n",
    "\n",
    "Model : LLama 2 7B (worst model, wasn't trained on code) ; Mistral 7B (better) ; Qwen2.5/3-7B (best)\n",
    "\n",
    "Dataset: OpenCodeInstruct (it is Python only, validated by automated unit testing and LLM-based quality judgement)\n",
    "\n",
    "Benchmark: HumanEval, MBPP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c5948",
   "metadata": {},
   "source": [
    "- SMC for RL: we generate sentences using SMC (or just LCD?), and we apply RL thanks to those sentences. It might be a big problem because bad sentences are integrated into the loss, thus when we have a bad example we are moving far from those examples.\n",
    "\n",
    "see: random down-sampling, max-reward down-sampling and max-variance down-sampling in \"Not All Rollouts are Useful: Down-Sampling Rollouts\n",
    "in LLM Reinforcement Learning\"\n",
    "\n",
    "\n",
    "The number of uncorrect sentences is much greater than the number of correct ones. Including uncorrect ones is useful as it provies information about what not to do. However, the information provided by a correct response is much more important. \n",
    "The question is, by sampling trajectories from another policy (the one defined by LCD or by SMC), can we improve the upgrading of $\\theta$? To answer this, I first need to understand TRPO, PPO and GRPO mathematically (from where are the formulaes derived, the estimator of what are we computing, etc. ). I hope that by perfectly understanding this, I will be able to demonstrate that sampling trajectories from the constrained distributions can improve the estimator by diminushing the variance of the estimator?..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04806e",
   "metadata": {},
   "source": [
    "According to \"What Makes a Reward Model a Good Teacher? An Optimization Perspective\", the lower the variance of the reward $Var_{y \\sim \\pi_{\\theta}(.|x)}(r_{RM}(x,y))$ is, the faster the maximization of the expectation $E_{x \\sim \\mathcal{S}}[E_{y \\sim \\pi_{\\theta}(.|x)}[r_{RM}(x,y)]]$ can theoretically be. \n",
    "\n",
    "To maximize this quantity, TRPO, PPO, GRPO use an algorithm. Maybe that by sampling from a constrained distribution (locally or globally), we can improve the speed at whoch the quantity is being optimized. Or by replacing $\\pi_{old}$ by $\\pi_{old}^{constrained}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea09c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import outlines\n",
    "import constraintlm as clm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de0cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "from lark.indenter import PythonIndenter\n",
    "\n",
    "kwargs = dict(postlex=PythonIndenter(), start='file_input')\n",
    "\n",
    "\n",
    "python_parser = Lark(python_grammar, parser='lalr', lexer='contextual', postlex=PythonIndenter())\n",
    "python_parser3 = Lark.open_from_package('lark', 'python.lark', ['grammars'], parser='lalr', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e4fc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('DEF', 'def')\n",
      "Token('NAME', 'f')\n",
      "Token('LPAR', '(')\n",
      "Token('NAME', 'x')\n",
      "Token('RPAR', ')')\n",
      "Token('COLON', ':')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('_INDENT', '    ')\n",
      "Token('NAME', 'x')\n",
      "Token('EQUAL', '=')\n",
      "Token('DEC_NUMBER', '3')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('RETURN', 'return')\n",
      "Token('NAME', 'x')\n",
      "Token('_NEWLINE', '\\n')\n",
      "Token('_DEDENT', '')\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"def f(x):\n",
    "    x = 3\n",
    "    return x\n",
    "\"\"\"\n",
    "for tok in python_parser.lex(code):\n",
    "    print(repr(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36b51d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree(Token('RULE', 'start'), [Tree(Token('RULE', 'file_input'), [Tree(Token('RULE', 'funcdef'), [Tree(Token('RULE', 'name'), [Token('NAME', 'f')]), Tree(Token('RULE', 'parameters'), [Tree(Token('RULE', 'name'), [Token('NAME', 'x')]), None, None]), None, Tree(Token('RULE', 'suite'), [Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '3')])])]), Tree(Token('RULE', 'return_stmt'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])])])])])])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = \"\"\"def f(x):\n",
    "    x = 3\n",
    "    return x\n",
    "\"\"\"\n",
    "\n",
    "python_parser.parse(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854e8c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree(Token('RULE', 'start'), [Tree(Token('RULE', 'file_input'), [Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '10')])])]), Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'y')])]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '0')])])]), Tree(Token('RULE', 'while_stmt'), [Tree(Token('RULE', 'comparison'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'comp_op'), [Token('MORETHAN', '>')]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '0')])]), Tree(Token('RULE', 'suite'), [Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'y')])]), Tree(Token('RULE', 'arith_expr'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'y')])]), Token('PLUS', '+'), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '2')])])])]), Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'arith_expr'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Token('MINUS', '-'), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '1')])])])])]), None])])])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_test = \"\"\"x=10\\\n",
    "\\ny=0 \\nwhile x>0: \\n    y = y + 2 \\n    x = x - 1 \\n\"\"\"\n",
    "\n",
    "python_parser.parse(code_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e70450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model = outlines.models.transformers(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4aa2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\outlines\\fsm\\guide.py:110: UserWarning: Outlines' public *community-contributed* CFG structured generation is experimental. Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, y=0, while (x>0) and (y==(y+2)) and (x==(x-1\n"
     ]
    }
   ],
   "source": [
    "generator = outlines.generate.cfg(model, python_grammar)\n",
    "sequence_p = generator(\"In the programming language Python, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in Python:\\nx=\", max_tokens=30)\n",
    "\n",
    "print(sequence_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44246eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def max_non_overlapping_tasks(tasks):\n",
      "    # Sort the tasks by their start times\n",
      "    # Use a heap to keep track of the tasks that can be selected\n",
      "    # The heap will store the tasks that are currently selected\n",
      "    # The heap will be sorted by their end times\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The\n"
     ]
    }
   ],
   "source": [
    "sequence_python = generator(\"You are given a list of `n` tasks, each represented as a tuple `(start, end)`, indicating the start and end times of the task. The tasks are sorted by their start times. Your goal is to determine the maximum number of non-overlapping tasks that can be selected. Two tasks are considered non-overlapping if the start time of one task is greater than or equal to the end time of the other. \\\n",
    "\\\n",
    "**Input:**\\\n",
    "- An integer `n` representing the number of tasks.\\\n",
    "- A list of `n` tuples, where each tuple `(start, end)` represents the start and end times of a task.\\\n",
    "\\\n",
    "**Output:**\\\n",
    "- An integer representing the maximum number of non-overlapping tasks that can be selected.\\\n",
    "\\\n",
    "**Constraints:**\\\n",
    "- `1 <= n <= 10^5`\\\n",
    "- `0 <= start < end <= 10^9`\\\n",
    "\\\n",
    "**Sample Input:**\\\n",
    "```\\\n",
    "3\\\n",
    "1 3\\\n",
    "2 5\\\n",
    "4 6\\\n",
    "```\\\n",
    "\\\n",
    "**Sample Output:**\\\n",
    "```\\\n",
    "2\\\n",
    "```\\\n",
    "```python \\n\", max_tokens=300)\n",
    "print(sequence_python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
