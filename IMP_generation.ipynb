{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5d1cd6",
   "metadata": {},
   "source": [
    "# IMP Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86572d37",
   "metadata": {},
   "source": [
    "Definition of the CFG in the Lark-style EBNF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d413057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f79df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_grammar = r\"\"\"\n",
    "    ########################\n",
    "    #  Top-level & program #\n",
    "    ########################\n",
    "    ?start: command                       -> program        # one or more commands\n",
    "\n",
    "    ################\n",
    "    #  Commands    #\n",
    "    ################\n",
    "    ?command: simple\n",
    "            | simple \";\" command           -> seq            # right-associative sequencing\n",
    "\n",
    "    ?simple: \"skip\"                        -> skip\n",
    "           | NAME \":=\" aexpr               -> assign\n",
    "           | \"if\" bexpr \"then\" command \"else\" command                -> if\n",
    "           | \"while\" bexpr \"do\" command    -> while\n",
    "           | \"(\" command \")\"                               # optional grouping\n",
    "\n",
    "    ########################\n",
    "    #  Arithmetic grammar  #\n",
    "    ########################\n",
    "    ?aexpr: term\n",
    "          | aexpr \"+\" term                 -> add\n",
    "          | aexpr \"-\" term                 -> sub\n",
    "\n",
    "    ?term: factor\n",
    "         | term \"*\" factor                 -> mul\n",
    "         | term \"/\" factor                 -> div\n",
    "\n",
    "    ?factor: NUMBER                        -> number\n",
    "           | NAME                          -> var\n",
    "           | \"-\" factor                    -> neg\n",
    "           | \"(\" aexpr \")\"                 -> aparen\n",
    "\n",
    "    #######################\n",
    "    #  Boolean grammar    #\n",
    "    #######################\n",
    "    ?bexpr: bterm\n",
    "          | bexpr \"||\" bterm               -> or\n",
    "\n",
    "    ?bterm: bfactor\n",
    "          | bterm \"&&\" bfactor             -> and\n",
    "\n",
    "    ?bfactor: \"true\"                       -> true\n",
    "            | \"false\"                      -> false\n",
    "            | \"!\" bfactor                  -> not\n",
    "            | aexpr relop aexpr            -> rel\n",
    "            | \"(\" bexpr \")\"                -> bparen\n",
    "\n",
    "    relop: \"==\" | \"!=\" | \"<\" | \"<=\" | \">\" | \">=\"\n",
    "\n",
    "    #################\n",
    "    #  Terminals    #\n",
    "    #################\n",
    "    NAME: /[A-Za-z_][A-Za-z0-9_]*/          # identifiers\n",
    "    %import common.NUMBER                   # ints from Lark’s standard library\n",
    "    %import common.WS_INLINE\n",
    "    %import common.WS\n",
    "    %ignore WS_INLINE                       # skip spaces and tabs\n",
    "    %ignore WS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d68b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_grammar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = outlines.models.transformers(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-0.5B\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m generator = outlines.generate.cfg(model, \u001b[43mimp_grammar\u001b[49m)\n\u001b[32m      3\u001b[39m sequence = generator(\u001b[33m\"\u001b[39m\u001b[33mIn the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:\u001b[39m\u001b[33m\"\u001b[39m, max_tokens=\u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(sequence)\n",
      "\u001b[31mNameError\u001b[39m: name 'imp_grammar' is not defined"
     ]
    }
   ],
   "source": [
    "model = outlines.models.transformers(\"Qwen/Qwen2.5-0.5B\")\n",
    "generator = outlines.generate.cfg(model, imp_grammar)\n",
    "sequence = generator(\"In the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:\", max_tokens=50)\n",
    "\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821c592",
   "metadata": {},
   "source": [
    "x:=10; y:=0; while x>0 do y:=y+2; x:=x-1; endwhile:= y; What"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a7abd",
   "metadata": {},
   "source": [
    "No error: endwhile is just a variable name here, we assign the value in y to endwhile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570359f4",
   "metadata": {},
   "source": [
    "# ConstraintLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969ebf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import constraintlm as clm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272e1a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "qwenllm = clm.TransformersLM(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e5d3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"In the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:\"]\n",
    "batch = qwenllm.tokenizer(prompt, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc0fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = clm.CLMCFGLogitsProcessor(imp_grammar, model.tokenizer, qwenllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8555039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "['In the programming language IMP, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in IMP:  \\t(x:= 10;y:=']\n"
     ]
    }
   ],
   "source": [
    "cfg_multinomial = clm.MultinomialSeqSampler(qwenllm, logits_processor=cfg)\n",
    "cons_generated_token_ids = cfg_multinomial.sample(batch.input_ids, max_length=10, top_k=5)\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, cons_generated_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaeeb7",
   "metadata": {},
   "source": [
    "# Python parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb47b4",
   "metadata": {},
   "source": [
    "## First try with a custom grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb0170",
   "metadata": {},
   "source": [
    "for := x; y:=y+2'\n",
    "\n",
    "This code is valid because we are not checking that variable are assigned, and for is not a keyword here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d2ae7",
   "metadata": {},
   "source": [
    "%import common.NEWLINE   -> _NL\n",
    "%ignore _NL\n",
    "%ignore /[ \\t]+/         // whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987efc5",
   "metadata": {},
   "source": [
    "_NEWLINE: ( /\\r?\\n[\\t ]*/ | COMMENT )+\n",
    "\n",
    "%ignore /[\\t \\f]+/  // WS\n",
    "%ignore /\\\\[\\t \\f]*\\r?\\n/   // LINE_CONT\n",
    "%ignore COMMENT\n",
    "%declare _INDENT _DEDENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9be63d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_grammar = r\"\"\"\n",
    "\n",
    "start: file_input\n",
    "\n",
    "single_input: _NEWLINE | simple_stmt | compound_stmt _NEWLINE\n",
    "file_input: (_NEWLINE | stmt)*\n",
    "eval_input: testlist _NEWLINE*\n",
    "\n",
    "decorator: \"@\" dotted_name [ \"(\" [arguments] \")\" ] _NEWLINE\n",
    "decorators: decorator+\n",
    "decorated: decorators (classdef | funcdef | async_funcdef)\n",
    "\n",
    "async_funcdef: \"async\" funcdef\n",
    "funcdef: \"def\" name \"(\" [parameters] \")\" [\"->\" test] \":\" suite\n",
    "\n",
    "parameters: paramvalue (\",\" paramvalue)* [\",\" SLASH (\",\" paramvalue)*] [\",\" [starparams | kwparams]]\n",
    "          | starparams\n",
    "          | kwparams\n",
    "\n",
    "SLASH: \"/\" // Otherwise the it will completely disappear and it will be undisguisable in the result\n",
    "starparams: (starparam | starguard) poststarparams\n",
    "starparam: \"*\" typedparam\n",
    "starguard: \"*\"\n",
    "poststarparams: (\",\" paramvalue)* [\",\" kwparams]\n",
    "kwparams: \"**\" typedparam \",\"?\n",
    "\n",
    "?paramvalue: typedparam (\"=\" test)?\n",
    "?typedparam: name (\":\" test)?\n",
    "\n",
    "\n",
    "lambdef: \"lambda\" [lambda_params] \":\" test\n",
    "lambdef_nocond: \"lambda\" [lambda_params] \":\" test_nocond\n",
    "lambda_params: lambda_paramvalue (\",\" lambda_paramvalue)* [\",\" [lambda_starparams | lambda_kwparams]]\n",
    "          | lambda_starparams\n",
    "          | lambda_kwparams\n",
    "?lambda_paramvalue: name (\"=\" test)?\n",
    "lambda_starparams: \"*\" [name]  (\",\" lambda_paramvalue)* [\",\" [lambda_kwparams]]\n",
    "lambda_kwparams: \"**\" name \",\"?\n",
    "\n",
    "\n",
    "?stmt: simple_stmt | compound_stmt\n",
    "?simple_stmt: small_stmt (\";\" small_stmt)* [\";\"] _NEWLINE\n",
    "?small_stmt: (expr_stmt | assign_stmt | del_stmt | pass_stmt | flow_stmt | import_stmt | global_stmt | nonlocal_stmt | assert_stmt)\n",
    "expr_stmt: testlist_star_expr\n",
    "assign_stmt: annassign | augassign | assign\n",
    "\n",
    "annassign: testlist_star_expr \":\" test [\"=\" test]\n",
    "assign: testlist_star_expr (\"=\" (yield_expr|testlist_star_expr))+\n",
    "augassign: testlist_star_expr augassign_op (yield_expr|testlist)\n",
    "!augassign_op: \"+=\" | \"-=\" | \"*=\" | \"@=\" | \"/=\" | \"%=\" | \"&=\" | \"|=\" | \"^=\" | \"<<=\" | \">>=\" | \"**=\" | \"//=\"\n",
    "?testlist_star_expr: test_or_star_expr\n",
    "                   | test_or_star_expr (\",\" test_or_star_expr)+ \",\"?  -> tuple\n",
    "                   | test_or_star_expr \",\"  -> tuple\n",
    "\n",
    "del_stmt: \"del\" exprlist\n",
    "pass_stmt: \"pass\"\n",
    "?flow_stmt: break_stmt | continue_stmt | return_stmt | raise_stmt | yield_stmt\n",
    "break_stmt: \"break\"\n",
    "continue_stmt: \"continue\"\n",
    "return_stmt: \"return\" [testlist]\n",
    "yield_stmt: yield_expr\n",
    "raise_stmt: \"raise\" [test [\"from\" test]]\n",
    "import_stmt: import_name | import_from\n",
    "import_name: \"import\" dotted_as_names\n",
    "import_from: \"from\" (dots? dotted_name | dots) \"import\" (\"*\" | \"(\" import_as_names \")\" | import_as_names)\n",
    "!dots: \".\"+\n",
    "import_as_name: name [\"as\" name]\n",
    "dotted_as_name: dotted_name [\"as\" name]\n",
    "import_as_names: import_as_name (\",\" import_as_name)* [\",\"]\n",
    "dotted_as_names: dotted_as_name (\",\" dotted_as_name)*\n",
    "dotted_name: name (\".\" name)*\n",
    "global_stmt: \"global\" name (\",\" name)*\n",
    "nonlocal_stmt: \"nonlocal\" name (\",\" name)*\n",
    "assert_stmt: \"assert\" test [\",\" test]\n",
    "\n",
    "?compound_stmt: if_stmt | while_stmt | for_stmt | try_stmt | match_stmt\n",
    "              | with_stmt | funcdef | classdef | decorated | async_stmt\n",
    "async_stmt: \"async\" (funcdef | with_stmt | for_stmt)\n",
    "if_stmt: \"if\" test \":\" suite elifs [\"else\" \":\" suite]\n",
    "elifs: elif_*\n",
    "elif_: \"elif\" test \":\" suite\n",
    "while_stmt: \"while\" test \":\" suite [\"else\" \":\" suite]\n",
    "for_stmt: \"for\" exprlist \"in\" testlist \":\" suite [\"else\" \":\" suite]\n",
    "try_stmt: \"try\" \":\" suite except_clauses [\"else\" \":\" suite] [finally]\n",
    "        | \"try\" \":\" suite finally   -> try_finally\n",
    "finally: \"finally\" \":\" suite\n",
    "except_clauses: except_clause+\n",
    "except_clause: \"except\" [test [\"as\" name]] \":\" suite\n",
    "// NB compile.c makes sure that the default except clause is last\n",
    "\n",
    "\n",
    "with_stmt: \"with\" with_items \":\" suite\n",
    "with_items: with_item (\",\" with_item)*\n",
    "with_item: test [\"as\" name]\n",
    "\n",
    "match_stmt: \"match\" test \":\" _NEWLINE _INDENT case+ _DEDENT\n",
    "\n",
    "case: \"case\" pattern [\"if\" test] \":\" suite\n",
    "\n",
    "?pattern: sequence_item_pattern \",\" _sequence_pattern -> sequence_pattern\n",
    "        | as_pattern\n",
    "?as_pattern: or_pattern (\"as\" NAME)?\n",
    "?or_pattern: closed_pattern (\"|\" closed_pattern)*\n",
    "?closed_pattern: literal_pattern\n",
    "               | NAME -> capture_pattern\n",
    "               | \"_\" -> any_pattern\n",
    "               | attr_pattern\n",
    "               | \"(\" as_pattern \")\"\n",
    "               | \"[\" _sequence_pattern \"]\" -> sequence_pattern\n",
    "               | \"(\" (sequence_item_pattern \",\" _sequence_pattern)? \")\" -> sequence_pattern\n",
    "               | \"{\" (mapping_item_pattern (\",\" mapping_item_pattern)* \",\"?)?\"}\" -> mapping_pattern\n",
    "               | \"{\" (mapping_item_pattern (\",\" mapping_item_pattern)* \",\")? \"**\" NAME \",\"? \"}\" -> mapping_star_pattern\n",
    "               | class_pattern\n",
    "\n",
    "literal_pattern: inner_literal_pattern\n",
    "\n",
    "?inner_literal_pattern: \"None\" -> const_none\n",
    "                      | \"True\" -> const_true\n",
    "                      | \"False\" -> const_false\n",
    "                      | STRING -> string\n",
    "                      | number\n",
    "\n",
    "attr_pattern: NAME (\".\" NAME)+ -> value\n",
    "\n",
    "name_or_attr_pattern: NAME (\".\" NAME)* -> value\n",
    "\n",
    "mapping_item_pattern: (literal_pattern|attr_pattern) \":\" as_pattern\n",
    "\n",
    "_sequence_pattern: (sequence_item_pattern (\",\" sequence_item_pattern)* \",\"?)?\n",
    "?sequence_item_pattern: as_pattern\n",
    "                      | \"*\" NAME -> star_pattern\n",
    "\n",
    "class_pattern: name_or_attr_pattern \"(\" [arguments_pattern \",\"?] \")\"\n",
    "arguments_pattern: pos_arg_pattern [\",\" keyws_arg_pattern]\n",
    "                 | keyws_arg_pattern -> no_pos_arguments\n",
    "\n",
    "pos_arg_pattern: as_pattern (\",\" as_pattern)*\n",
    "keyws_arg_pattern: keyw_arg_pattern (\",\" keyw_arg_pattern)*\n",
    "keyw_arg_pattern: NAME \"=\" as_pattern\n",
    "\n",
    "\n",
    "\n",
    "suite: simple_stmt | _NEWLINE _INDENT stmt+ _DEDENT\n",
    "\n",
    "?test: or_test (\"if\" or_test \"else\" test)?\n",
    "     | lambdef\n",
    "     | assign_expr\n",
    "\n",
    "assign_expr: name \":=\" test\n",
    "\n",
    "?test_nocond: or_test | lambdef_nocond\n",
    "\n",
    "?or_test: and_test (\"or\" and_test)*\n",
    "?and_test: not_test_ (\"and\" not_test_)*\n",
    "?not_test_: \"not\" not_test_ -> not_test\n",
    "         | comparison\n",
    "?comparison: expr (comp_op expr)*\n",
    "star_expr: \"*\" expr\n",
    "\n",
    "?expr: or_expr\n",
    "?or_expr: xor_expr (\"|\" xor_expr)*\n",
    "?xor_expr: and_expr (\"^\" and_expr)*\n",
    "?and_expr: shift_expr (\"&\" shift_expr)*\n",
    "?shift_expr: arith_expr (_shift_op arith_expr)*\n",
    "?arith_expr: term (_add_op term)*\n",
    "?term: factor (_mul_op factor)*\n",
    "?factor: _unary_op factor | power\n",
    "\n",
    "!_unary_op: \"+\"|\"-\"|\"~\"\n",
    "!_add_op: \"+\"|\"-\"\n",
    "!_shift_op: \"<<\"|\">>\"\n",
    "!_mul_op: \"*\"|\"@\"|\"/\"|\"%\"|\"//\"\n",
    "!comp_op: \"<\"|\">\"|\"==\"|\">=\"|\"<=\"|\"<>\"|\"!=\"|\"in\"|\"not\" \"in\"|\"is\"|\"is\" \"not\"\n",
    "\n",
    "?power: await_expr (\"**\" factor)?\n",
    "?await_expr: AWAIT? atom_expr\n",
    "AWAIT: \"await\"\n",
    "\n",
    "?atom_expr: atom_expr \"(\" [arguments] \")\"      -> funccall\n",
    "          | atom_expr \"[\" subscriptlist \"]\"  -> getitem\n",
    "          | atom_expr \".\" name               -> getattr\n",
    "          | atom\n",
    "\n",
    "?atom: \"(\" yield_expr \")\"\n",
    "     | \"(\" _tuple_inner? \")\" -> tuple\n",
    "     | \"(\" comprehension{test_or_star_expr} \")\" -> tuple_comprehension\n",
    "     | \"[\" _exprlist? \"]\"  -> list\n",
    "     | \"[\" comprehension{test_or_star_expr} \"]\"  -> list_comprehension\n",
    "     | \"{\" _dict_exprlist? \"}\" -> dict\n",
    "     | \"{\" comprehension{key_value} \"}\" -> dict_comprehension\n",
    "     | \"{\" _exprlist \"}\" -> set\n",
    "     | \"{\" comprehension{test} \"}\" -> set_comprehension\n",
    "     | name -> var\n",
    "     | number\n",
    "     | string_concat\n",
    "     | \"(\" test \")\"\n",
    "     | \"...\" -> ellipsis\n",
    "     | \"None\"    -> const_none\n",
    "     | \"True\"    -> const_true\n",
    "     | \"False\"   -> const_false\n",
    "\n",
    "\n",
    "?string_concat: string+\n",
    "\n",
    "_tuple_inner: test_or_star_expr ((\",\" test_or_star_expr)+ [\",\"] | \",\")\n",
    "\n",
    "?test_or_star_expr: test\n",
    "                 | star_expr\n",
    "\n",
    "?subscriptlist: subscript\n",
    "              | subscript ((\",\" subscript)+ [\",\"] | \",\") -> subscript_tuple\n",
    "?subscript: test | ([test] \":\" [test] [sliceop]) -> slice\n",
    "sliceop: \":\" [test]\n",
    "?exprlist: (expr|star_expr)\n",
    "         | (expr|star_expr) ((\",\" (expr|star_expr))+ [\",\"]|\",\")\n",
    "?testlist: test | testlist_tuple\n",
    "testlist_tuple: test ((\",\" test)+ [\",\"] | \",\")\n",
    "_dict_exprlist: (key_value | \"**\" expr) (\",\" (key_value | \"**\" expr))* [\",\"]\n",
    "\n",
    "key_value: test \":\"  test\n",
    "\n",
    "_exprlist: test_or_star_expr (\",\"  test_or_star_expr)* [\",\"]\n",
    "\n",
    "classdef: \"class\" name [\"(\" [arguments] \")\"] \":\" suite\n",
    "\n",
    "\n",
    "\n",
    "arguments: argvalue (\",\" argvalue)*  (\",\" [ starargs | kwargs])?\n",
    "         | starargs\n",
    "         | kwargs\n",
    "         | comprehension{test}\n",
    "\n",
    "starargs: stararg (\",\" stararg)* (\",\" argvalue)* [\",\" kwargs]\n",
    "stararg: \"*\" test\n",
    "kwargs: \"**\" test (\",\" argvalue)*\n",
    "\n",
    "?argvalue: test (\"=\" test)?\n",
    "\n",
    "\n",
    "comprehension{comp_result}: comp_result comp_fors [comp_if]\n",
    "comp_fors: comp_for+\n",
    "comp_for: [ASYNC] \"for\" exprlist \"in\" or_test\n",
    "ASYNC: \"async\"\n",
    "?comp_if: \"if\" test_nocond\n",
    "\n",
    "encoding_decl: name\n",
    "\n",
    "yield_expr: \"yield\" [testlist]\n",
    "          | \"yield\" \"from\" test -> yield_from\n",
    "\n",
    "number: DEC_NUMBER | HEX_NUMBER | BIN_NUMBER | OCT_NUMBER | FLOAT_NUMBER | IMAG_NUMBER\n",
    "string: STRING \n",
    "\n",
    "\n",
    "_NEWLINE: ( /\\r?\\n[\\t ]*/ | COMMENT )+\n",
    "\n",
    "%import common.WS_INLINE\n",
    "%ignore WS_INLINE\n",
    "%ignore /\\\\[\\t \\f]*\\r?\\n/   // LINE_CONT\n",
    "%ignore COMMENT\n",
    "%declare _INDENT _DEDENT\n",
    "\n",
    "\n",
    "\n",
    "!name: NAME | \"match\" | \"case\"\n",
    "NAME: /[^\\W\\d]\\w*/\n",
    "COMMENT: /\\#[^\\n]*/\n",
    "\n",
    "// 1) Prefix and escape definitions (still tokens)\n",
    "PREFIX       : /[uUbBfF]?[rR]?/ | /[rR][uUbBfF]/\n",
    "ESCAPED_CHAR : /\\\\./\n",
    "\n",
    "// 2) Short (single-line) strings as tokens\n",
    "STRING_DOUBLE: \"\\\"\" ( ESCAPED_CHAR | /[^\"\\\\]/ )* \"\\\"\"\n",
    "STRING_SINGLE: \"'\"  ( ESCAPED_CHAR | /[^'\\\\]/ )* \"'\"\n",
    "\n",
    "// 3) Expose a parser-rule to combine them\n",
    "STRING: PREFIX? (STRING_DOUBLE | STRING_SINGLE)\n",
    "\n",
    "// 4) Triple-quote markers as tokens\n",
    "TRIPLE_DQ : \"\\\"\\\"\\\"\"\n",
    "TRIPLE_SQ : \"'''\"\n",
    "\n",
    "// 5) The “content” parts as parser-rules\n",
    "long_double_content : ( ESCAPED_CHAR | \"\\\"\\\"\" | /[^\"\\\\]/ )*\n",
    "long_single_content : ( ESCAPED_CHAR | \"''\" | /[^'\\\\]/ )*\n",
    "\n",
    "// 6) The long-string parser-rule\n",
    "long_string : PREFIX? ( TRIPLE_DQ long_double_content TRIPLE_DQ\n",
    "            | TRIPLE_SQ long_single_content TRIPLE_SQ )\n",
    "\n",
    "_SPECIAL_DEC: \"0\"..\"9\"        (\"_\"?  \"0\"..\"9\"                       )*\n",
    "BAD_DEC_NUMBER: \"0\" ( \"_\" | \"0\" )* \"1\"..\"9\" ( \"_\"? \"0\"..\"9\" )*\n",
    "DEC_NUMBER: \"0\" ( \"_\"? \"0\" )*\n",
    "          | \"1\"..\"9\" ( \"_\"? \"0\"..\"9\" )* \n",
    "HEX_NUMBER.2: \"0\" (\"x\" | \"X\") (\"_\"? (\"0\"..\"9\" | \"a\"..\"f\" | \"A\"..\"F\"))+\n",
    "OCT_NUMBER.2: \"0\" (\"o\" | \"O\") (\"_\"?  \"0\"..\"7\"                       )+\n",
    "BIN_NUMBER.2: \"0\" (\"b\" | \"B\") (\"_\"?  \"0\"..\"1\"                       )+\n",
    "\n",
    "_EXP: (\"e\"|\"E\") [\"+\" | \"-\"] _SPECIAL_DEC\n",
    "DECIMAL: \".\" _SPECIAL_DEC | _SPECIAL_DEC \".\" _SPECIAL_DEC?\n",
    "FLOAT_NUMBER.2: _SPECIAL_DEC _EXP | DECIMAL _EXP?\n",
    "IMAG_NUMBER.2: (_SPECIAL_DEC      | FLOAT_NUMBER) (\"J\" | \"j\")\n",
    "\n",
    "// Comma-separated list (with an optional trailing comma)\n",
    "cs_list{item}: item (\",\" item)* \",\"?\n",
    "_cs_list{item}: item (\",\" item)* \",\"?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4041c4c",
   "metadata": {},
   "source": [
    "LONGSTRING, DEC_NUMBER, STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe6a03",
   "metadata": {},
   "source": [
    "- SFT including masking (it means that LCD is needed at inference-time)\n",
    "- SFT w/ \"distillation\" (binary cross entropy: masked probs / unmasked probs) (We get rid of LCD at inference-time)\n",
    "\n",
    "Model : LLama 2 7B (worst model, wasn't trained on code) ; Mistral 7B (better) ; Qwen2.5/3-7B (best)\n",
    "\n",
    "Dataset: OpenCodeInstruct (it is Python only, validated by automated unit testing and LLM-based quality judgement)\n",
    "\n",
    "Benchmark: HumanEval, MBPP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c5948",
   "metadata": {},
   "source": [
    "- SMC for RL: we generate sentences using SMC (or just LCD?), and we apply RL thanks to those sentences. It might be a big problem because bad sentences are integrated into the loss, thus when we have a bad example we are moving far from those examples.\n",
    "\n",
    "see: random down-sampling, max-reward down-sampling and max-variance down-sampling in \"Not All Rollouts are Useful: Down-Sampling Rollouts\n",
    "in LLM Reinforcement Learning\"\n",
    "\n",
    "\n",
    "The number of uncorrect sentences is much greater than the number of correct ones. Including uncorrect ones is useful as it provies information about what not to do. However, the information provided by a correct response is much more important. \n",
    "The question is, by sampling trajectories from another policy (the one defined by LCD or by SMC), can we improve the upgrading of $\\theta$? To answer this, I first need to understand TRPO, PPO and GRPO mathematically (from where are the formulaes derived, the estimator of what are we computing, etc. ). I hope that by perfectly understanding this, I will be able to demonstrate that sampling trajectories from the constrained distributions can improve the estimator by diminushing the variance of the estimator?..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04806e",
   "metadata": {},
   "source": [
    "According to \"What Makes a Reward Model a Good Teacher? An Optimization Perspective\", the lower the variance of the reward $Var_{y \\sim \\pi_{\\theta}(.|x)}(r_{RM}(x,y))$ is, the faster the maximization of the expectation $E_{x \\sim \\mathcal{S}}[E_{y \\sim \\pi_{\\theta}(.|x)}[r_{RM}(x,y)]]$ can theoretically be. \n",
    "\n",
    "To maximize this quantity, TRPO, PPO, GRPO use an algorithm. Maybe that by sampling from a constrained distribution (locally or globally), we can improve the speed at whoch the quantity is being optimized. Or by replacing $\\pi_{old}$ by $\\pi_{old}^{constrained}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea09c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import outlines\n",
    "import constraintlm as clm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3de0cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "from lark.indenter import PythonIndenter\n",
    "\n",
    "kwargs = dict(postlex=PythonIndenter(), start='file_input')\n",
    "\n",
    "\n",
    "python_parser = Lark(python_grammar, parser='lalr', postlex=PythonIndenter())       # lexer='contextual',\n",
    "python_parser3 = Lark.open_from_package('lark', 'python.lark', ['grammars'], parser='lalr', lexer = 'basic', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04e4fc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('DEF', 'def')\n",
      "Token('NAME', 'f')\n",
      "Token('LPAR', '(')\n",
      "Token('NAME', 'x')\n",
      "Token('RPAR', ')')\n",
      "Token('COLON', ':')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('_INDENT', '    ')\n",
      "Token('NAME', 'x')\n",
      "Token('EQUAL', '=')\n",
      "Token('DEC_NUMBER', '3')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('RETURN', 'return')\n",
      "Token('NAME', 'x')\n",
      "Token('_NEWLINE', '\\n')\n",
      "Token('_DEDENT', '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree(Token('RULE', 'start'), [Tree(Token('RULE', 'file_input'), [Tree(Token('RULE', 'funcdef'), [Tree(Token('RULE', 'name'), [Token('NAME', 'f')]), Tree(Token('RULE', 'parameters'), [Tree(Token('RULE', 'name'), [Token('NAME', 'x')]), None, None]), None, Tree(Token('RULE', 'suite'), [Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '3')])])]), Tree(Token('RULE', 'return_stmt'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])])])])])])])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = \"\"\"def f(x):\n",
    "    x = 3\n",
    "    return x\n",
    "\"\"\"\n",
    "for tok in python_parser.lex(code):\n",
    "    print(repr(tok))\n",
    "\n",
    "python_parser.parse(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854e8c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree(Token('RULE', 'start'), [Tree(Token('RULE', 'file_input'), [Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '10')])])]), Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'y')])]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '0')])])]), Tree(Token('RULE', 'while_stmt'), [Tree(Token('RULE', 'comparison'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'comp_op'), [Token('MORETHAN', '>')]), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '0')])]), Tree(Token('RULE', 'suite'), [Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'y')])]), Tree(Token('RULE', 'arith_expr'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'y')])]), Token('PLUS', '+'), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '2')])])])]), Tree(Token('RULE', 'assign_stmt'), [Tree(Token('RULE', 'assign'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Tree(Token('RULE', 'arith_expr'), [Tree('var', [Tree(Token('RULE', 'name'), [Token('NAME', 'x')])]), Token('MINUS', '-'), Tree(Token('RULE', 'number'), [Token('DEC_NUMBER', '1')])])])])]), None])])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_test = \"\"\"x=10\\\n",
    "\\ny=0 \\nwhile x>0: \\n    y = y + 2 \\n    x = x - 1 \\n\"\"\"\n",
    "\n",
    "python_parser.parse(code_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a010864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('DEF', 'def')\n",
      "Token('NAME', 'f')\n",
      "Token('LPAR', '(')\n",
      "Token('NAME', 'x')\n",
      "Token('RPAR', ')')\n",
      "Token('COLON', ':')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('_INDENT', '    ')\n",
      "Token('STRING', '\"\"')\n",
      "Token('STRING', '\" This is a paragraph \"')\n",
      "Token('STRING', '\"\"')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('NAME', 'x')\n",
      "Token('EQUAL', '=')\n",
      "Token('DEC_NUMBER', '3')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('RETURN', 'return')\n",
      "Token('NAME', 'x')\n",
      "Token('_NEWLINE', '\\n')\n",
      "Token('_DEDENT', '')\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"def f(x):\n",
    "    \\\"\\\"\\\" This is a paragraph \\\"\\\"\\\"\n",
    "    x = 3\n",
    "    return x\n",
    "\"\"\"\n",
    "\n",
    "python_parser.parse(code)\n",
    "for tok in python_parser.lex(code):\n",
    "    print(repr(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56e70450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model = outlines.models.transformers(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de4aa2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.  while  (x>0)  :  y = y + 2  ;  x = x - 1\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "generator = outlines.generate.cfg(model, python_grammar)\n",
    "sequence_p = generator(\"In the programming language Python, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in Python:\\n\", max_tokens=30)\n",
    "\n",
    "print(sequence_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44246eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def max_non_overlapping_tasks(tasks):\n",
      "    # Sort the tasks by their start times\n",
      "    # Use a heap to keep track of the tasks that can be selected\n",
      "    # The heap will store the tasks that are currently selected\n",
      "    # The heap will be sorted by their end times\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The\n"
     ]
    }
   ],
   "source": [
    "sequence_python = generator(\"You are given a list of `n` tasks, each represented as a tuple `(start, end)`, indicating the start and end times of the task. The tasks are sorted by their start times. Your goal is to determine the maximum number of non-overlapping tasks that can be selected. Two tasks are considered non-overlapping if the start time of one task is greater than or equal to the end time of the other. \\\n",
    "\\\n",
    "**Input:**\\\n",
    "- An integer `n` representing the number of tasks.\\\n",
    "- A list of `n` tuples, where each tuple `(start, end)` represents the start and end times of a task.\\\n",
    "\\\n",
    "**Output:**\\\n",
    "- An integer representing the maximum number of non-overlapping tasks that can be selected.\\\n",
    "\\\n",
    "**Constraints:**\\\n",
    "- `1 <= n <= 10^5`\\\n",
    "- `0 <= start < end <= 10^9`\\\n",
    "\\\n",
    "**Sample Input:**\\\n",
    "```\\\n",
    "3\\\n",
    "1 3\\\n",
    "2 5\\\n",
    "4 6\\\n",
    "```\\\n",
    "\\\n",
    "**Sample Output:**\\\n",
    "```\\\n",
    "2\\\n",
    "```\\\n",
    "```python \\n\", max_tokens=300)\n",
    "print(sequence_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc2cc5",
   "metadata": {},
   "source": [
    "## Testing new python grammar WITH LONG_STRING and STRING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4ed8f",
   "metadata": {},
   "source": [
    "I have to do this because my current grammar don't recognize long_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ed5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_test_grammar = r'''\n",
    "// Python 3 grammar for Lark\n",
    "\n",
    "// This grammar should parse all python 3.x code successfully.\n",
    "\n",
    "// Adapted from: https://docs.python.org/3/reference/grammar.html\n",
    "\n",
    "// Start symbols for the grammar:\n",
    "//       single_input is a single interactive statement;\n",
    "//       file_input is a module or sequence of commands read from an input file;\n",
    "//       eval_input is the input for the eval() functions.\n",
    "// NB: compound_stmt in single_input is followed by extra NEWLINE!\n",
    "//\n",
    "\n",
    "start: file_input\n",
    "\n",
    "single_input: _NEWLINE | simple_stmt | compound_stmt _NEWLINE\n",
    "file_input: (_NEWLINE | stmt)*\n",
    "eval_input: testlist _NEWLINE*\n",
    "\n",
    "decorator: \"@\" dotted_name [ \"(\" [arguments] \")\" ] _NEWLINE\n",
    "decorators: decorator+\n",
    "decorated: decorators (classdef | funcdef | async_funcdef)\n",
    "\n",
    "async_funcdef: \"async\" funcdef\n",
    "funcdef: \"def\" name \"(\" [parameters] \")\" [\"->\" test] \":\" suite\n",
    "\n",
    "parameters: paramvalue (\",\" paramvalue)* [\",\" SLASH (\",\" paramvalue)*] [\",\" [starparams | kwparams]]\n",
    "          | starparams\n",
    "          | kwparams\n",
    "\n",
    "SLASH: \"/\" // Otherwise the it will completely disappear and it will be undisguisable in the result\n",
    "starparams: (starparam | starguard) poststarparams\n",
    "starparam: \"*\" typedparam\n",
    "starguard: \"*\"\n",
    "poststarparams: (\",\" paramvalue)* [\",\" kwparams]\n",
    "kwparams: \"**\" typedparam \",\"?\n",
    "\n",
    "?paramvalue: typedparam (\"=\" test)?\n",
    "?typedparam: name (\":\" test)?\n",
    "\n",
    "\n",
    "lambdef: \"lambda\" [lambda_params] \":\" test\n",
    "lambdef_nocond: \"lambda\" [lambda_params] \":\" test_nocond\n",
    "lambda_params: lambda_paramvalue (\",\" lambda_paramvalue)* [\",\" [lambda_starparams | lambda_kwparams]]\n",
    "          | lambda_starparams\n",
    "          | lambda_kwparams\n",
    "?lambda_paramvalue: name (\"=\" test)?\n",
    "lambda_starparams: \"*\" [name]  (\",\" lambda_paramvalue)* [\",\" [lambda_kwparams]]\n",
    "lambda_kwparams: \"**\" name \",\"?\n",
    "\n",
    "\n",
    "?stmt: simple_stmt | compound_stmt\n",
    "?simple_stmt: small_stmt (\";\" small_stmt)* [\";\"] _NEWLINE\n",
    "?small_stmt: (expr_stmt | assign_stmt | del_stmt | pass_stmt | flow_stmt | import_stmt | global_stmt | nonlocal_stmt | assert_stmt)\n",
    "expr_stmt: testlist_star_expr\n",
    "assign_stmt: annassign | augassign | assign\n",
    "\n",
    "annassign: testlist_star_expr \":\" test [\"=\" test]\n",
    "assign: testlist_star_expr (\"=\" (yield_expr|testlist_star_expr))+\n",
    "augassign: testlist_star_expr augassign_op (yield_expr|testlist)\n",
    "!augassign_op: \"+=\" | \"-=\" | \"*=\" | \"@=\" | \"/=\" | \"%=\" | \"&=\" | \"|=\" | \"^=\" | \"<<=\" | \">>=\" | \"**=\" | \"//=\"\n",
    "?testlist_star_expr: test_or_star_expr\n",
    "                   | test_or_star_expr (\",\" test_or_star_expr)+ \",\"?  -> tuple\n",
    "                   | test_or_star_expr \",\"  -> tuple\n",
    "\n",
    "// For normal and annotated assignments, additional restrictions enforced by the interpreter\n",
    "del_stmt: \"del\" exprlist\n",
    "pass_stmt: \"pass\"\n",
    "?flow_stmt: break_stmt | continue_stmt | return_stmt | raise_stmt | yield_stmt\n",
    "break_stmt: \"break\"\n",
    "continue_stmt: \"continue\"\n",
    "return_stmt: \"return\" [testlist]\n",
    "yield_stmt: yield_expr\n",
    "raise_stmt: \"raise\" [test [\"from\" test]]\n",
    "import_stmt: import_name | import_from\n",
    "import_name: \"import\" dotted_as_names\n",
    "// note below: the (\".\" | \"...\") is necessary because \"...\" is tokenized as ELLIPSIS\n",
    "import_from: \"from\" (dots? dotted_name | dots) \"import\" (\"*\" | \"(\" import_as_names \")\" | import_as_names)\n",
    "!dots: \".\"+\n",
    "import_as_name: name [\"as\" name]\n",
    "dotted_as_name: dotted_name [\"as\" name]\n",
    "import_as_names: import_as_name (\",\" import_as_name)* [\",\"]\n",
    "dotted_as_names: dotted_as_name (\",\" dotted_as_name)*\n",
    "dotted_name: name (\".\" name)*\n",
    "global_stmt: \"global\" name (\",\" name)*\n",
    "nonlocal_stmt: \"nonlocal\" name (\",\" name)*\n",
    "assert_stmt: \"assert\" test [\",\" test]\n",
    "\n",
    "?compound_stmt: if_stmt | while_stmt | for_stmt | try_stmt | match_stmt\n",
    "              | with_stmt | funcdef | classdef | decorated | async_stmt\n",
    "async_stmt: \"async\" (funcdef | with_stmt | for_stmt)\n",
    "if_stmt: \"if\" test \":\" suite elifs [\"else\" \":\" suite]\n",
    "elifs: elif_*\n",
    "elif_: \"elif\" test \":\" suite\n",
    "while_stmt: \"while\" test \":\" suite [\"else\" \":\" suite]\n",
    "for_stmt: \"for\" exprlist \"in\" testlist \":\" suite [\"else\" \":\" suite]\n",
    "try_stmt: \"try\" \":\" suite except_clauses [\"else\" \":\" suite] [finally]\n",
    "        | \"try\" \":\" suite finally   -> try_finally\n",
    "finally: \"finally\" \":\" suite\n",
    "except_clauses: except_clause+\n",
    "except_clause: \"except\" [test [\"as\" name]] \":\" suite\n",
    "// NB compile.c makes sure that the default except clause is last\n",
    "\n",
    "\n",
    "with_stmt: \"with\" with_items \":\" suite\n",
    "with_items: with_item (\",\" with_item)*\n",
    "with_item: test [\"as\" name]\n",
    "\n",
    "match_stmt: \"match\" test \":\" _NEWLINE _INDENT case+ _DEDENT\n",
    "\n",
    "case: \"case\" pattern [\"if\" test] \":\" suite\n",
    "\n",
    "?pattern: sequence_item_pattern \",\" _sequence_pattern -> sequence_pattern\n",
    "        | as_pattern\n",
    "?as_pattern: or_pattern (\"as\" NAME)?\n",
    "?or_pattern: closed_pattern (\"|\" closed_pattern)*\n",
    "?closed_pattern: literal_pattern\n",
    "               | NAME -> capture_pattern\n",
    "               | \"_\" -> any_pattern\n",
    "               | attr_pattern\n",
    "               | \"(\" as_pattern \")\"\n",
    "               | \"[\" _sequence_pattern \"]\" -> sequence_pattern\n",
    "               | \"(\" (sequence_item_pattern \",\" _sequence_pattern)? \")\" -> sequence_pattern\n",
    "               | \"{\" (mapping_item_pattern (\",\" mapping_item_pattern)* \",\"?)?\"}\" -> mapping_pattern\n",
    "               | \"{\" (mapping_item_pattern (\",\" mapping_item_pattern)* \",\")? \"**\" NAME \",\"? \"}\" -> mapping_star_pattern\n",
    "               | class_pattern\n",
    "\n",
    "literal_pattern: inner_literal_pattern\n",
    "\n",
    "?inner_literal_pattern: \"None\" -> const_none\n",
    "                      | \"True\" -> const_true\n",
    "                      | \"False\" -> const_false\n",
    "                      | STRING -> string\n",
    "                      | number\n",
    "\n",
    "attr_pattern: NAME (\".\" NAME)+ -> value\n",
    "\n",
    "name_or_attr_pattern: NAME (\".\" NAME)* -> value\n",
    "\n",
    "mapping_item_pattern: (literal_pattern|attr_pattern) \":\" as_pattern\n",
    "\n",
    "_sequence_pattern: (sequence_item_pattern (\",\" sequence_item_pattern)* \",\"?)?\n",
    "?sequence_item_pattern: as_pattern\n",
    "                      | \"*\" NAME -> star_pattern\n",
    "\n",
    "class_pattern: name_or_attr_pattern \"(\" [arguments_pattern \",\"?] \")\"\n",
    "arguments_pattern: pos_arg_pattern [\",\" keyws_arg_pattern]\n",
    "                 | keyws_arg_pattern -> no_pos_arguments\n",
    "\n",
    "pos_arg_pattern: as_pattern (\",\" as_pattern)*\n",
    "keyws_arg_pattern: keyw_arg_pattern (\",\" keyw_arg_pattern)*\n",
    "keyw_arg_pattern: NAME \"=\" as_pattern\n",
    "\n",
    "\n",
    "\n",
    "suite: simple_stmt | _NEWLINE _INDENT stmt+ _DEDENT\n",
    "\n",
    "?test: or_test (\"if\" or_test \"else\" test)?\n",
    "     | lambdef\n",
    "     | assign_expr\n",
    "\n",
    "assign_expr: name \":=\" test\n",
    "\n",
    "?test_nocond: or_test | lambdef_nocond\n",
    "\n",
    "?or_test: and_test (\"or\" and_test)*\n",
    "?and_test: not_test_ (\"and\" not_test_)*\n",
    "?not_test_: \"not\" not_test_ -> not_test\n",
    "         | comparison\n",
    "?comparison: expr (comp_op expr)*\n",
    "star_expr: \"*\" expr\n",
    "\n",
    "?expr: or_expr\n",
    "?or_expr: xor_expr (\"|\" xor_expr)*\n",
    "?xor_expr: and_expr (\"^\" and_expr)*\n",
    "?and_expr: shift_expr (\"&\" shift_expr)*\n",
    "?shift_expr: arith_expr (_shift_op arith_expr)*\n",
    "?arith_expr: term (_add_op term)*\n",
    "?term: factor (_mul_op factor)*\n",
    "?factor: _unary_op factor | power\n",
    "\n",
    "!_unary_op: \"+\"|\"-\"|\"~\"\n",
    "!_add_op: \"+\"|\"-\"\n",
    "!_shift_op: \"<<\"|\">>\"\n",
    "!_mul_op: \"*\"|\"@\"|\"/\"|\"%\"|\"//\"\n",
    "// <> isn't actually a valid comparison operator in Python. It's here for the\n",
    "// sake of a __future__ import described in PEP 401 (which really works :-)\n",
    "!comp_op: \"<\"|\">\"|\"==\"|\">=\"|\"<=\"|\"<>\"|\"!=\"|\"in\"|\"not\" \"in\"|\"is\"|\"is\" \"not\"\n",
    "\n",
    "?power: await_expr (\"**\" factor)?\n",
    "?await_expr: AWAIT? atom_expr\n",
    "AWAIT: \"await\"\n",
    "\n",
    "?atom_expr: atom_expr \"(\" [arguments] \")\"      -> funccall\n",
    "          | atom_expr \"[\" subscriptlist \"]\"  -> getitem\n",
    "          | atom_expr \".\" name               -> getattr\n",
    "          | atom\n",
    "\n",
    "?atom: \"(\" yield_expr \")\"\n",
    "     | \"(\" _tuple_inner? \")\" -> tuple\n",
    "     | \"(\" comprehension{test_or_star_expr} \")\" -> tuple_comprehension\n",
    "     | \"[\" _exprlist? \"]\"  -> list\n",
    "     | \"[\" comprehension{test_or_star_expr} \"]\"  -> list_comprehension\n",
    "     | \"{\" _dict_exprlist? \"}\" -> dict\n",
    "     | \"{\" comprehension{key_value} \"}\" -> dict_comprehension\n",
    "     | \"{\" _exprlist \"}\" -> set\n",
    "     | \"{\" comprehension{test} \"}\" -> set_comprehension\n",
    "     | name -> var\n",
    "     | number\n",
    "     | string_concat\n",
    "     | \"(\" test \")\"\n",
    "     | \"...\" -> ellipsis\n",
    "     | \"None\"    -> const_none\n",
    "     | \"True\"    -> const_true\n",
    "     | \"False\"   -> const_false\n",
    "\n",
    "\n",
    "?string_concat: string+\n",
    "\n",
    "_tuple_inner: test_or_star_expr ((\",\" test_or_star_expr)+ [\",\"] | \",\")\n",
    "\n",
    "?test_or_star_expr: test\n",
    "                 | star_expr\n",
    "\n",
    "?subscriptlist: subscript\n",
    "              | subscript ((\",\" subscript)+ [\",\"] | \",\") -> subscript_tuple\n",
    "?subscript: test | ([test] \":\" [test] [sliceop]) -> slice\n",
    "sliceop: \":\" [test]\n",
    "?exprlist: (expr|star_expr)\n",
    "         | (expr|star_expr) ((\",\" (expr|star_expr))+ [\",\"]|\",\")\n",
    "?testlist: test | testlist_tuple\n",
    "testlist_tuple: test ((\",\" test)+ [\",\"] | \",\")\n",
    "_dict_exprlist: (key_value | \"**\" expr) (\",\" (key_value | \"**\" expr))* [\",\"]\n",
    "\n",
    "key_value: test \":\"  test\n",
    "\n",
    "_exprlist: test_or_star_expr (\",\"  test_or_star_expr)* [\",\"]\n",
    "\n",
    "classdef: \"class\" name [\"(\" [arguments] \")\"] \":\" suite\n",
    "\n",
    "\n",
    "\n",
    "arguments: argvalue (\",\" argvalue)*  (\",\" [ starargs | kwargs])?\n",
    "         | starargs\n",
    "         | kwargs\n",
    "         | comprehension{test}\n",
    "\n",
    "starargs: stararg (\",\" stararg)* (\",\" argvalue)* [\",\" kwargs]\n",
    "stararg: \"*\" test\n",
    "kwargs: \"**\" test (\",\" argvalue)*\n",
    "\n",
    "?argvalue: test (\"=\" test)?\n",
    "\n",
    "\n",
    "comprehension{comp_result}: comp_result comp_fors [comp_if]\n",
    "comp_fors: comp_for+\n",
    "comp_for: [ASYNC] \"for\" exprlist \"in\" or_test\n",
    "ASYNC: \"async\"\n",
    "?comp_if: \"if\" test_nocond\n",
    "\n",
    "// not used in grammar, but may appear in \"node\" passed from Parser to Compiler\n",
    "encoding_decl: name\n",
    "\n",
    "yield_expr: \"yield\" [testlist]\n",
    "          | \"yield\" \"from\" test -> yield_from\n",
    "\n",
    "number: DEC_NUMBER | HEX_NUMBER | BIN_NUMBER | OCT_NUMBER | FLOAT_NUMBER | IMAG_NUMBER\n",
    "string: STRING | LONG_STRING\n",
    "\n",
    "// Other terminals\n",
    "\n",
    "_NEWLINE: ( /\\r?\\n[\\t ]*/ | COMMENT )+\n",
    "\n",
    "%ignore /[\\t \\f]+/  // WS\n",
    "%ignore /\\\\[\\t \\f]*\\r?\\n/   // LINE_CONT\n",
    "%ignore COMMENT\n",
    "%declare _INDENT _DEDENT\n",
    "\n",
    "\n",
    "// Python terminals\n",
    "\n",
    "!name: NAME | \"match\" | \"case\"\n",
    "NAME: /[^\\W\\d]\\w*/\n",
    "COMMENT: /#[^\\n]*/\n",
    "STRING: /([ubf]?r?|r[ubf])((\"\"|\"(?:\\\\\\\\)*?\"|\"(.*?)([^\\n\\\\])(\\\\\\\\)*?\")|(''|'(?:\\\\\\\\)*?'|'(.*?)(?:[^\\n\\\\])(?:\\\\\\\\)*?'))/i\n",
    "LONG_STRING: /([ubf]?r?|r[ubf])((\"\"\"\"\"\"|\"\"\"(?:\\\\\\\\)*?\"\"\"|\"\"\"(.*?)(?:[^\\n\\\\])(?:\\\\\\\\)*?\"\"\")|(''' + r\"\"\"''''''|'''(?:\\\\\\\\)*?'''|'''(.*?)(?:[^\\n\\\\])(?:\\\\\\\\)*?'''))/is\"\"\" +r'''\n",
    "\n",
    "_SPECIAL_DEC: \"0\"..\"9\"        (\"_\"?  \"0\"..\"9\"                       )*\n",
    "_ILLEGAL_LEADING_ZERO: /\"0\"(\"_\"?\"0\")*(\"_\"?\"1\"..\"9\")+/\n",
    "DEC_NUMBER:   \"1\"..\"9\"        (\"_\"?  \"0\"..\"9\"                       )*\n",
    "          |   \"0\"             (\"_\"?  \"0\"                            )*\n",
    "HEX_NUMBER.2: \"0\" (\"x\" | \"X\") (\"_\"? (\"0\"..\"9\" | \"a\"..\"f\" | \"A\"..\"F\"))+\n",
    "OCT_NUMBER.2: \"0\" (\"o\" | \"O\") (\"_\"?  \"0\"..\"7\"                       )+\n",
    "BIN_NUMBER.2: \"0\" (\"b\" | \"B\") (\"_\"?  \"0\"..\"1\"                       )+\n",
    "\n",
    "_EXP: (\"e\"|\"E\") [\"+\" | \"-\"] _SPECIAL_DEC\n",
    "DECIMAL: \".\" _SPECIAL_DEC | _SPECIAL_DEC \".\" _SPECIAL_DEC?\n",
    "FLOAT_NUMBER.2: _SPECIAL_DEC _EXP | DECIMAL _EXP?\n",
    "IMAG_NUMBER.2: (_SPECIAL_DEC      | FLOAT_NUMBER) (\"J\" | \"j\")\n",
    "\n",
    "\n",
    "// Comma-separated list (with an optional trailing comma)\n",
    "cs_list{item}: item (\",\" item)* \",\"?\n",
    "_cs_list{item}: item (\",\" item)* \",\"?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0acca78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import outlines\n",
    "import constraintlm as clm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818fd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "from lark.indenter import PythonIndenter, PostLex\n",
    "\n",
    "python_test_parser = Lark(python_test_grammar, parser='lalr', lexer='basic', postlex=PythonIndenter())\n",
    "python_test_parser2 = Lark(python_test_grammar, parser='lalr', lexer='contextual', postlex=PythonIndenter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe5d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PY_KEYWORDS = {\n",
    "    \"False\",\"None\",\"True\",\"and\",\"as\",\"assert\",\"async\",\"await\",\n",
    "    \"break\",\"class\",\"continue\",\"def\",\"del\",\"elif\",\"else\",\"except\",\n",
    "    \"finally\",\"for\",\"from\",\"global\",\"if\",\"import\",\"in\",\"is\",\"lambda\",\n",
    "    \"nonlocal\",\"not\",\"or\",\"pass\",\"raise\",\"return\",\"try\",\n",
    "    \"while\",\"with\",\"yield\"\n",
    "}\n",
    "\n",
    "class KeywordLocker(PythonIndenter):\n",
    "    _keyword_tokens = tuple(k.upper() for k in PY_KEYWORDS)\n",
    "\n",
    "    # override the *property*\n",
    "    @property\n",
    "    def always_accept(self):\n",
    "        return super().always_accept + self._keyword_tokens\n",
    "\n",
    "    def process(self, stream):\n",
    "        for tok in super().process(stream):\n",
    "            if tok.type == \"NAME\" and tok.value in PY_KEYWORDS:\n",
    "                tok.type = tok.value.upper()\n",
    "            yield tok\n",
    "\n",
    "python_test_parser3 = Lark(python_test_grammar, parser='lalr', lexer='contextual', postlex=KeywordLocker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa86abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('DEF', 'def')\n",
      "Token('NAME', 'f')\n",
      "Token('LPAR', '(')\n",
      "Token('NAME', 'x')\n",
      "Token('RPAR', ')')\n",
      "Token('COLON', ':')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('_INDENT', '    ')\n",
      "Token('LONG_STRING', '\"\"\"This is a paragraph\"\"\"')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('NAME', 's')\n",
      "Token('EQUAL', '=')\n",
      "Token('STRING', '\"a\"')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('NAME', 'x')\n",
      "Token('EQUAL', '=')\n",
      "Token('DEC_NUMBER', '3')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('RETURN', 'return')\n",
      "Token('NAME', 'x')\n",
      "Token('_NEWLINE', '\\n')\n",
      "Token('_DEDENT', '')\n",
      "start\n",
      "  file_input\n",
      "    funcdef\n",
      "      name\tf\n",
      "      parameters\n",
      "        name\tx\n",
      "        None\n",
      "        None\n",
      "      None\n",
      "      suite\n",
      "        expr_stmt\n",
      "          string\t\"\"\"This is a paragraph\"\"\"\n",
      "        assign_stmt\n",
      "          assign\n",
      "            var\n",
      "              name\ts\n",
      "            string\t\"a\"\n",
      "        assign_stmt\n",
      "          assign\n",
      "            var\n",
      "              name\tx\n",
      "            number\t3\n",
      "        return_stmt\n",
      "          var\n",
      "            name\tx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code = '''def f(x):\n",
    "    \"\"\"This is a paragraph\"\"\"\n",
    "    s = \"a\"\n",
    "    x = 3\n",
    "    return x\n",
    "'''\n",
    "for tok in python_test_parser.lex(code):\n",
    "    print(repr(tok))\n",
    "\n",
    "print(python_test_parser.parse(code).pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc79d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('DEF', 'def')\n",
      "Token('NAME', 'max_non_overlapping_tasks')\n",
      "Token('LPAR', '(')\n",
      "Token('NAME', 'tasks')\n",
      "Token('RPAR', ')')\n",
      "Token('COLON', ':')\n",
      "Token('_NEWLINE', '\\n    ')\n",
      "Token('LONG_STRING', '\"\"\"\\n    Returns the maximum number of non-overlapping tasks that can be selected from a list of tasks.\\n\\n    :param tasks: List of tuples, where each tuple (start, end) represents the start and end times of a task.\\n    :return: Integer representing the maximum number of non-overlapping tasks.\\n    \"\"\"')\n",
      "Token('_NEWLINE', '\\n    ')\n"
     ]
    },
    {
     "ename": "UnexpectedToken",
     "evalue": "Unexpected token Token('LONG_STRING', '\"\"\"\\n    Returns the maximum number of non-overlapping tasks that can be selected from a list of tasks.\\n\\n    :param tasks: List of tuples, where each tuple (start, end) represents the start and end times of a task.\\n    :return: Integer representing the maximum number of non-overlapping tasks.\\n    \"\"\"') at line 2, column 5.\nExpected one of: \n\t* _INDENT\nPrevious tokens: [Token('_NEWLINE', '\\n    ')]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnexpectedCharacters\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/lexer.py:665\u001b[39m, in \u001b[36mContextualLexer.lex\u001b[39m\u001b[34m(self, lexer_state, parser_state)\u001b[39m\n\u001b[32m    664\u001b[39m         lexer = \u001b[38;5;28mself\u001b[39m.lexers[parser_state.position]\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m lexer.next_token(lexer_state, parser_state)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/lexer.py:598\u001b[39m, in \u001b[36mBasicLexer.next_token\u001b[39m\u001b[34m(self, lex_state, parser_state)\u001b[39m\n\u001b[32m    597\u001b[39m         allowed = {\u001b[33m\"\u001b[39m\u001b[33m<END-OF-FILE>\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedCharacters(lex_state.text, line_ctr.char_pos, line_ctr.line, line_ctr.column,\n\u001b[32m    599\u001b[39m                                allowed=allowed, token_history=lex_state.last_token \u001b[38;5;129;01mand\u001b[39;00m [lex_state.last_token],\n\u001b[32m    600\u001b[39m                                state=parser_state, terminals_by_name=\u001b[38;5;28mself\u001b[39m.terminals_by_name)\n\u001b[32m    602\u001b[39m value, type_ = res\n",
      "\u001b[31mUnexpectedCharacters\u001b[39m: No terminal matches '\"' in the current parser context, at line 2 col 5\n\n    \"\"\"\n    ^\nExpected one of: \n\t* FALSE\n\t* NOT\n\t* TRUE\n\t* AS\n\t* DEL\n\t* AWAIT\n\t* CONTINUE\n\t* EXCEPT\n\t* ELSE\n\t* RETURN\n\t* WITH\n\t* YIELD\n\t* ELIF\n\t* FOR\n\t* DEF\n\t* PASS\n\t* AND\n\t* FINALLY\n\t* CLASS\n\t* ASYNC\n\t* WHILE\n\t* NONLOCAL\n\t* BREAK\n\t* GLOBAL\n\t* FROM\n\t* IF\n\t* NONE\n\t* OR\n\t* IS\n\t* LAMBDA\n\t* RAISE\n\t* TRY\n\t* IN\n\t* ASSERT\n\t* IMPORT\n\nPrevious tokens: Token('_NEWLINE', '\\n    ')\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnexpectedToken\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m python_test_parser3.lex(code):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(tok))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(python_test_parser3.parse(code).pretty())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/lark.py:655\u001b[39m, in \u001b[36mLark.parse\u001b[39m\u001b[34m(self, text, start, on_error)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, start: Optional[\u001b[38;5;28mstr\u001b[39m]=\u001b[38;5;28;01mNone\u001b[39;00m, on_error: \u001b[33m'\u001b[39m\u001b[33mOptional[Callable[[UnexpectedInput], bool]]\u001b[39m\u001b[33m'\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m'\u001b[39m\u001b[33mParseTree\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    638\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the given text, according to the options provided.\u001b[39;00m\n\u001b[32m    639\u001b[39m \n\u001b[32m    640\u001b[39m \u001b[33;03m    Parameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    653\u001b[39m \n\u001b[32m    654\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parser.parse(text, start=start, on_error=on_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/parser_frontends.py:104\u001b[39m, in \u001b[36mParsingFrontend.parse\u001b[39m\u001b[34m(self, text, start, on_error)\u001b[39m\n\u001b[32m    102\u001b[39m kw = {} \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mon_error\u001b[39m\u001b[33m'\u001b[39m: on_error}\n\u001b[32m    103\u001b[39m stream = \u001b[38;5;28mself\u001b[39m._make_lexer_thread(text)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parser.parse(stream, chosen_start, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:42\u001b[39m, in \u001b[36mLALR_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, on_error)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, lexer, start, on_error=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parser.parse(lexer, start)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedInput \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:88\u001b[39m, in \u001b[36m_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_interactive:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m InteractiveParser(\u001b[38;5;28mself\u001b[39m, parser_state, parser_state.lexer)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_from_state(parser_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:111\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:100\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     99\u001b[39m     token = last_token\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m state.lexer.lex(state):\n\u001b[32m    101\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    102\u001b[39m         state.feed_token(token)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mKeywordLocker.process\u001b[39m\u001b[34m(self, stream)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m t.type == \u001b[33m\"\u001b[39m\u001b[33mNAME\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m t.value \u001b[38;5;129;01min\u001b[39;00m PY_KEYWORDS:\n\u001b[32m     17\u001b[39m             t.type = t.value.upper()       \u001b[38;5;66;03m# e.g. while → WHILE\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/clm_finetune/lib/python3.11/site-packages/lark/lexer.py:674\u001b[39m, in \u001b[36mContextualLexer.lex\u001b[39m\u001b[34m(self, lexer_state, parser_state)\u001b[39m\n\u001b[32m    672\u001b[39m     last_token = lexer_state.last_token  \u001b[38;5;66;03m# Save last_token. Calling root_lexer.next_token will change this to the wrong token\u001b[39;00m\n\u001b[32m    673\u001b[39m     token = \u001b[38;5;28mself\u001b[39m.root_lexer.next_token(lexer_state, parser_state)\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedToken(token, e.allowed, state=parser_state, token_history=[last_token], terminals_by_name=\u001b[38;5;28mself\u001b[39m.root_lexer.terminals_by_name)\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedCharacters:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mUnexpectedToken\u001b[39m: Unexpected token Token('LONG_STRING', '\"\"\"\\n    Returns the maximum number of non-overlapping tasks that can be selected from a list of tasks.\\n\\n    :param tasks: List of tuples, where each tuple (start, end) represents the start and end times of a task.\\n    :return: Integer representing the maximum number of non-overlapping tasks.\\n    \"\"\"') at line 2, column 5.\nExpected one of: \n\t* _INDENT\nPrevious tokens: [Token('_NEWLINE', '\\n    ')]\n"
     ]
    }
   ],
   "source": [
    "code = '''def max_non_overlapping_tasks(tasks):\n",
    "    \"\"\"\n",
    "    Returns the maximum number of non-overlapping tasks that can be selected from a list of tasks.\n",
    "    \n",
    "    :param tasks: List of tuples, where each tuple (start, end) represents the start and end times of a task.\n",
    "    :return: Integer representing the maximum number of non-overlapping tasks.\n",
    "    \"\"\"\n",
    "    '''\n",
    "for tok in python_test_parser3.lex(code):\n",
    "    print(repr(tok))\n",
    "\n",
    "print(python_test_parser3.parse(code).pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4afefe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('NAME', 'A')\n",
      "Token('DOT', '.')\n",
      "Token('WHILE', 'while')\n",
      "Token('LPAR', '(')\n",
      "Token('NAME', 'x')\n",
      "Token('MORETHAN', '>')\n",
      "Token('DEC_NUMBER', '0')\n",
      "Token('RPAR', ')')\n",
      "Token('COLON', ':')\n",
      "Token('NAME', 'y')\n",
      "Token('EQUAL', '=')\n",
      "Token('NAME', 'y')\n",
      "Token('PLUS', '+')\n",
      "Token('DEC_NUMBER', '2')\n",
      "Token('SEMICOLON', ';')\n",
      "Token('NAME', 'x')\n",
      "Token('EQUAL', '=')\n",
      "Token('NAME', 'x')\n",
      "Token('MINUS', '-')\n",
      "Token('DEC_NUMBER', '1')\n",
      "Token('_NEWLINE', '\\n')\n"
     ]
    },
    {
     "ename": "UnexpectedToken",
     "evalue": "Unexpected token Token('WHILE', 'while') at line 1, column 5.\nExpected one of: \n\t* CASE\n\t* MATCH\n\t* NAME\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser_state.py:77\u001b[39m, in \u001b[36mParserState.feed_token\u001b[39m\u001b[34m(self, token, is_end)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     action, arg = \u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'WHILE'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnexpectedToken\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m python_test_parser.lex(code):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(tok))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mpython_test_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\lark.py:655\u001b[39m, in \u001b[36mLark.parse\u001b[39m\u001b[34m(self, text, start, on_error)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, start: Optional[\u001b[38;5;28mstr\u001b[39m]=\u001b[38;5;28;01mNone\u001b[39;00m, on_error: \u001b[33m'\u001b[39m\u001b[33mOptional[Callable[[UnexpectedInput], bool]]\u001b[39m\u001b[33m'\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m'\u001b[39m\u001b[33mParseTree\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    638\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the given text, according to the options provided.\u001b[39;00m\n\u001b[32m    639\u001b[39m \n\u001b[32m    640\u001b[39m \u001b[33;03m    Parameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    653\u001b[39m \n\u001b[32m    654\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parser_frontends.py:104\u001b[39m, in \u001b[36mParsingFrontend.parse\u001b[39m\u001b[34m(self, text, start, on_error)\u001b[39m\n\u001b[32m    102\u001b[39m kw = {} \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mon_error\u001b[39m\u001b[33m'\u001b[39m: on_error}\n\u001b[32m    103\u001b[39m stream = \u001b[38;5;28mself\u001b[39m._make_lexer_thread(text)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:42\u001b[39m, in \u001b[36mLALR_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, on_error)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, lexer, start, on_error=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedInput \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:88\u001b[39m, in \u001b[36m_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_interactive:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m InteractiveParser(\u001b[38;5;28mself\u001b[39m, parser_state, parser_state.lexer)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:111\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:102\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m state.lexer.lex(state):\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m end_token = Token.new_borrow_pos(\u001b[33m'\u001b[39m\u001b[33m$END\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, token) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;28;01melse\u001b[39;00m Token(\u001b[33m'\u001b[39m\u001b[33m$END\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state.feed_token(end_token, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser_state.py:80\u001b[39m, in \u001b[36mParserState.feed_token\u001b[39m\u001b[34m(self, token, is_end)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m     79\u001b[39m     expected = {s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states[state].keys() \u001b[38;5;28;01mif\u001b[39;00m s.isupper()}\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedToken(token, expected, state=\u001b[38;5;28mself\u001b[39m, interactive_parser=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m arg != end_state\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m Shift:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# shift once and return\u001b[39;00m\n",
      "\u001b[31mUnexpectedToken\u001b[39m: Unexpected token Token('WHILE', 'while') at line 1, column 5.\nExpected one of: \n\t* CASE\n\t* MATCH\n\t* NAME\n"
     ]
    }
   ],
   "source": [
    "code = '''A.  while  (x>0)  :  y = y + 2  ;  x = x - 1 \\n'''\n",
    "for tok in python_test_parser.lex(code):\n",
    "    print(repr(tok))\n",
    "\n",
    "python_test_parser.parse(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca20502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "model = outlines.from_transformers(\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53176699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\outlines\\processors\\guide.py:188: UserWarning: Outlines' public *community-contributed* CFG structured generation is experimental. Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.  while  (x>0)\n"
     ]
    }
   ],
   "source": [
    "from outlines.types import CFG\n",
    "from outlines import Generator\n",
    "\n",
    "generator = Generator(model, CFG(python_test_grammar))\n",
    "sequence_p = generator(\"In the programming language Python, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in Python:\\n\", max_new_tokens=10)\n",
    "print(sequence_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33f1a8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def max_non_overlapping_tasks(tasks):\n",
      "    # Sort the tasks by their start times\n",
      "    # Use a heap to keep track of the tasks that can be selected\n",
      "    # The heap will store the tasks that are currently selected\n",
      "    # The heap will be sorted by their end times\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The heap will be used to select the tasks that are non-overlapping\n",
      "    # The\n"
     ]
    }
   ],
   "source": [
    "sequence_python = generator(\"You are given a list of `n` tasks, each represented as a tuple `(start, end)`, indicating the start and end times of the task. The tasks are sorted by their start times. Your goal is to determine the maximum number of non-overlapping tasks that can be selected. Two tasks are considered non-overlapping if the start time of one task is greater than or equal to the end time of the other. \\\n",
    "\\\n",
    "**Input:**\\\n",
    "- An integer `n` representing the number of tasks.\\\n",
    "- A list of `n` tuples, where each tuple `(start, end)` represents the start and end times of a task.\\\n",
    "\\\n",
    "**Output:**\\\n",
    "- An integer representing the maximum number of non-overlapping tasks that can be selected.\\\n",
    "\\\n",
    "**Constraints:**\\\n",
    "- `1 <= n <= 10^5`\\\n",
    "- `0 <= start < end <= 10^9`\\\n",
    "\\\n",
    "**Sample Input:**\\\n",
    "```\\\n",
    "3\\\n",
    "1 3\\\n",
    "2 5\\\n",
    "4 6\\\n",
    "```\\\n",
    "\\\n",
    "**Sample Output:**\\\n",
    "```\\\n",
    "2\\\n",
    "```\\\n",
    "```python \\n\", max_tokens=300)\n",
    "print(sequence_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783811b8",
   "metadata": {},
   "source": [
    "## Using ConstraintLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87fcc0",
   "metadata": {},
   "source": [
    "The tokens are sampled now (outlines only allows Greedy decoding for cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32bab4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwenllm = clm.TransformersLM(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1469f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\Documents\\Etudes supérieures\\ENS - MVA\\stage INRIA\\ConstraintLM\\constraintlm\\processors\\guide.py:64: UserWarning: Outlines' public *community-contributed* CFG structured generation is experimental. Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "python_logits_processor = clm.CLMCFGLogitsProcessor(python_test_grammar, model.tokenizer, qwenllm, tensor_library_name=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9d2e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"In the programming language Python, a code that do : x=10, y=0, while x>0: y = y + 2, x = x - 1, would be in Python:\\n\"]\n",
    "batch = qwenllm.tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ff890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "A.  x = 2, whilex > 0, do:={y=={y==y+2,x=={x\n"
     ]
    }
   ],
   "source": [
    "python_multinomial = clm.MultinomialSeqSampler(qwenllm, logits_processor=python_logits_processor)\n",
    "python_generated_token_ids = python_multinomial.sample(batch.input_ids, max_length=30, top_k=5)\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, python_generated_token_ids], dim=-1))[0][len(prompts[0]):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128802d",
   "metadata": {},
   "source": [
    "# The true python grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0e657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c62b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_python_grammar = Path(\"python.lark\").read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbb6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "from lark.indenter import PythonIndenter\n",
    "\n",
    "real_python_parser = Lark(real_python_grammar, parser='lalr', lexer='basic', postlex=PythonIndenter(), start='file_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f66480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token('NAME', 'A')\n",
      "Token('DOT', '.')\n",
      "Token('WHILE', 'while')\n",
      "Token('LPAR', '(')\n",
      "Token('NAME', 'x')\n",
      "Token('MORETHAN', '>')\n",
      "Token('DEC_NUMBER', '0')\n",
      "Token('RPAR', ')')\n",
      "Token('COLON', ':')\n",
      "Token('NAME', 'y')\n",
      "Token('EQUAL', '=')\n",
      "Token('NAME', 'y')\n",
      "Token('PLUS', '+')\n",
      "Token('DEC_NUMBER', '2')\n",
      "Token('SEMICOLON', ';')\n",
      "Token('NAME', 'x')\n",
      "Token('EQUAL', '=')\n",
      "Token('NAME', 'x')\n",
      "Token('MINUS', '-')\n",
      "Token('DEC_NUMBER', '1')\n",
      "Token('_NEWLINE', '\\n')\n"
     ]
    },
    {
     "ename": "UnexpectedToken",
     "evalue": "Unexpected token Token('WHILE', 'while') at line 1, column 5.\nExpected one of: \n\t* MATCH\n\t* CASE\n\t* NAME\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser_state.py:77\u001b[39m, in \u001b[36mParserState.feed_token\u001b[39m\u001b[34m(self, token, is_end)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     action, arg = \u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'WHILE'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnexpectedToken\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m real_python_parser.lex(code):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(tok))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mreal_python_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\lark.py:655\u001b[39m, in \u001b[36mLark.parse\u001b[39m\u001b[34m(self, text, start, on_error)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, start: Optional[\u001b[38;5;28mstr\u001b[39m]=\u001b[38;5;28;01mNone\u001b[39;00m, on_error: \u001b[33m'\u001b[39m\u001b[33mOptional[Callable[[UnexpectedInput], bool]]\u001b[39m\u001b[33m'\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m'\u001b[39m\u001b[33mParseTree\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    638\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the given text, according to the options provided.\u001b[39;00m\n\u001b[32m    639\u001b[39m \n\u001b[32m    640\u001b[39m \u001b[33;03m    Parameters:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    653\u001b[39m \n\u001b[32m    654\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parser_frontends.py:104\u001b[39m, in \u001b[36mParsingFrontend.parse\u001b[39m\u001b[34m(self, text, start, on_error)\u001b[39m\n\u001b[32m    102\u001b[39m kw = {} \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mon_error\u001b[39m\u001b[33m'\u001b[39m: on_error}\n\u001b[32m    103\u001b[39m stream = \u001b[38;5;28mself\u001b[39m._make_lexer_thread(text)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:42\u001b[39m, in \u001b[36mLALR_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, on_error)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, lexer, start, on_error=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedInput \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:88\u001b[39m, in \u001b[36m_Parser.parse\u001b[39m\u001b[34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_interactive:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m InteractiveParser(\u001b[38;5;28mself\u001b[39m, parser_state, parser_state.lexer)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:111\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser.py:102\u001b[39m, in \u001b[36m_Parser.parse_from_state\u001b[39m\u001b[34m(self, state, last_token)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m state.lexer.lex(state):\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m end_token = Token.new_borrow_pos(\u001b[33m'\u001b[39m\u001b[33m$END\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, token) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;28;01melse\u001b[39;00m Token(\u001b[33m'\u001b[39m\u001b[33m$END\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state.feed_token(end_token, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\lark\\parsers\\lalr_parser_state.py:80\u001b[39m, in \u001b[36mParserState.feed_token\u001b[39m\u001b[34m(self, token, is_end)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m     79\u001b[39m     expected = {s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states[state].keys() \u001b[38;5;28;01mif\u001b[39;00m s.isupper()}\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedToken(token, expected, state=\u001b[38;5;28mself\u001b[39m, interactive_parser=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m arg != end_state\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m Shift:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# shift once and return\u001b[39;00m\n",
      "\u001b[31mUnexpectedToken\u001b[39m: Unexpected token Token('WHILE', 'while') at line 1, column 5.\nExpected one of: \n\t* MATCH\n\t* CASE\n\t* NAME\n"
     ]
    }
   ],
   "source": [
    "code = '''A.  while  (x>0)  :  y = y + 2  ;  x = x - 1 \\n'''\n",
    "for tok in real_python_parser.lex(code):\n",
    "    print(repr(tok))\n",
    "\n",
    "real_python_parser.parse(code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clm_finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
