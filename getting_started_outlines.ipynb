{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289dd86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import constraintlm as clm\n",
    "import outlines\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6b0320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "qwenllm = clm.TransformersLM(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b6eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwenllm_outlines = outlines.models.transformers(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ba8cc",
   "metadata": {},
   "source": [
    "# Length of word <= N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98dba78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"In July 1789 the French\", \n",
    "    \"The best basketball player of all time is Michael\",\n",
    "    \"Ludwig Wittgenstein was a\"\n",
    "]\n",
    "batch = qwenllm.tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91cb790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenword = outlines.processors.RegexLogitsProcessor(\"\\s([A-Za-z0-9]{1,5}[.!?,]?\\s)+\", qwenllm_outlines.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb45727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "['In July 1789 the French army, under the lead of the Duke of O', 'The best basketball player of all time is Michael James, and this is his 40th', 'Ludwig Wittgenstein was a<|endoftext|> 20th c. Swiss lingu, logic']\n"
     ]
    }
   ],
   "source": [
    "cons_multinomial = clm.MultinomialSeqSampler(qwenllm, logits_processor=lenword)\n",
    "cons_generated_token_ids = cons_multinomial.sample(batch.input_ids, max_length=10, top_k=5)\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, cons_generated_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9aba93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling...\n",
      "1\n",
      "Resampling...\n",
      "2\n",
      "Resampling...\n",
      "3\n",
      "Resampling...\n",
      "4\n",
      "Resampling...\n",
      "5\n",
      "Resampling...\n",
      "6\n",
      "Resampling...\n",
      "7\n",
      "Resampling...\n",
      "8\n",
      "Resampling...\n",
      "9\n",
      "Resampling...\n",
      "10\n",
      "Resampling...\n",
      "11\n",
      "Resampling...\n",
      "12\n",
      "Resampling...\n",
      "13\n",
      "Resampling...\n",
      "14\n",
      "Resampling...\n",
      "15\n",
      "Resampling...\n",
      "16\n",
      "Resampling...\n",
      "17\n",
      "Resampling...\n",
      "18\n",
      "Resampling...\n",
      "19\n",
      "Resampling...\n",
      "In July 1789 the French first deput for the new era of the City of Paris, Louis XVI.\n",
      "He thrud to\n",
      "In July 1789 the French first deput for the new era of the City of Paris, Louis XVI.\n",
      "He thrud to\n",
      "In July 1789 the French first deput for the city of Paris and\n",
      "the first hear of the Bill of Enact in\n",
      "In July 1789 the French first deput for the city of Paris and\n",
      "the first hear of the Bill of the Third of\n",
      "In July 1789 the French first deput for the city of Paris and\n",
      "the first hear of the Bill of the Third of\n",
      "The best basketball player of all time is Michael Calab Graza\n",
      "Diego Gatt\n",
      "Send an email to AZ Eyes\n",
      "Last week saw\n",
      "The best basketball player of all time is Michael Calab Graza\n",
      "Diego Gatt\n",
      "Send an email to AZ Eyes\n",
      "Last week saw\n",
      "The best basketball player of all time is Michael Calab Graza\n",
      "Diego Gatt\n",
      "Send an email to AZ Eyes\n",
      "Last week,\n",
      "The best basketball player of all time is Michael Calab Graza\n",
      "Diego Gatt\n",
      "Send an Email to Anale Keil\n",
      "on\n",
      "The best basketball player of all time is Michael Calab Graza\n",
      "Diego Gatt\n",
      "Send an Email to Anale Olapk\n",
      "Home\n",
      "Ludwig Wittgenstein was a<|endoftext|> Procr upon all steps, from right to left. In his words, he was very keen to\n",
      "Ludwig Wittgenstein was a<|endoftext|> Procr upon all steps, from right to left. Even the obel sobel was not an\n",
      "Ludwig Wittgenstein was a<|endoftext|> Procr upon all steps, from right to left. In his words, he was a what?\n",
      "\n",
      "Ludwig Wittgenstein was a<|endoftext|> Procr upon all steps, from right to left. In his words, he was a what?\n",
      "\n",
      "Ludwig Wittgenstein was a<|endoftext|> Procr upon all steps, from right to left. Even the obel sobel was not an\n"
     ]
    }
   ],
   "source": [
    "smc_sampler_fsm = clm.SMCSampler(qwenllm, lenword)\n",
    "num_particles = 5\n",
    "max_length = 20\n",
    "B=len(prompts)\n",
    "smc_generated_token_ids_fsm = smc_sampler_fsm.sample(batch.input_ids, max_length=max_length, num_particles=num_particles, ess_threshold=10)\n",
    "for a,b in zip(qwenllm.tokenizer.batch_decode(batch.input_ids.repeat_interleave(num_particles, dim=0)), qwenllm.tokenizer.batch_decode(smc_generated_token_ids_fsm[0].reshape(B*num_particles,max_length))):\n",
    "    print(a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4c829",
   "metadata": {},
   "source": [
    "# RPN Typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ac4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "_number_or_op_or_var = re.compile(r\"\\d+|[+\\-*/]|[a-zA-Z][a-zA-Z0-9]*\")\n",
    "_var_name = re.compile(r'^[A-Za-z][A-Za-z0-9]*$')\n",
    "\n",
    "class RPNTypedLogitsProcessor(outlines.processors.base_logits_processor.OutlinesLogitsProcessor):\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fsm_digitsvar = outlines.processors.RegexLogitsProcessor(r\"(\\d+|[ +\\-*/]|[a-zA-Z][a-zA-Z0-9]*)+\", qwenllm_outlines.tokenizer)\n",
    "    \n",
    "\n",
    "    def _extract_symbols(self, text: str) -> Optional[List[str]]:\n",
    "        \"\"\"\n",
    "        Parse `text` into a list of RPN tokens (integers and +, -, *, /).\n",
    "        Returns None if any invalid characters are present.\n",
    "        \"\"\"\n",
    "        matches = list(_number_or_op_or_var.finditer(text))    # list of re.Match objects (it contains the substring, the position of the beginning and the end of the substring)\n",
    "        symbols = [m.group(0) for m in matches]         # list of substrings that match \\d+ or [+\\-*/] or ([a-zA-Z][a-zA-Z0-9])+\n",
    "        cleaned = _number_or_op_or_var.sub(\"\", text)           # remove the match from text, we should have \"   \" only\n",
    "        if cleaned.replace(\" \", \"\"):                    # we should obtain \"\", otherwise text contained a non-digit-nor-operator char\n",
    "            return None\n",
    "        return symbols\n",
    "    \n",
    "\n",
    "    def prefix(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Given the token IDs generated so far (input_ids), \n",
    "        return the score associated by the constraint.\n",
    "        \"\"\"\n",
    "        # Decode all sequences at once\n",
    "        texts = self.tokenizer.batch_decode(\n",
    "            input_ids, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        scores = []\n",
    "        \n",
    "        for text in texts:\n",
    "            stack = []          # New stack for each sentence\n",
    "            variables = {}      # New table of variables for each sentence\n",
    "            \n",
    "            if not text:\n",
    "                scores.append(0.0)      # empty context is always a valid prefix\n",
    "                continue\n",
    "            symbols = self._extract_symbols(text)\n",
    "            if symbols is None:             # text contains non-digit-nor-operator char => symbols = None \n",
    "                scores.append(float('-inf'))\n",
    "                continue\n",
    "            depth = 0\n",
    "            valid = True\n",
    "            for sym in symbols:\n",
    "                if sym.isdigit() or bool(_var_name.match(sym)):\n",
    "                    depth += 1\n",
    "                else:\n",
    "                    # operator\n",
    "                    if depth < 2:\n",
    "                        scores.append(float('-inf'))\n",
    "                        valid = False\n",
    "                        break\n",
    "                    depth -= 1\n",
    "                \n",
    "                if sym.isdigit():\n",
    "                    stack.append(float(sym))\n",
    "                elif bool(_var_name.match(sym)):\n",
    "                    stack.append(str(sym))\n",
    "                elif sym == \"=\":\n",
    "                    if type(stack[-2]) == float:\n",
    "                        scores.append(float('-inf'))\n",
    "                        valid = False\n",
    "                        break\n",
    "                    else: # stack[-2] is a string. We never append operators to the stack\n",
    "                        if type(stack[-1]) == float:    # stack[-1] is a number\n",
    "                            variables[stack[-2]] = stack[-1]\n",
    "                            new_value = stack[-1]\n",
    "                            _ = stack.pop()\n",
    "                            _ = stack.pop()\n",
    "                            stack.append(new_value)\n",
    "                        elif stack[-1] in variables:    # stack[-1] is a defined variable \n",
    "                            variables[stack[-2]] = variables[stack[-1]]\n",
    "                            new_value = variables[stack[-1]]\n",
    "                            _ = stack.pop()\n",
    "                            _ = stack.pop()\n",
    "                            stack.append(new_value)\n",
    "                        else :                          # stack[-1] is an undefined variable\n",
    "                            scores.append(float('-inf'))\n",
    "                            valid = False\n",
    "                            break\n",
    "                else:   # any operator that isn't \"=\"\n",
    "                    if stack[-1] or stack[-2] not in variables: # if there is an undefined variable\n",
    "                        scores.append(float('-inf'))\n",
    "                        valid = False\n",
    "                        break\n",
    "                    if stack[-1] in variables:                  # if defined, we convert variables into their value\n",
    "                        stack[-1] = variables[stack[-1]]\n",
    "                    elif stack[-2] in variables:                # if defined, we convert variables into their value\n",
    "                        stack[-2] = variables[stack[-2]] \n",
    "                    # Now we sould have type(stack[-1]) == float and type(stack[-2]) == float\n",
    "                    b = stack.pop()\n",
    "                    a = stack.pop()\n",
    "                    stack.append(apply_op(a,b, sym))\n",
    "\n",
    "            if valid:\n",
    "                scores.append(0.0 if depth >= 1 else float('-inf'))\n",
    "        return torch.tensor(scores, dtype=torch.float)\n",
    "\n",
    "\n",
    "    def complete(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Given the token IDs of a complete (EOS-terminated) sequence (input_ids), \n",
    "        return the score associated by the constraint.\n",
    "        \"\"\"\n",
    "        # Decode all sequences at once\n",
    "        texts = self.tokenizer.batch_decode(\n",
    "            input_ids, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        scores = []\n",
    "        stack = []\n",
    "        variables = {}\n",
    "        for text in texts:\n",
    "            symbols = self._extract_symbols(text)\n",
    "            if symbols is None:             # text contains non-digit-nor-operator char => symbols = None \n",
    "                scores.append(float('-inf'))\n",
    "                continue\n",
    "            depth = 0\n",
    "            valid = True\n",
    "            for sym in symbols:\n",
    "                if sym.isdigit() or bool(_var_name.match(sym)):\n",
    "                    depth += 1\n",
    "                else:\n",
    "                    # operator\n",
    "                    if depth < 2:\n",
    "                        scores.append(float('-inf'))\n",
    "                        valid = False\n",
    "                        break\n",
    "                    depth -= 1\n",
    "                \n",
    "                if sym.isdigit():\n",
    "                    stack.append(float(sym))\n",
    "                elif bool(_var_name.match(sym)):\n",
    "                    stack.append(str(sym))\n",
    "                elif sym == \"=\":\n",
    "                    if type(stack[-2]) == float:\n",
    "                        valid = False\n",
    "                        break\n",
    "                    else: # stack[-2] is a string. We never append operators to the stack\n",
    "                        if type(stack[-1]) == float:    # stack[-1] is a number\n",
    "                            variables[stack[-2]] = stack[-1]\n",
    "                            new_value = stack[-1]\n",
    "                            _ = stack.pop()\n",
    "                            _ = stack.pop()\n",
    "                            stack.append(new_value)\n",
    "                        elif stack[-1] in variables:    # stack[-1] is a defined variable \n",
    "                            variables[stack[-2]] = variables[stack[-1]]\n",
    "                            new_value = variables[stack[-1]]\n",
    "                            _ = stack.pop()\n",
    "                            _ = stack.pop()\n",
    "                            stack.append(new_value)\n",
    "                        else :                          # stack[-1] is an undefined variable\n",
    "                            valid = False\n",
    "                            break\n",
    "                else:   # any operator that isn't \"=\"\n",
    "                    if stack[-1] or stack[-2] not in variables: # if there is an undefined variable\n",
    "                        valid = False\n",
    "                        break\n",
    "                    if stack[-1] in variables:                  # if defined, we convert variables into their value\n",
    "                        stack[-1] = variables[stack[-1]]\n",
    "                    elif stack[-2] in variables:                # if defined, we convert variables into their value\n",
    "                        stack[-2] = variables[stack[-2]] \n",
    "                    # Now we sould have type(stack[-1]) == float and type(stack[-2]) == float\n",
    "                    b = stack.pop()\n",
    "                    a = stack.pop()\n",
    "                    stack.append(apply_op(a,b, sym))\n",
    "\n",
    "            if valid:\n",
    "                scores.append(0.0 if depth == 1 else float('-inf'))\n",
    "        return torch.tensor(scores, dtype=torch.float)\n",
    "\n",
    "    def score(self, batch_ids: torch.LongTensor) -> torch.Tensor:\n",
    "            B, L = batch_ids.shape\n",
    "\n",
    "            # Boolean mask of which sequences end in EOS\n",
    "            is_eos = batch_ids[:, -1] == self.tokenizer.eos_token_id  # (B,)\n",
    "            idx_eos, idx_pref = torch.nonzero(is_eos, as_tuple=True)[0], torch.nonzero(~is_eos, as_tuple=True)[0]   # indices of sequences\n",
    "\n",
    "            # Prepare output: one score per sequence\n",
    "            out = torch.empty(B, device=batch_ids.device, dtype=torch.get_default_dtype())\n",
    "\n",
    "            # Score the non-EOS prefixes\n",
    "            if idx_pref.numel() > 0:\n",
    "                pref_inputs  = batch_ids[idx_pref]          # (Np, L)\n",
    "                pref_scores  = self.prefix(pref_inputs)     # (Np,) \n",
    "                out[idx_pref] = pref_scores\n",
    "\n",
    "            # Score the EOS-terminated sequences\n",
    "            if idx_eos.numel() > 0:\n",
    "                eos_inputs   = batch_ids[idx_eos]           # (Ne, L)\n",
    "                eos_scores   = self.complete(eos_inputs)    # (Ne,) \n",
    "                out[idx_eos] = eos_scores\n",
    "\n",
    "            return out\n",
    "    \n",
    "    def process_logits(self, input_ids: torch.LongTensor, logits: torch.FloatTensor) -> torch.Tensor:\n",
    "        # first apply your FSM-based masking\n",
    "        \n",
    "        process_logits1 = self.fsm_digitsvar.process_logits(input_ids, logits)\n",
    "\n",
    "        device, dtype = logits.device, logits.dtype\n",
    "\n",
    "\n",
    "        # ensure batch dimension\n",
    "        if logits.dim() == 1:\n",
    "            process_logits1 = process_logits1.unsqueeze(0)\n",
    "        B, V = process_logits1.shape\n",
    "\n",
    "        if input_ids is None or input_ids.numel() == 0:\n",
    "            input_ids = torch.empty((B, 0), dtype=torch.long, device=device)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "        mask = torch.zeros_like(process_logits1, dtype=torch.float32, device=device)\n",
    "        \n",
    "        for b in range(B):\n",
    "            # pick all tokens whose logit != -inf (i.e. not already fully masked)\n",
    "            candidate_tokens = torch.nonzero(\n",
    "                process_logits1[b] != float(\"-inf\"),\n",
    "                as_tuple=True\n",
    "            )[0]\n",
    "            if candidate_tokens.numel() == 0:\n",
    "                print(f\"for batch {b}, all tokens are already masked\")\n",
    "                continue\n",
    "\n",
    "            # build (prefix + each candidate) batches\n",
    "            seq = input_ids[b]                          # (seqlen,)\n",
    "            reps = candidate_tokens.size(0)             # # of candidates\n",
    "            seqs = seq.unsqueeze(0).repeat(reps, 1)     # (reps, seqlen)\n",
    "            next_ids = candidate_tokens.unsqueeze(1)    # (reps, 1)\n",
    "            batch = torch.cat([seqs, next_ids], dim=1)  # (reps, seqlen+1)\n",
    "\n",
    "            # score each continuation: 0 or -inf\n",
    "            scores = self.score(batch)                       # (reps,)\n",
    "\n",
    "            # place them into mask\n",
    "            mask[b, candidate_tokens] = scores\n",
    "\n",
    "        # add your mask so that disallowed tokens go to -inf\n",
    "        processed_logits = process_logits1 + mask     # (B, V)\n",
    "        return processed_logits\n",
    "\n",
    "\n",
    "def apply_op(a: float, b: float, op: str) -> float:\n",
    "    if op == '+':\n",
    "        return a + b\n",
    "    elif op == '-':\n",
    "        return a - b\n",
    "    elif op == '*':\n",
    "        return a * b\n",
    "    elif op == '/':\n",
    "        return a / b\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported operator {op!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed491538",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_rpntyped = [\n",
    "    \"Example 1:\\nInput: foo = 4, (3 + foo) * 5\\nOutput: 3 foo 4 = + 5 *\\n\\nExample 2:\\nInput: bar = 3, 7 - (2 + bar) * 4\\nOutput: 7 2 bar 3 = + 4 * -\\n\\nExample 3:\\nInput: foofoo = 2, 8 + (foofoo * (4 - 1))\\nOutput:\", \n",
    "    \"Example 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: bar = 3,  7 - (2 + bar) * 4\\nOutput: 7 2 bar 3 = + 4 * -\\n\\nExample 3:\\nInput: foofoo = 5, (3 + 4) * foofoo − 6 \\nOutput:\", \n",
    "]\n",
    "batch_rpntyped = qwenllm.tokenizer(prompts_rpntyped, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e61fd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpntyped_c = RPNTypedLogitsProcessor(qwenllm.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b9c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling...\n",
      "1\n",
      "Resampling...\n",
      "2\n",
      "Resampling...\n",
      "3\n",
      "Resampling...\n",
      "4\n",
      "Resampling...\n",
      "5\n",
      "Resampling...\n",
      "6\n",
      "Resampling...\n",
      "7\n",
      "Resampling...\n",
      "8\n",
      "Resampling...\n",
      "9\n",
      "Resampling...\n",
      "--------------- \n",
      " Example 1:\n",
      "Input: foo = 4, (3 + foo) * 5\n",
      "Output: 3 foo 4 = + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3, 7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 2, 8 + (foofoo * (4 - 1))\n",
      "Output: Henceforth the enemy is unable to relate to a\n",
      "--------------- \n",
      " Example 1:\n",
      "Input: foo = 4, (3 + foo) * 5\n",
      "Output: 3 foo 4 = + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3, 7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 2, 8 + (foofoo * (4 - 1))\n",
      "Output: Henceforth the enemy is unable to find any weapon\n",
      "--------------- \n",
      " Example 1:\n",
      "Input: foo = 4, (3 + foo) * 5\n",
      "Output: 3 foo 4 = + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3, 7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 2, 8 + (foofoo * (4 - 1))\n",
      "Output: Henceforth the enemy is unable to find any door\n",
      "--------------- \n",
      " Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3,  7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 5, (3 + 4) * foofoo − 6 \n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>FindTestResultException 10 3 b\n",
      "--------------- \n",
      " Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3,  7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 5, (3 + 4) * foofoo − 6 \n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>FindTestResultException 10 3 b\n",
      "--------------- \n",
      " Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3,  7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 5, (3 + 4) * foofoo − 6 \n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>FindTestResultException 10 3 \n"
     ]
    }
   ],
   "source": [
    "smc_sampler_rpntyped = clm.SMCSampler(qwenllm, rpntyped_c)\n",
    "num_particles = 3\n",
    "max_length = 10\n",
    "B=len(prompts_rpntyped)\n",
    "smc_generated_token_ids_rpntyped = smc_sampler_rpntyped.sample(batch_rpntyped.input_ids, max_length=max_length, num_particles=num_particles, ess_threshold=10)\n",
    "for a,b in zip(qwenllm.tokenizer.batch_decode(batch_rpntyped.input_ids.repeat_interleave(num_particles, dim=0)), qwenllm.tokenizer.batch_decode(smc_generated_token_ids_rpntyped[0].reshape(B*num_particles,max_length))):\n",
    "    print(\"--------------- \\n\",a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447f2b1",
   "metadata": {},
   "source": [
    "# RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa7e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_number_or_op = re.compile(r\"\\d+|[+\\-*/]\")\n",
    "\n",
    "class RPNLogitsProcessor(outlines.processors.base_logits_processor.OutlinesLogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fsm_digits = outlines.processors.RegexLogitsProcessor(r\"(\\d+|[ +\\-*/])+\", qwenllm_outlines.tokenizer)\n",
    "\n",
    "    def _extract_symbols(self, text: str) -> Optional[List[str]]:\n",
    "        \"\"\"\n",
    "        Parse `text` into a list of RPN tokens (integers and +, -, *, /).\n",
    "        Returns None if any invalid characters are present.\n",
    "        \"\"\"\n",
    "        matches = list(_number_or_op.finditer(text))    # list of re.Match objects (it contains the substring, the position of the beginning and the end of the substring)\n",
    "        symbols = [m.group(0) for m in matches]         # list of substrings that match \\d or [+\\-*/]\n",
    "        cleaned = _number_or_op.sub(\"\", text)           # remove the match from text, we should have \"   \" only\n",
    "        if cleaned.replace(\" \", \"\"):                    # we should obtain \"\", otherwise text contained a non-digit-nor-operator char\n",
    "            return None\n",
    "        return symbols\n",
    "\n",
    "    def prefix(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Given the token IDs generated so far (input_ids), \n",
    "        return the score associated by the constraint.\n",
    "        \"\"\"\n",
    "        # Decode all sequences at once\n",
    "        texts = self.tokenizer.batch_decode(\n",
    "            input_ids, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        scores = []\n",
    "        for text in texts:\n",
    "            if not text:\n",
    "                scores.append(0.0)      # empty context is always a valid prefix\n",
    "                continue\n",
    "            symbols = self._extract_symbols(text)\n",
    "            if symbols is None:             # text contains non-digit-nor-operator char => symbols = None \n",
    "                scores.append(float('-inf'))\n",
    "                continue\n",
    "            depth = 0\n",
    "            valid = True\n",
    "            for sym in symbols:\n",
    "                if sym.isdigit():\n",
    "                    depth += 1\n",
    "                else:\n",
    "                    # operator\n",
    "                    if depth < 2:\n",
    "                        scores.append(float('-inf'))\n",
    "                        valid = False\n",
    "                        break\n",
    "                    depth -= 1\n",
    "            if valid:\n",
    "                scores.append(0.0 if depth >= 1 else float('-inf'))\n",
    "        return torch.tensor(scores, dtype=torch.float)\n",
    "\n",
    "    def complete(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Given the token IDs of a complete (EOS-terminated) sequence (input_ids), \n",
    "        return the score associated by the constraint.\n",
    "        \"\"\"\n",
    "        texts = self.tokenizer.batch_decode(\n",
    "            input_ids, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        scores = []\n",
    "        for text in texts:\n",
    "            symbols = self._extract_symbols(text)\n",
    "            if symbols is None:\n",
    "                scores.append(float('-inf'))\n",
    "                continue\n",
    "            depth = 0\n",
    "            valid = True\n",
    "            for sym in symbols:\n",
    "                if sym.isdigit():\n",
    "                    depth += 1\n",
    "                else:\n",
    "                    # operator\n",
    "                    if depth < 2:\n",
    "                        scores.append(float('-inf'))\n",
    "                        valid = False\n",
    "                        break\n",
    "                    depth -= 1\n",
    "            if valid:\n",
    "                scores.append(0.0 if depth == 1 else float('-inf'))\n",
    "        return torch.tensor(scores, dtype=torch.float)\n",
    "\n",
    "    def score(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        For each sequence in `input_ids`, apply `complete` if it ends with EOS, else `prefix`.\n",
    "        Vectorized selection via torch.where; no explicit Python loop over batch.\n",
    "        \"\"\"\n",
    "        B, L = input_ids.shape\n",
    "\n",
    "        # Boolean mask of which sequences end in EOS\n",
    "        is_eos = input_ids[:, -1] == self.tokenizer.eos_token_id  # (B,)\n",
    "        idx_eos, idx_pref = torch.nonzero(is_eos, as_tuple=True)[0], torch.nonzero(~is_eos, as_tuple=True)[0]   # indices of sequences\n",
    "\n",
    "        # Prepare output: one score per sequence\n",
    "        out = torch.empty(B, device=input_ids.device, dtype=torch.get_default_dtype())\n",
    "\n",
    "        # Score the non-EOS prefixes\n",
    "        if idx_pref.numel() > 0:\n",
    "            pref_inputs  = input_ids[idx_pref]          # (Np, L)\n",
    "            pref_scores  = self.prefix(pref_inputs)     # (Np,) \n",
    "            out[idx_pref] = pref_scores\n",
    "\n",
    "        # Score the EOS-terminated sequences\n",
    "        if idx_eos.numel() > 0:\n",
    "            eos_inputs   = input_ids[idx_eos]           # (Ne, L)\n",
    "            eos_scores   = self.complete(eos_inputs)    # (Ne,) \n",
    "            out[idx_eos] = eos_scores\n",
    "\n",
    "        return out  # shape: (B,)\n",
    "\n",
    "    def process_logits(self, input_ids: torch.LongTensor, logits: torch.FloatTensor) -> torch.Tensor:\n",
    "        # First apply the regex‐based FSM masking\n",
    "        logits_after_fsm = self.fsm_digits.process_logits(input_ids, logits)\n",
    "\n",
    "        device, dtype = logits.device, logits.dtype\n",
    "\n",
    "        # ensure batch dim\n",
    "        if logits_after_fsm.dim() == 1:\n",
    "            logits_after_fsm = logits_after_fsm.unsqueeze(0)\n",
    "        B, V = logits_after_fsm.shape\n",
    "\n",
    "        if input_ids is None or input_ids.numel() == 0:\n",
    "            input_ids = torch.empty((B, 0), dtype=torch.long, device=device)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "        # build mask of shape (B, V)\n",
    "        mask = torch.zeros_like(logits_after_fsm, dtype=torch.float32, device=device)\n",
    "        \n",
    "        for b in range(B):\n",
    "            # tokens not already fully masked\n",
    "            candidates = torch.nonzero(\n",
    "                logits_after_fsm[b] != float(\"-inf\"),\n",
    "                as_tuple=True\n",
    "            )[0]\n",
    "            if candidates.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # build batch of prefix + each candidate token\n",
    "            prefix = input_ids[b]                              # (L,)\n",
    "            reps = candidates.size(0)                          # R\n",
    "            prefixes = prefix.unsqueeze(0).repeat(reps, 1)     # (R, L)\n",
    "            next_ids = candidates.unsqueeze(1)                 # (R, 1)\n",
    "            batch = torch.cat([prefixes, next_ids], dim=1)     # (R, L+1)\n",
    "\n",
    "            # compute 0 or -inf per continuation\n",
    "            sc = self.score(batch)                                  # (R,)\n",
    "\n",
    "            # fill mask\n",
    "            mask[b, candidates] = sc\n",
    "\n",
    "        # apply mask to logits\n",
    "        return logits_after_fsm + mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def score(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "    #     \"\"\"\n",
    "    #     Score every sequence in `input_ids` (shape: B × L).\n",
    "\n",
    "    #     • If the last token *is* EOS → treat sequence as **complete**  \n",
    "    #       valid IFF final stack depth == 1\n",
    "\n",
    "    #     • Otherwise                       → treat sequence as **prefix**  \n",
    "    #       valid IFF final stack depth ≥ 1 (empty prefix is allowed)\n",
    "\n",
    "    #     Returns a tensor of shape (B,) filled with `0.0` for valid and\n",
    "    #     `-inf` for invalid sequences.\n",
    "    #     \"\"\"\n",
    "    #     device = input_ids.device\n",
    "    #     batch  = input_ids.size(0)\n",
    "\n",
    "    #     # 1. Which rows are complete?\n",
    "    #     is_complete = input_ids[:, -1] == self.tokenizer.eos_token_id   # (B,)\n",
    "\n",
    "    #     # 2. Decode once for the whole batch\n",
    "    #     texts = self.tokenizer.batch_decode(\n",
    "    #         input_ids, clean_up_tokenization_spaces=False\n",
    "    #     )\n",
    "\n",
    "    #     out = torch.empty(batch, dtype=torch.float, device=device)\n",
    "\n",
    "    #     # 3. Per-sequence check (O(#tokens) but only a single small Python loop)\n",
    "    #     for i, text in enumerate(texts):\n",
    "\n",
    "    #         # ------------------------------------ fast-path: empty *prefix*\n",
    "    #         if not is_complete[i] and text == \"\":\n",
    "    #             out[i] = 0.0\n",
    "    #             continue\n",
    "\n",
    "    #         symbols = self._extract_symbols(text)\n",
    "    #         if symbols is None:                    # illegal character\n",
    "    #             out[i] = float(\"-inf\")\n",
    "    #             continue\n",
    "\n",
    "    #         depth, valid = 0, True\n",
    "    #         for sym in symbols:\n",
    "    #             if sym.isdigit():                  # push\n",
    "    #                 depth += 1\n",
    "    #             else:                              # operator: needs ≥2 operands\n",
    "    #                 if depth < 2:\n",
    "    #                     valid = False\n",
    "    #                     break\n",
    "    #                 depth -= 1                     # pop two, push one\n",
    "\n",
    "    #         if not valid:\n",
    "    #             out[i] = float(\"-inf\")\n",
    "    #             continue\n",
    "\n",
    "    #         # ------------------------------------ final stack check\n",
    "    #         if is_complete[i]:                     # complete expression\n",
    "    #             out[i] = 0.0 if depth == 1 else float(\"-inf\")\n",
    "    #         else:                                  # still generating (prefix)\n",
    "    #             out[i] = 0.0 if depth >= 1 else float(\"-inf\")\n",
    "\n",
    "    #     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff076c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_rpn = [\n",
    "    \"Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\\n\\nExample 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: 7 - (2 + 3) * 4\\nOutput: 7 2 3 + 4 * -\\n\\nExample 3:\\nInput: (8 / 2) + (3 * (4 - 1))\\nOutput:\", \n",
    "    \"Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\\n\\nExample 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: 7 - (2 + 3) * 4\\nOutput: 7 2 3 + 4 * -\\n\\nExample 3:\\nInput: (3 + 4) * 5 − 6 / (1 + 2)\\nOutput:\", \n",
    "    \"Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\\n\\nExample 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: 7 - (2 + 3) * 4\\nOutput: 7 2 3 + 4 * -\\n\\nExample 3:\\nInput: (6 + 2) * 3 − 4\\nOutput:\", \n",
    "]\n",
    "batch_rpn = qwenllm.tokenizer(prompts_rpn, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5afbc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_c = RPNLogitsProcessor(qwenllm.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8432dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling...\n",
      "1\n",
      "Resampling...\n",
      "2\n",
      "Resampling...\n",
      "3\n",
      "Resampling...\n",
      "4\n",
      "Resampling...\n",
      "5\n",
      "Resampling...\n",
      "6\n",
      "Resampling...\n",
      "7\n",
      "Resampling...\n",
      "8\n",
      "Resampling...\n",
      "9\n",
      "Resampling...\n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (8 / 2) + (3 * (4 - 1))\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|>2 4 - 1 + 3 *\n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (8 / 2) + (3 * (4 - 1))\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|>2 4 - 1 + 3 *\n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (8 / 2) + (3 * (4 - 1))\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|>2 4 - 1 + 3 *\n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (3 + 4) * 5 − 6 / (1 + 2)\n",
      "Output:3 4 + 5 6 * -\n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (3 + 4) * 5 − 6 / (1 + 2)\n",
      "Output:3 4 + 5 6 1\n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (3 + 4) * 5 − 6 / (1 + 2)\n",
      "Output:3 4 + 5 6 1\n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (6 + 2) * 3 − 4\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>1 2 3 + 4 -  \n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (6 + 2) * 3 − 4\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>1 2 3 + 4 -  \n",
      "--------------- \n",
      " Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (6 + 2) * 3 − 4\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>1 2 3 + 4 * -\n"
     ]
    }
   ],
   "source": [
    "smc_sampler_rpn = clm.SMCSampler(qwenllm, rpn_c)\n",
    "num_particles = 3\n",
    "max_length = 10\n",
    "B=len(prompts_rpn)\n",
    "smc_generated_token_ids_rpn = smc_sampler_rpn.sample(batch_rpn.input_ids, max_length=max_length, num_particles=num_particles, ess_threshold=10)\n",
    "for a,b in zip(qwenllm.tokenizer.batch_decode(batch_rpn.input_ids.repeat_interleave(num_particles, dim=0)), qwenllm.tokenizer.batch_decode(smc_generated_token_ids_rpn[0].reshape(B*num_particles,max_length))):\n",
    "    print(\"--------------- \\n\",a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b530f",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33980e4",
   "metadata": {},
   "source": [
    "Build dataset: infix <=> RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5d4106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8*(8/5*1*23))  ->  8 8 5 / 1 * 23 * *\n",
      "4+2+54+2/41  ->  4 2 + 54 + 2 41 / +\n",
      "(2-2/2+2)  ->  2 2 2 / - 2 +\n",
      "(44+((14*29)*(9/1)))  ->  44 14 29 * 9 1 / * +\n",
      "((6-73)-(48-(16-46)))  ->  6 73 - 48 16 46 - - -\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "OPERATORS  = ['+', '-', '*', '/']\n",
    "MAX_NUMBER = 99\n",
    "\n",
    "WEIGHTS = [1.0 / n for n in range(1, MAX_NUMBER + 1)]\n",
    "NUMBER_RANGE = list(range(1, MAX_NUMBER + 1))\n",
    "\n",
    "def sample_number():\n",
    "    return str(random.choices(NUMBER_RANGE, weights=WEIGHTS, k=1)[0])\n",
    "\n",
    "def gen_expr(depth=0, max_depth=3, parent_op=None):\n",
    "    # Base case: emit a number\n",
    "    if depth >= max_depth or random.random() < 0.3:\n",
    "        return sample_number()\n",
    "    # Otherwise pick an operator (ban '/' if parent was '/')\n",
    "    possible_ops = OPERATORS.copy()\n",
    "    if parent_op == '/':\n",
    "        possible_ops.remove('/')\n",
    "    op = random.choice(possible_ops)\n",
    "    # Build two sub-expressions\n",
    "    left  = gen_expr(depth+1, max_depth, parent_op=op)\n",
    "    right = gen_expr(depth+1, max_depth, parent_op=op)\n",
    "    expr = f\"{left}{op}{right}\"\n",
    "    # Randomly wrap in parentheses\n",
    "    if random.random() < 0.5:\n",
    "        expr = f\"({expr})\"\n",
    "    return expr\n",
    "\n",
    "def tokenize(expr):\n",
    "    return re.findall(r'\\d+|[()+\\-*/]', expr)\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    prec = {'+':1, '-':1, '*':2, '/':2}\n",
    "    out, stack = [], []\n",
    "    for t in tokens:\n",
    "        if re.fullmatch(r'\\d+', t):\n",
    "            out.append(t)\n",
    "        elif t == '(':\n",
    "            stack.append(t)\n",
    "        elif t == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                out.append(stack.pop())\n",
    "            stack.pop()\n",
    "        else:  # operator\n",
    "            while stack and stack[-1] != '(' and prec[t] <= prec[stack[-1]]:\n",
    "                out.append(stack.pop())\n",
    "            stack.append(t)\n",
    "    out.extend(stack[::-1])\n",
    "    return out\n",
    "\n",
    "def build_dataset(N, max_depth=3):\n",
    "    dataset = []\n",
    "    while len(dataset) < N:\n",
    "        infix = gen_expr(max_depth=max_depth)\n",
    "        # reject if no operator at all\n",
    "        if not re.search(r'[+\\-*/]', infix):\n",
    "            continue\n",
    "        tokens = tokenize(infix)\n",
    "        postfix = ' '.join(infix_to_postfix(tokens))\n",
    "        dataset.append((infix, postfix))\n",
    "    return dataset\n",
    "\n",
    "samples = build_dataset(5, max_depth=3)\n",
    "for infix, postfix in samples:\n",
    "    print(f\"{infix}  ->  {postfix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674756f4",
   "metadata": {},
   "source": [
    "Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "samples = build_dataset(10000, max_depth=2)\n",
    "\n",
    "data_dicts = [{\"infix\": infix, \"postfix\": postfix} for infix, postfix in samples]\n",
    "\n",
    "ds = Dataset.from_list(data_dicts)\n",
    "\n",
    "# Create prompt/completion strings\n",
    "def make_io(example):\n",
    "    prompt     = \"Translate infix to RPN:\\n\" + example[\"infix\"] + \"\\nRPN:\"\n",
    "    completion = \" \" + example[\"postfix\"]  # leading space to separate from tokenizer's BOS\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "ds = ds.map(make_io, remove_columns=[\"infix\",\"postfix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ded50",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Make sure the tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "\n",
    "def compute_len(ex):\n",
    "    toks = tokenizer(ex[\"prompt\"], ex[\"completion\"], \n",
    "                     padding=False, truncation=False)\n",
    "    return {\"length\": len(toks[\"input_ids\"])}\n",
    "\n",
    "ds_lens = ds.map(compute_len, batched=False)\n",
    "max_len = max(ds_lens[\"length\"])\n",
    "print(f\"Longest example is {max_len} tokens\")\n",
    "\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    # Truncate/pad as needed; pick max_length to cover prompt+answer\n",
    "    tokens = tokenizer(\n",
    "        example[\"prompt\"],\n",
    "        example[\"completion\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "        truncation=False,\n",
    "    )\n",
    "    # We only want the model to compute loss on the completion part\n",
    "    input_ids  = tokens[\"input_ids\"]\n",
    "    labels     = input_ids.copy()\n",
    "    # mask out the prompt tokens with -100 so they don’t contribute to loss\n",
    "    prompt_len = len(tokenizer(example[\"prompt\"])[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": tokens[\"attention_mask\"], \"labels\": labels}\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_fn, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80306cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned-rpn\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c6e43",
   "metadata": {},
   "source": [
    "Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22487359",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./qwen-finetuned-rpn\")\n",
    "tokenizer.save_pretrained(\"./qwen-finetuned-rpn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09baa46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwenllm_rpn_ft = clm.TransformersLM(\"./qwen-finetuned-rpn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_c = RPNLogitsProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba280311",
   "metadata": {},
   "outputs": [],
   "source": [
    "smc_sampler_rpn_ft = clm.SMCSampler(qwenllm_rpn_ft, rpn_c)\n",
    "num_particles = 3\n",
    "max_length = 10\n",
    "B=len(prompts_rpn)\n",
    "smc_generated_token_ids_rpn_ft = smc_sampler_rpn_ft.sample(batch_rpn.input_ids, max_length=max_length, num_particles=num_particles, ess_threshold=10)\n",
    "for a,b in zip(qwenllm_rpn_ft.tokenizer.batch_decode(batch_rpn.input_ids.repeat_interleave(num_particles, dim=0)), qwenllm_rpn_ft.tokenizer.batch_decode(smc_generated_token_ids_rpn_ft[0].reshape(B*num_particles,max_length))):\n",
    "    print(\"--------------- \\n\",a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8223a11f",
   "metadata": {},
   "source": [
    "## Random model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ff95d",
   "metadata": {},
   "source": [
    "Compare SFT / LCD / SMC / Random model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a534024",
   "metadata": {},
   "source": [
    "With SMC: show how increasing the num_particles increases the log_prob of the more probable sentence. See if it improves result of RPN translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f1066",
   "metadata": {},
   "source": [
    "IMP : Do I need to implement a way to estimate the normalizing constant when doing rejection sampling?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
