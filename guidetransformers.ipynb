{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import constraintlm as clm\n",
    "import constraintlm.models.transformers\n",
    "import constraintlm.sampling.sequence_sampling.smc\n",
    "import constraintlm.sampling.sequence_sampling.multinomial\n",
    "import constraintlm.sampling.token_sampling.tokensampler\n",
    "import constraintlm.constraints.lengthword\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "qwenllm = clm.models.transformers.TransformersLM(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small intro to HF's transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġis', 'Ġa', 'Ġtest', ',', 'Ġright', '?', 'ĠLike', 'Ġthis', '.', '<|endoftext|>'] <class 'list'>\n",
      "[1986, 374, 264, 1273, 11, 1290, 30, 8909, 419, 13, 151643]\n",
      "['This', 'Ġis', 'Ġa', 'Ġtest', ',', 'Ġright', '?', 'ĠLike', 'Ġthis', '.', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "tokens = qwenllm.tokenizer.tokenize(\"This is a test, right? Like this.<|endoftext|>\", return_tensors=\"pt\")\n",
    "print(tokens, type(tokens))\n",
    "ids = qwenllm.tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "re_tokens = qwenllm.tokenizer.convert_ids_to_tokens(ids)\n",
    "print(re_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>'] [151643, 151644, 151645, 151646, 151647, 151648, 151649, 151650, 151651, 151652, 151653, 151654, 151655, 151656]\n",
      "<|endoftext|> 151643\n",
      "<|endoftext|> 151643\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(qwenllm.tokenizer.all_special_tokens, qwenllm.tokenizer.all_special_ids)\n",
    "print(qwenllm.tokenizer.eos_token, qwenllm.tokenizer.eos_token_id)\n",
    "print(qwenllm.tokenizer.pad_token, qwenllm.tokenizer.pad_token_id)\n",
    "print(qwenllm.tokenizer.bos_token, qwenllm.tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Here is the first sentence.<|endoftext|>\",     #here <|endoftext|> is converted into eos_token_id. \n",
    "    \"And the second sentence is here, and it is much longer\",\n",
    "    \"Finally we can see the third one\"\n",
    "]\n",
    "batch = qwenllm.tokenizer(\n",
    "    sentences,\n",
    "    padding=True,         # pad each up to the longest in this batch\n",
    "    truncation=True,      # (optional) cut off anything beyond max_length\n",
    "    max_length=32,        # (optional) force a maximum\n",
    "    return_tensors=\"pt\"   # PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below it is really weird: in the first sentence of batch, '<|endoftext|>' has the id corresponding to the eos_token_id. However, the attention_mask don't pad this token. But since all the same tokenized sentence have the same size, the tokenizer adds new '<|endoftext|>'. It means that creating the mask and padding with '<|endoftext|>' is two different things, and the attention mask only consider extra added '<|endoftext|>', not the ones that were already here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  8420,    374,    279,   1156,  11652,     13, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643],\n",
       "        [  3036,    279,   2086,  11652,    374,   1588,     11,    323,    432,\n",
       "            374,   1753,   5021],\n",
       "        [ 23949,    582,    646,   1490,    279,   4843,    825, 151643, 151643,\n",
       "         151643, 151643, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is the first sentence.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwenllm.tokenizer.batch_decode(batch.input_ids[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " 'Ġis',\n",
       " 'Ġthe',\n",
       " 'Ġfirst',\n",
       " 'Ġsentence',\n",
       " '.',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwenllm.tokenizer.convert_ids_to_tokens(batch.input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the vocab size of a HF transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two sources to obtain the vocab_size, and none of them corresponds to the shape of the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size: 151665\n",
      " Vocab_size bis: 151643\n",
      " Difference: 22\n",
      " Number of added tokens: 22\n",
      " Number of special tokens: 14\n",
      "Vocab_size ter: 151936\n",
      "Number of outputted logits: 151936\n",
      " New token ['ĠMessi']\n"
     ]
    }
   ],
   "source": [
    "# mapping: token string → integer ID\n",
    "vocab_dict = qwenllm.tokenizer.get_vocab()  \n",
    "vocab_size = len(vocab_dict)\n",
    "# is equal to len(tokenizer) which is equal to tokenizer.vocab_size + tokenizer.added_tokens_encoder\n",
    "\n",
    "vocab_size_bis = qwenllm.tokenizer.vocab_size\n",
    "\n",
    "\n",
    "vocab_size_ter = qwenllm.model.config.vocab_size\n",
    "\n",
    "inputs = qwenllm.tokenizer(\"The best football player in the world is Leo\", return_tensors=\"pt\")\n",
    "logits, _ = qwenllm.logits(inputs.input_ids)\n",
    "probs = torch.softmax(logits, dim=-1, dtype=torch.float) \n",
    "new_tok = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "print(f'Vocab_size: {vocab_size}\\n', f'Vocab_size bis: {vocab_size_bis}\\n', f'Difference: {vocab_size - vocab_size_bis}\\n', f'Number of added tokens: {len(qwenllm.tokenizer.added_tokens_encoder)}\\n', f'Number of special tokens: {len(qwenllm.tokenizer.all_special_tokens)}') \n",
    "print(f'Vocab_size ter: {vocab_size_ter}') \n",
    "print(f'Number of outputted logits: {logits.shape[1]}\\n', f'New token {qwenllm.tokenizer.convert_ids_to_tokens(new_tok)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â½Ĺ\n",
      "<|file_sep|>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(qwenllm.tokenizer.convert_ids_to_tokens(151642)) # the 151643th : the last base token\n",
    "\n",
    "print(qwenllm.tokenizer.convert_ids_to_tokens(151664)) # the 151665th : the last base + added token\n",
    "print(qwenllm.tokenizer.convert_ids_to_tokens(151665)) # the 151666th : None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18992.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "151936/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 3 vocab_size: \n",
    "* the base size (obtained via tokenizer.vocab_size) \n",
    "* the full size that containts the base size + all added tokens (it includes the special tokens) (obtained via len(tokenizer.get_vocab()) or len(tokenizer))\n",
    "* the embedding size or the logits/probs size: the last vector outputed by the forward of the model\n",
    "\n",
    "For optimization purposes, the last one must me a multiple of 8 (151936 = 8*18992). \n",
    "\n",
    "Theoretically, we could sample a token_id larger than the tokenizer length, and thus the id will be converted into None. (This is very unlikely, and even more if you are using a top_k / top_p).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Sampling Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_sentences = [\n",
    "    \"La révolution française\",\n",
    "    \"Aujourd'hui; maman est morte. Ou peut-être\",\n",
    "    \"The best basketball player of all time is Michael\"\n",
    "]\n",
    "tests101 = qwenllm.tokenizer(tests_sentences, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multin_sampler = clm.sampling.token_sampling.tokensampler.TokenSampler(top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La révolution française<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Vous', \"Aujourd'hui; maman est morte. Ou peut-être qu\", 'The best basketball player of all time is Michael<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Human']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tests101.input_ids\n",
    "attn_mask_0 = tests101.attention_mask\n",
    "\n",
    "# --- t=0 ---\n",
    "next_token_logits, past_key_values = qwenllm.logits(input_ids, attn_mask_0)\n",
    "attn_mask = torch.cat([attn_mask_0, torch.ones((len(tests_sentences), 1))], dim=-1)\n",
    "new_ids = multin_sampler.sample(next_token_logits)\n",
    "\n",
    "prompt_gen_ids = torch.cat([input_ids, new_ids], dim=-1)\n",
    "print(qwenllm.tokenizer.batch_decode(prompt_gen_ids))\n",
    "# --- end of t=0 ---\n",
    "\n",
    "gen_ids = new_ids.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La révolution française<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Vous êtes', \"Aujourd'hui; maman est morte. Ou peut-être qu'il\", 'The best basketball player of all time is Michael<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Human beings']\n",
      "['Vous êtes', \" qu'il\", 'Human beings']\n",
      "['La révolution française<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Vous êtes à', \"Aujourd'hui; maman est morte. Ou peut-être qu'il est\", 'The best basketball player of all time is Michael<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Human beings\\n']\n",
      "['Vous êtes à', \" qu'il est\", 'Human beings\\n']\n",
      "['La révolution française<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Vous êtes à la', \"Aujourd'hui; maman est morte. Ou peut-être qu'il est mort\", 'The best basketball player of all time is Michael<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Human beings\\nThe']\n",
      "['Vous êtes à la', \" qu'il est mort\", 'Human beings\\nThe']\n"
     ]
    }
   ],
   "source": [
    "for t in range(3):\n",
    "    next_token_logits, past_key_values = qwenllm.logits(new_ids, attn_mask, past_key_values)\n",
    "    attn_mask = torch.cat([attn_mask, torch.ones((len(tests_sentences), 1))], dim=-1)\n",
    "    new_ids = multin_sampler.sample(next_token_logits)\n",
    "\n",
    "    prompt_gen_ids = torch.cat([prompt_gen_ids, new_ids], dim=-1)\n",
    "    print(qwenllm.tokenizer.batch_decode(prompt_gen_ids))\n",
    "\n",
    "    gen_ids = torch.cat([gen_ids, new_ids], dim=-1)\n",
    "    print(qwenllm.tokenizer.batch_decode(gen_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A: 1800-1830 - 1830\\nA la suite de la chute des Habsbourg en', \" plus. Je suis désolé.\\nAujourd'hui; mon père est mort. Ou peut-être plus. Je suis désolé.\\n\", 'Find the answer to the following question, calculate the number of digits in the answer. The answer is 5.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']\n",
      "['La révolution française<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>A: 1800-1830 - 1830\\nA la suite de la chute des Habsbourg en', \"Aujourd'hui; maman est morte. Ou peut-être plus. Je suis désolé.\\nAujourd'hui; mon père est mort. Ou peut-être plus. Je suis désolé.\\n\", 'The best basketball player of all time is Michael<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Find the answer to the following question, calculate the number of digits in the answer. The answer is 5.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "sequence_sampler = clm.sampling.sequence_sampling.multinomial.MultinomialSeqSampler(qwenllm, multin_sampler)\n",
    "gen_ids = sequence_sampler.sample(input_ids, 30)\n",
    "\n",
    "prompt_gen_ids = torch.cat([input_ids, gen_ids], dim=-1)\n",
    "print(qwenllm.tokenizer.batch_decode(gen_ids))\n",
    "print(qwenllm.tokenizer.batch_decode(prompt_gen_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word5 = clm.constraints.lengthword.LengthWord(qwenllm, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-inf, -inf, -inf])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "score = word5.prefix(tests101.input_ids)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
