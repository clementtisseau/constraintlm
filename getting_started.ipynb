{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cleme\\anaconda3\\envs\\llmsmc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import constraintlm as clm\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConstraintLM tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a LM from your favorite package (`transformers`, `vllm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "qwenllm = clm.TransformersLM(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem about masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151665 151665\n"
     ]
    }
   ],
   "source": [
    "print(len(qwenllm.tokenizer.get_vocab()), qwenllm.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151643 151643\n",
      "151643 151643\n"
     ]
    }
   ],
   "source": [
    "print(qwenllm.tokenizer.eos_token_id, qwenllm.tokenizer.pad_token_id)\n",
    "print(qwenllm.model.config.eos_token_id, qwenllm.model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    43,    661,  36922,  82176,   4370,  12429,    572,    264, 151643]])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Ludwig Wittgenstein was a<|endoftext|>\"\n",
    "input_ids = qwenllm.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nos\n",
      " \n",
      "n\n",
      "o\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "numo = 11891\n",
    "print(qwenllm.tokenizer.decode(numo))\n",
    "for c in qwenllm.tokenizer.decode(numo):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    43,    661,  36922,  82176,   4370,  12429,    572,    264, 151643])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(qwenllm.tokenizer.batch_decode(torch.tensor([14])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwenllm.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ludwig Wittgenstein was a<|endoftext|>A. a man of']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = qwenllm.model.generate(input_ids, do_sample=True, max_new_tokens=5)\n",
    "qwenllm.tokenizer.batch_decode(outputs, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5312,  3.4844,  1.3984,  ..., -6.0938, -6.0938, -6.0938]],\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
    "output_logits = qwenllm.model(input_ids, attention_mask=attn_mask)\n",
    "output_logits.logits[...,-2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   43,   661, 36922, 82176,  4370, 12429,   572,   264]])\n",
      "tensor([[ 4.4688,  3.4219,  1.4062,  ..., -6.0625, -6.0625, -6.0625]],\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"Ludwig Wittgenstein was a\"\n",
    "input_ids2 = qwenllm.tokenizer(prompt2, return_tensors=\"pt\").input_ids\n",
    "print(input_ids2)\n",
    "output_logits2 = qwenllm.model(input_ids2)\n",
    "print(output_logits2.logits[...,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max abs diff: tensor(0.2500, dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "diff = (output_logits.logits[:, -2,:] - output_logits2.logits[:, -1,:]).abs().max()\n",
    "print(\"Max abs diff:\", diff)  # should be extremely close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.5312,  3.4844,  1.3984,  ..., -6.0938, -6.0938, -6.0938]],\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "(tensor([[11.0000, 11.8750, 15.6875,  ..., -4.2812, -4.2812, -4.2812]],\n",
      "       dtype=torch.bfloat16), <transformers.cache_utils.DynamicCache object at 0x000002B5CB867790>)\n",
      "tensor([[ 4.4688,  3.4219,  1.4062,  ..., -6.0625, -6.0625, -6.0625]],\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "(tensor([[ 4.4688,  3.4219,  1.4062,  ..., -6.0625, -6.0625, -6.0625]],\n",
      "       dtype=torch.bfloat16), <transformers.cache_utils.DynamicCache object at 0x000002B5F6C14110>)\n"
     ]
    }
   ],
   "source": [
    "print(output_logits.logits[:, -2,:])\n",
    "print(qwenllm.logits(input_ids, attn_mask))\n",
    "print(output_logits2.logits[:, -1,:])\n",
    "print(qwenllm.logits(input_ids2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"In July 1789 the French\", \n",
    "    \"The best basketball player of all time is Michael\",\n",
    "    \"Ludwig Wittgenstein was a\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   641,   5768,    220,     16,     22,     23,     24,    279,   8585],\n",
      "        [   785,   1850,  19240,   2781,    315,    678,    882,    374,   7937],\n",
      "        [    43,    661,  36922,  82176,   4370,  12429,    572,    264, 151643]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch = qwenllm.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "print(batch.input_ids, batch.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.9688,  3.9844,  0.9844,  ..., -4.0625, -4.0625, -4.0625],\n",
      "        [ 5.6250,  1.3125,  0.1367,  ..., -8.3125, -8.3125, -8.3125],\n",
      "        [11.0000, 11.8750, 15.6875,  ..., -4.2812, -4.2812, -4.2812]],\n",
      "       dtype=torch.bfloat16) torch.Size([3, 151936])\n"
     ]
    }
   ],
   "source": [
    "logits, past_key_values = qwenllm.logits(batch.input_ids, attention_mask=batch.attention_mask)\n",
    "print(logits, logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It works as usual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the logits of the next token..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and sample from the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Parliament', ' Jordan', 'Human']\n",
      "['In July 1789 the French Parliament', 'The best basketball player of all time is Michael Jordan', 'Ludwig Wittgenstein was a<|endoftext|>Human']\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(logits, dim=-1)\n",
    "next_token_ids = qwenllm.sample(probs, top_k=5)\n",
    "print(qwenllm.tokenizer.batch_decode(next_token_ids))\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, next_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample()` function allows you to sample the next token with a vast variety of techniques, by specifying the desired ones in the args.\n",
    "\n",
    "To sample whole sequences, you need to define a `SequenceSampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "['In July 1789 the French Parliament approved a bill that established a new constitution.', 'The best basketball player of all time is Michael Jordan.\\nMichael Jordan is the best known basketball player', \"Ludwig Wittgenstein was a<|endoftext|>Let's be clear: Wittgenstein's ideas\"]\n"
     ]
    }
   ],
   "source": [
    "multinomial = clm.MultinomialSeqSampler(qwenllm)\n",
    "generated_token_ids = multinomial.sample(batch.input_ids, 10, top_k = 5)\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, generated_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept of Constraint\n",
    "It allows you to constrain (control) the way the LM generate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, instantiate a `Constraint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word5 = clm.LengthWord(qwenllm, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows you to apply a constraint on the logits outputted by your LM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits, dim=-1)\n",
    "constrained_probs, _ = word5.apply(input_ids=torch.tensor([]), probs=probs) # for the 1st generated token, set input_ids as an empty tensor or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1905e-06, 8.1956e-07, 4.0745e-08,  ..., 2.6193e-10, 2.6193e-10,\n",
      "         2.6193e-10],\n",
      "        [4.1723e-06, 5.5647e-08, 1.7229e-08,  ..., 3.6948e-12, 3.6948e-12,\n",
      "         3.6948e-12],\n",
      "        [3.0708e-04, 7.4005e-04, 3.3447e-02,  ..., 7.0941e-11, 7.0941e-11,\n",
      "         7.0941e-11]], dtype=torch.bfloat16)\n",
      "tensor([[1.1504e-05, 4.2915e-06, 2.1420e-07,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [4.2915e-04, 5.7220e-06, 1.7732e-06,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [3.3569e-04, 8.0872e-04, 3.6621e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(probs)\n",
    "print(constrained_probs) #as we can see it is not the same as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and sample accordingly to this constraint (the generated tokens don't exceed 5 letters)... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' colon', ' Al', 'a']\n",
      "['In July 1789 the French colon', 'The best basketball player of all time is Michael Al', 'Ludwig Wittgenstein was a<|endoftext|>a']\n"
     ]
    }
   ],
   "source": [
    "next_const_token_ids = qwenllm.sample(constrained_probs)\n",
    "print(qwenllm.tokenizer.batch_decode(next_const_token_ids))\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, next_const_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or to sample whole sequences according to this constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In July 1789 the French army under the Duke', 'The best basketball player of all time is Michael Dwyer\\n', 'Ludwig Wittgenstein was a<|endoftext|>Human Logic and Lingu']\n"
     ]
    }
   ],
   "source": [
    "cons_multinomial = clm.MultinomialSeqSampler(qwenllm, constraint=word5)\n",
    "cons_generated_token_ids = cons_multinomial.sample(batch.input_ids, max_length=4, top_k=5)\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, cons_generated_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The sentence w/ L. Wittgenstein is padded by one eos_token (same as pad_token), and it performs consistently worse than the others by printing weird tokens after the eos_token. See answer above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Sequence Sampling technique, which consists in sampling tokens that respect the constraint one by one, called Locally Constraint decoding (LCD), is a little bit naive. Indeed, the sequences we are sampling don't follow the target distribution (distribution of sequences generated by the LMs and that respect the constraint): $\\frac{p(x) \\Phi(x)}{\\sum p(x) \\Phi(x)}$, where $\\Phi(x)$ is the non-negative value assigned by the constraint $\\Phi$ to the  sentence $x$.\n",
    "\n",
    "We can try to approximate this target distribution using Sequential Monte Carlo methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SMC Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "smc_sampler = clm.SMCSampler(qwenllm, word5)\n",
    "smc_generated_token_ids = smc_sampler.sample(batch.input_ids, max_length=5, num_particles=2, ess_threshold=1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1033,   911,   311,  3164,   320],\n",
       "         [13390,    11, 62857,   553,  2799]],\n",
       "\n",
       "        [[ 7801, 15640,    13,  1260,   572],\n",
       "         [97138,   702,   264,  2244,  2487]],\n",
       "\n",
       "        [[12895,   279,  3719,    71,  5235],\n",
       "         [33975, 13962,   374,   279,  1852]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smc_generated_token_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' were about to win (', ' army, aided by techn', ' James Jr. He was', ' Kidd has a great body', 'Could the Subh Black', 'Human evil is the same']\n"
     ]
    }
   ],
   "source": [
    "print(qwenllm.tokenizer.batch_decode(smc_generated_token_ids[0].reshape(6,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.1885, -3.4590],\n",
       "        [-5.6890, -6.9258],\n",
       "        [-6.4526, -4.8926]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smc_generated_token_ids[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSM Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex\n",
    "Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 14 characters are special unless back-slashed: .^$*+?{}[]\\|()\n",
    "\n",
    ". : any character except newline\n",
    "\n",
    "\\* : 0 or more\n",
    "\n",
    "\\+ : 1 or more\n",
    "\n",
    "? : 0 or 1\n",
    "\n",
    "{4} : exactly 4\n",
    "\n",
    "{4,6} : between 4 and 6 (inclusive)\n",
    "\n",
    "{4,} : 4 or more\n",
    "\n",
    "[abc] : “a”, “b”, or “c”\n",
    "\n",
    "[a-zA-Z0-9_] : any of those\n",
    "\n",
    "[^…] : negated set\n",
    "\n",
    "\\ : if it is \\\\. (or \\\\?, \\\\$, ...) it means a literal . (or ?, $, ...), but with non special characters, \n",
    "\n",
    "\\d : digit [0-9] (Unicode digits if UNICODE)\n",
    "\n",
    "\\D : not a digit\n",
    "\n",
    "\\w : word char [a-zA-Z0-9_]\n",
    "\n",
    "\\W : not word\n",
    "\n",
    "\\s : any whitespace ([ \\t\\n\\r\\f\\v], plus other Unicode spaces)\n",
    "\n",
    "\\S : not whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraint using FSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenWord(clm.FSMConstraint):\n",
    "    def __init__(self, llm, fsm):\n",
    "        super().__init__(llm, fsm)\n",
    "\n",
    "    def prefix():\n",
    "        pass\n",
    "    def complete():\n",
    "        pass\n",
    "\n",
    "    def score(self, input_ids):\n",
    "        \"\"\"\n",
    "        Given the token IDs generated so far (input_ids), \n",
    "        return the score associated by the constraint.\n",
    "        \"\"\"\n",
    "        if input_ids is None or input_ids.numel() == 0:\n",
    "            return 1\n",
    "        # Flatten batch dimensions\n",
    "        batch_shape = input_ids.shape[:-1]\n",
    "        seq_length = input_ids.shape[-1]\n",
    "        flat_ids = input_ids.view(-1, seq_length)\n",
    "        \n",
    "        # Decode each sequence to text\n",
    "        #Big issue here, a EOS_token such as <|endoftext|> will count as a long word\n",
    "        texts = self.model.tokenizer.batch_decode(flat_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Determine penalty per sequence\n",
    "        scores = []\n",
    "        for text in texts:\n",
    "            # Split on whitespace to get words\n",
    "            words = text.split()\n",
    "            # Check if any word is too long\n",
    "            if any(len(word) > self.N for word in words):\n",
    "                scores.append(0.0)\n",
    "            else:\n",
    "                scores.append(1.0)\n",
    "        \n",
    "        # Convert to tensor and reshape to original batch shape\n",
    "        scores = torch.tensor(scores, dtype=torch.float, device=input_ids.device)\n",
    "        return scores.view(batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(IN, [(CATEGORY, CATEGORY_SPACE)]), (MAX_REPEAT, (1, MAXREPEAT, [(SUBPATTERN, (1, 0, 0, [(MAX_REPEAT, (1, 5, [(IN, [(RANGE, (65, 90)), (RANGE, (97, 122)), (RANGE, (48, 57))])])), (MAX_REPEAT, (0, 1, [(IN, [(LITERAL, 46), (LITERAL, 33), (LITERAL, 63), (LITERAL, 44)])])), (IN, [(CATEGORY, CATEGORY_SPACE)])]))]))]\n",
      "{(0, '\\r'): {1}, (0, ' '): {1}, (0, '\\x0c'): {1}, (0, '\\t'): {1}, (0, '\\n'): {1}, (0, '\\x0b'): {1}, (1, ''): {34}, (34, ''): {4, 6, 10, 16, 24}, (4, 'j'): {5}, (4, '8'): {5}, (4, 'Q'): {5}, (4, 'E'): {5}, (4, 'i'): {5}, (4, 'x'): {5}, (4, 'X'): {5}, (4, 'e'): {5}, (4, 'D'): {5}, (4, 'G'): {5}, (4, 'U'): {5}, (4, 'A'): {5}, (4, 'r'): {5}, (4, 'a'): {5}, (4, '3'): {5}, (4, 'N'): {5}, (4, 's'): {5}, (4, 'h'): {5}, (4, 'c'): {5}, (4, 'L'): {5}, (4, 'y'): {5}, (4, 'l'): {5}, (4, '9'): {5}, (4, 'u'): {5}, (4, 'o'): {5}, (4, 'f'): {5}, (4, 'R'): {5}, (4, 'Z'): {5}, (4, '5'): {5}, (4, '2'): {5}, (4, 'm'): {5}, (4, 'P'): {5}, (4, 'k'): {5}, (4, 'S'): {5}, (4, 'K'): {5}, (4, 't'): {5}, (4, 'W'): {5}, (4, 'g'): {5}, (4, '6'): {5}, (4, 'w'): {5}, (4, 'M'): {5}, (4, 'v'): {5}, (4, 'T'): {5}, (4, 'z'): {5}, (4, 'n'): {5}, (4, '7'): {5}, (4, 'V'): {5}, (4, 'C'): {5}, (4, 'H'): {5}, (4, 'Y'): {5}, (4, 'p'): {5}, (4, 'I'): {5}, (4, 'J'): {5}, (4, 'O'): {5}, (4, '4'): {5}, (4, 'd'): {5}, (4, 'b'): {5}, (4, '1'): {5}, (4, 'q'): {5}, (4, '0'): {5}, (4, 'B'): {5}, (4, 'F'): {5}, (5, ''): {35}, (6, 'j'): {7}, (6, '8'): {7}, (6, 'Q'): {7}, (6, 'E'): {7}, (6, 'i'): {7}, (6, 'x'): {7}, (6, 'X'): {7}, (6, 'e'): {7}, (6, 'D'): {7}, (6, 'G'): {7}, (6, 'U'): {7}, (6, 'A'): {7}, (6, 'r'): {7}, (6, 'a'): {7}, (6, '3'): {7}, (6, 'N'): {7}, (6, 's'): {7}, (6, 'h'): {7}, (6, 'c'): {7}, (6, 'L'): {7}, (6, 'y'): {7}, (6, 'l'): {7}, (6, '9'): {7}, (6, 'u'): {7}, (6, 'o'): {7}, (6, 'f'): {7}, (6, 'R'): {7}, (6, 'Z'): {7}, (6, '5'): {7}, (6, '2'): {7}, (6, 'm'): {7}, (6, 'P'): {7}, (6, 'k'): {7}, (6, 'S'): {7}, (6, 'K'): {7}, (6, 't'): {7}, (6, 'W'): {7}, (6, 'g'): {7}, (6, '6'): {7}, (6, 'w'): {7}, (6, 'M'): {7}, (6, 'v'): {7}, (6, 'T'): {7}, (6, 'z'): {7}, (6, 'n'): {7}, (6, '7'): {7}, (6, 'V'): {7}, (6, 'C'): {7}, (6, 'H'): {7}, (6, 'Y'): {7}, (6, 'p'): {7}, (6, 'I'): {7}, (6, 'J'): {7}, (6, 'O'): {7}, (6, '4'): {7}, (6, 'd'): {7}, (6, 'b'): {7}, (6, '1'): {7}, (6, 'q'): {7}, (6, '0'): {7}, (6, 'B'): {7}, (6, 'F'): {7}, (7, ''): {8}, (8, 'j'): {9}, (8, '8'): {9}, (8, 'Q'): {9}, (8, 'E'): {9}, (8, 'i'): {9}, (8, 'x'): {9}, (8, 'X'): {9}, (8, 'e'): {9}, (8, 'D'): {9}, (8, 'G'): {9}, (8, 'U'): {9}, (8, 'A'): {9}, (8, 'r'): {9}, (8, 'a'): {9}, (8, '3'): {9}, (8, 'N'): {9}, (8, 's'): {9}, (8, 'h'): {9}, (8, 'c'): {9}, (8, 'L'): {9}, (8, 'y'): {9}, (8, 'l'): {9}, (8, '9'): {9}, (8, 'u'): {9}, (8, 'o'): {9}, (8, 'f'): {9}, (8, 'R'): {9}, (8, 'Z'): {9}, (8, '5'): {9}, (8, '2'): {9}, (8, 'm'): {9}, (8, 'P'): {9}, (8, 'k'): {9}, (8, 'S'): {9}, (8, 'K'): {9}, (8, 't'): {9}, (8, 'W'): {9}, (8, 'g'): {9}, (8, '6'): {9}, (8, 'w'): {9}, (8, 'M'): {9}, (8, 'v'): {9}, (8, 'T'): {9}, (8, 'z'): {9}, (8, 'n'): {9}, (8, '7'): {9}, (8, 'V'): {9}, (8, 'C'): {9}, (8, 'H'): {9}, (8, 'Y'): {9}, (8, 'p'): {9}, (8, 'I'): {9}, (8, 'J'): {9}, (8, 'O'): {9}, (8, '4'): {9}, (8, 'd'): {9}, (8, 'b'): {9}, (8, '1'): {9}, (8, 'q'): {9}, (8, '0'): {9}, (8, 'B'): {9}, (8, 'F'): {9}, (9, ''): {35}, (10, 'j'): {11}, (10, '8'): {11}, (10, 'Q'): {11}, (10, 'E'): {11}, (10, 'i'): {11}, (10, 'x'): {11}, (10, 'X'): {11}, (10, 'e'): {11}, (10, 'D'): {11}, (10, 'G'): {11}, (10, 'U'): {11}, (10, 'A'): {11}, (10, 'r'): {11}, (10, 'a'): {11}, (10, '3'): {11}, (10, 'N'): {11}, (10, 's'): {11}, (10, 'h'): {11}, (10, 'c'): {11}, (10, 'L'): {11}, (10, 'y'): {11}, (10, 'l'): {11}, (10, '9'): {11}, (10, 'u'): {11}, (10, 'o'): {11}, (10, 'f'): {11}, (10, 'R'): {11}, (10, 'Z'): {11}, (10, '5'): {11}, (10, '2'): {11}, (10, 'm'): {11}, (10, 'P'): {11}, (10, 'k'): {11}, (10, 'S'): {11}, (10, 'K'): {11}, (10, 't'): {11}, (10, 'W'): {11}, (10, 'g'): {11}, (10, '6'): {11}, (10, 'w'): {11}, (10, 'M'): {11}, (10, 'v'): {11}, (10, 'T'): {11}, (10, 'z'): {11}, (10, 'n'): {11}, (10, '7'): {11}, (10, 'V'): {11}, (10, 'C'): {11}, (10, 'H'): {11}, (10, 'Y'): {11}, (10, 'p'): {11}, (10, 'I'): {11}, (10, 'J'): {11}, (10, 'O'): {11}, (10, '4'): {11}, (10, 'd'): {11}, (10, 'b'): {11}, (10, '1'): {11}, (10, 'q'): {11}, (10, '0'): {11}, (10, 'B'): {11}, (10, 'F'): {11}, (11, ''): {12}, (12, 'j'): {13}, (12, '8'): {13}, (12, 'Q'): {13}, (12, 'E'): {13}, (12, 'i'): {13}, (12, 'x'): {13}, (12, 'X'): {13}, (12, 'e'): {13}, (12, 'D'): {13}, (12, 'G'): {13}, (12, 'U'): {13}, (12, 'A'): {13}, (12, 'r'): {13}, (12, 'a'): {13}, (12, '3'): {13}, (12, 'N'): {13}, (12, 's'): {13}, (12, 'h'): {13}, (12, 'c'): {13}, (12, 'L'): {13}, (12, 'y'): {13}, (12, 'l'): {13}, (12, '9'): {13}, (12, 'u'): {13}, (12, 'o'): {13}, (12, 'f'): {13}, (12, 'R'): {13}, (12, 'Z'): {13}, (12, '5'): {13}, (12, '2'): {13}, (12, 'm'): {13}, (12, 'P'): {13}, (12, 'k'): {13}, (12, 'S'): {13}, (12, 'K'): {13}, (12, 't'): {13}, (12, 'W'): {13}, (12, 'g'): {13}, (12, '6'): {13}, (12, 'w'): {13}, (12, 'M'): {13}, (12, 'v'): {13}, (12, 'T'): {13}, (12, 'z'): {13}, (12, 'n'): {13}, (12, '7'): {13}, (12, 'V'): {13}, (12, 'C'): {13}, (12, 'H'): {13}, (12, 'Y'): {13}, (12, 'p'): {13}, (12, 'I'): {13}, (12, 'J'): {13}, (12, 'O'): {13}, (12, '4'): {13}, (12, 'd'): {13}, (12, 'b'): {13}, (12, '1'): {13}, (12, 'q'): {13}, (12, '0'): {13}, (12, 'B'): {13}, (12, 'F'): {13}, (13, ''): {14}, (14, 'j'): {15}, (14, '8'): {15}, (14, 'Q'): {15}, (14, 'E'): {15}, (14, 'i'): {15}, (14, 'x'): {15}, (14, 'X'): {15}, (14, 'e'): {15}, (14, 'D'): {15}, (14, 'G'): {15}, (14, 'U'): {15}, (14, 'A'): {15}, (14, 'r'): {15}, (14, 'a'): {15}, (14, '3'): {15}, (14, 'N'): {15}, (14, 's'): {15}, (14, 'h'): {15}, (14, 'c'): {15}, (14, 'L'): {15}, (14, 'y'): {15}, (14, 'l'): {15}, (14, '9'): {15}, (14, 'u'): {15}, (14, 'o'): {15}, (14, 'f'): {15}, (14, 'R'): {15}, (14, 'Z'): {15}, (14, '5'): {15}, (14, '2'): {15}, (14, 'm'): {15}, (14, 'P'): {15}, (14, 'k'): {15}, (14, 'S'): {15}, (14, 'K'): {15}, (14, 't'): {15}, (14, 'W'): {15}, (14, 'g'): {15}, (14, '6'): {15}, (14, 'w'): {15}, (14, 'M'): {15}, (14, 'v'): {15}, (14, 'T'): {15}, (14, 'z'): {15}, (14, 'n'): {15}, (14, '7'): {15}, (14, 'V'): {15}, (14, 'C'): {15}, (14, 'H'): {15}, (14, 'Y'): {15}, (14, 'p'): {15}, (14, 'I'): {15}, (14, 'J'): {15}, (14, 'O'): {15}, (14, '4'): {15}, (14, 'd'): {15}, (14, 'b'): {15}, (14, '1'): {15}, (14, 'q'): {15}, (14, '0'): {15}, (14, 'B'): {15}, (14, 'F'): {15}, (15, ''): {35}, (16, 'j'): {17}, (16, '8'): {17}, (16, 'Q'): {17}, (16, 'E'): {17}, (16, 'i'): {17}, (16, 'x'): {17}, (16, 'X'): {17}, (16, 'e'): {17}, (16, 'D'): {17}, (16, 'G'): {17}, (16, 'U'): {17}, (16, 'A'): {17}, (16, 'r'): {17}, (16, 'a'): {17}, (16, '3'): {17}, (16, 'N'): {17}, (16, 's'): {17}, (16, 'h'): {17}, (16, 'c'): {17}, (16, 'L'): {17}, (16, 'y'): {17}, (16, 'l'): {17}, (16, '9'): {17}, (16, 'u'): {17}, (16, 'o'): {17}, (16, 'f'): {17}, (16, 'R'): {17}, (16, 'Z'): {17}, (16, '5'): {17}, (16, '2'): {17}, (16, 'm'): {17}, (16, 'P'): {17}, (16, 'k'): {17}, (16, 'S'): {17}, (16, 'K'): {17}, (16, 't'): {17}, (16, 'W'): {17}, (16, 'g'): {17}, (16, '6'): {17}, (16, 'w'): {17}, (16, 'M'): {17}, (16, 'v'): {17}, (16, 'T'): {17}, (16, 'z'): {17}, (16, 'n'): {17}, (16, '7'): {17}, (16, 'V'): {17}, (16, 'C'): {17}, (16, 'H'): {17}, (16, 'Y'): {17}, (16, 'p'): {17}, (16, 'I'): {17}, (16, 'J'): {17}, (16, 'O'): {17}, (16, '4'): {17}, (16, 'd'): {17}, (16, 'b'): {17}, (16, '1'): {17}, (16, 'q'): {17}, (16, '0'): {17}, (16, 'B'): {17}, (16, 'F'): {17}, (17, ''): {18}, (18, 'j'): {19}, (18, '8'): {19}, (18, 'Q'): {19}, (18, 'E'): {19}, (18, 'i'): {19}, (18, 'x'): {19}, (18, 'X'): {19}, (18, 'e'): {19}, (18, 'D'): {19}, (18, 'G'): {19}, (18, 'U'): {19}, (18, 'A'): {19}, (18, 'r'): {19}, (18, 'a'): {19}, (18, '3'): {19}, (18, 'N'): {19}, (18, 's'): {19}, (18, 'h'): {19}, (18, 'c'): {19}, (18, 'L'): {19}, (18, 'y'): {19}, (18, 'l'): {19}, (18, '9'): {19}, (18, 'u'): {19}, (18, 'o'): {19}, (18, 'f'): {19}, (18, 'R'): {19}, (18, 'Z'): {19}, (18, '5'): {19}, (18, '2'): {19}, (18, 'm'): {19}, (18, 'P'): {19}, (18, 'k'): {19}, (18, 'S'): {19}, (18, 'K'): {19}, (18, 't'): {19}, (18, 'W'): {19}, (18, 'g'): {19}, (18, '6'): {19}, (18, 'w'): {19}, (18, 'M'): {19}, (18, 'v'): {19}, (18, 'T'): {19}, (18, 'z'): {19}, (18, 'n'): {19}, (18, '7'): {19}, (18, 'V'): {19}, (18, 'C'): {19}, (18, 'H'): {19}, (18, 'Y'): {19}, (18, 'p'): {19}, (18, 'I'): {19}, (18, 'J'): {19}, (18, 'O'): {19}, (18, '4'): {19}, (18, 'd'): {19}, (18, 'b'): {19}, (18, '1'): {19}, (18, 'q'): {19}, (18, '0'): {19}, (18, 'B'): {19}, (18, 'F'): {19}, (19, ''): {20}, (20, 'j'): {21}, (20, '8'): {21}, (20, 'Q'): {21}, (20, 'E'): {21}, (20, 'i'): {21}, (20, 'x'): {21}, (20, 'X'): {21}, (20, 'e'): {21}, (20, 'D'): {21}, (20, 'G'): {21}, (20, 'U'): {21}, (20, 'A'): {21}, (20, 'r'): {21}, (20, 'a'): {21}, (20, '3'): {21}, (20, 'N'): {21}, (20, 's'): {21}, (20, 'h'): {21}, (20, 'c'): {21}, (20, 'L'): {21}, (20, 'y'): {21}, (20, 'l'): {21}, (20, '9'): {21}, (20, 'u'): {21}, (20, 'o'): {21}, (20, 'f'): {21}, (20, 'R'): {21}, (20, 'Z'): {21}, (20, '5'): {21}, (20, '2'): {21}, (20, 'm'): {21}, (20, 'P'): {21}, (20, 'k'): {21}, (20, 'S'): {21}, (20, 'K'): {21}, (20, 't'): {21}, (20, 'W'): {21}, (20, 'g'): {21}, (20, '6'): {21}, (20, 'w'): {21}, (20, 'M'): {21}, (20, 'v'): {21}, (20, 'T'): {21}, (20, 'z'): {21}, (20, 'n'): {21}, (20, '7'): {21}, (20, 'V'): {21}, (20, 'C'): {21}, (20, 'H'): {21}, (20, 'Y'): {21}, (20, 'p'): {21}, (20, 'I'): {21}, (20, 'J'): {21}, (20, 'O'): {21}, (20, '4'): {21}, (20, 'd'): {21}, (20, 'b'): {21}, (20, '1'): {21}, (20, 'q'): {21}, (20, '0'): {21}, (20, 'B'): {21}, (20, 'F'): {21}, (21, ''): {22}, (22, 'j'): {23}, (22, '8'): {23}, (22, 'Q'): {23}, (22, 'E'): {23}, (22, 'i'): {23}, (22, 'x'): {23}, (22, 'X'): {23}, (22, 'e'): {23}, (22, 'D'): {23}, (22, 'G'): {23}, (22, 'U'): {23}, (22, 'A'): {23}, (22, 'r'): {23}, (22, 'a'): {23}, (22, '3'): {23}, (22, 'N'): {23}, (22, 's'): {23}, (22, 'h'): {23}, (22, 'c'): {23}, (22, 'L'): {23}, (22, 'y'): {23}, (22, 'l'): {23}, (22, '9'): {23}, (22, 'u'): {23}, (22, 'o'): {23}, (22, 'f'): {23}, (22, 'R'): {23}, (22, 'Z'): {23}, (22, '5'): {23}, (22, '2'): {23}, (22, 'm'): {23}, (22, 'P'): {23}, (22, 'k'): {23}, (22, 'S'): {23}, (22, 'K'): {23}, (22, 't'): {23}, (22, 'W'): {23}, (22, 'g'): {23}, (22, '6'): {23}, (22, 'w'): {23}, (22, 'M'): {23}, (22, 'v'): {23}, (22, 'T'): {23}, (22, 'z'): {23}, (22, 'n'): {23}, (22, '7'): {23}, (22, 'V'): {23}, (22, 'C'): {23}, (22, 'H'): {23}, (22, 'Y'): {23}, (22, 'p'): {23}, (22, 'I'): {23}, (22, 'J'): {23}, (22, 'O'): {23}, (22, '4'): {23}, (22, 'd'): {23}, (22, 'b'): {23}, (22, '1'): {23}, (22, 'q'): {23}, (22, '0'): {23}, (22, 'B'): {23}, (22, 'F'): {23}, (23, ''): {35}, (24, 'j'): {25}, (24, '8'): {25}, (24, 'Q'): {25}, (24, 'E'): {25}, (24, 'i'): {25}, (24, 'x'): {25}, (24, 'X'): {25}, (24, 'e'): {25}, (24, 'D'): {25}, (24, 'G'): {25}, (24, 'U'): {25}, (24, 'A'): {25}, (24, 'r'): {25}, (24, 'a'): {25}, (24, '3'): {25}, (24, 'N'): {25}, (24, 's'): {25}, (24, 'h'): {25}, (24, 'c'): {25}, (24, 'L'): {25}, (24, 'y'): {25}, (24, 'l'): {25}, (24, '9'): {25}, (24, 'u'): {25}, (24, 'o'): {25}, (24, 'f'): {25}, (24, 'R'): {25}, (24, 'Z'): {25}, (24, '5'): {25}, (24, '2'): {25}, (24, 'm'): {25}, (24, 'P'): {25}, (24, 'k'): {25}, (24, 'S'): {25}, (24, 'K'): {25}, (24, 't'): {25}, (24, 'W'): {25}, (24, 'g'): {25}, (24, '6'): {25}, (24, 'w'): {25}, (24, 'M'): {25}, (24, 'v'): {25}, (24, 'T'): {25}, (24, 'z'): {25}, (24, 'n'): {25}, (24, '7'): {25}, (24, 'V'): {25}, (24, 'C'): {25}, (24, 'H'): {25}, (24, 'Y'): {25}, (24, 'p'): {25}, (24, 'I'): {25}, (24, 'J'): {25}, (24, 'O'): {25}, (24, '4'): {25}, (24, 'd'): {25}, (24, 'b'): {25}, (24, '1'): {25}, (24, 'q'): {25}, (24, '0'): {25}, (24, 'B'): {25}, (24, 'F'): {25}, (25, ''): {26}, (26, 'j'): {27}, (26, '8'): {27}, (26, 'Q'): {27}, (26, 'E'): {27}, (26, 'i'): {27}, (26, 'x'): {27}, (26, 'X'): {27}, (26, 'e'): {27}, (26, 'D'): {27}, (26, 'G'): {27}, (26, 'U'): {27}, (26, 'A'): {27}, (26, 'r'): {27}, (26, 'a'): {27}, (26, '3'): {27}, (26, 'N'): {27}, (26, 's'): {27}, (26, 'h'): {27}, (26, 'c'): {27}, (26, 'L'): {27}, (26, 'y'): {27}, (26, 'l'): {27}, (26, '9'): {27}, (26, 'u'): {27}, (26, 'o'): {27}, (26, 'f'): {27}, (26, 'R'): {27}, (26, 'Z'): {27}, (26, '5'): {27}, (26, '2'): {27}, (26, 'm'): {27}, (26, 'P'): {27}, (26, 'k'): {27}, (26, 'S'): {27}, (26, 'K'): {27}, (26, 't'): {27}, (26, 'W'): {27}, (26, 'g'): {27}, (26, '6'): {27}, (26, 'w'): {27}, (26, 'M'): {27}, (26, 'v'): {27}, (26, 'T'): {27}, (26, 'z'): {27}, (26, 'n'): {27}, (26, '7'): {27}, (26, 'V'): {27}, (26, 'C'): {27}, (26, 'H'): {27}, (26, 'Y'): {27}, (26, 'p'): {27}, (26, 'I'): {27}, (26, 'J'): {27}, (26, 'O'): {27}, (26, '4'): {27}, (26, 'd'): {27}, (26, 'b'): {27}, (26, '1'): {27}, (26, 'q'): {27}, (26, '0'): {27}, (26, 'B'): {27}, (26, 'F'): {27}, (27, ''): {28}, (28, 'j'): {29}, (28, '8'): {29}, (28, 'Q'): {29}, (28, 'E'): {29}, (28, 'i'): {29}, (28, 'x'): {29}, (28, 'X'): {29}, (28, 'e'): {29}, (28, 'D'): {29}, (28, 'G'): {29}, (28, 'U'): {29}, (28, 'A'): {29}, (28, 'r'): {29}, (28, 'a'): {29}, (28, '3'): {29}, (28, 'N'): {29}, (28, 's'): {29}, (28, 'h'): {29}, (28, 'c'): {29}, (28, 'L'): {29}, (28, 'y'): {29}, (28, 'l'): {29}, (28, '9'): {29}, (28, 'u'): {29}, (28, 'o'): {29}, (28, 'f'): {29}, (28, 'R'): {29}, (28, 'Z'): {29}, (28, '5'): {29}, (28, '2'): {29}, (28, 'm'): {29}, (28, 'P'): {29}, (28, 'k'): {29}, (28, 'S'): {29}, (28, 'K'): {29}, (28, 't'): {29}, (28, 'W'): {29}, (28, 'g'): {29}, (28, '6'): {29}, (28, 'w'): {29}, (28, 'M'): {29}, (28, 'v'): {29}, (28, 'T'): {29}, (28, 'z'): {29}, (28, 'n'): {29}, (28, '7'): {29}, (28, 'V'): {29}, (28, 'C'): {29}, (28, 'H'): {29}, (28, 'Y'): {29}, (28, 'p'): {29}, (28, 'I'): {29}, (28, 'J'): {29}, (28, 'O'): {29}, (28, '4'): {29}, (28, 'd'): {29}, (28, 'b'): {29}, (28, '1'): {29}, (28, 'q'): {29}, (28, '0'): {29}, (28, 'B'): {29}, (28, 'F'): {29}, (29, ''): {30}, (30, 'j'): {31}, (30, '8'): {31}, (30, 'Q'): {31}, (30, 'E'): {31}, (30, 'i'): {31}, (30, 'x'): {31}, (30, 'X'): {31}, (30, 'e'): {31}, (30, 'D'): {31}, (30, 'G'): {31}, (30, 'U'): {31}, (30, 'A'): {31}, (30, 'r'): {31}, (30, 'a'): {31}, (30, '3'): {31}, (30, 'N'): {31}, (30, 's'): {31}, (30, 'h'): {31}, (30, 'c'): {31}, (30, 'L'): {31}, (30, 'y'): {31}, (30, 'l'): {31}, (30, '9'): {31}, (30, 'u'): {31}, (30, 'o'): {31}, (30, 'f'): {31}, (30, 'R'): {31}, (30, 'Z'): {31}, (30, '5'): {31}, (30, '2'): {31}, (30, 'm'): {31}, (30, 'P'): {31}, (30, 'k'): {31}, (30, 'S'): {31}, (30, 'K'): {31}, (30, 't'): {31}, (30, 'W'): {31}, (30, 'g'): {31}, (30, '6'): {31}, (30, 'w'): {31}, (30, 'M'): {31}, (30, 'v'): {31}, (30, 'T'): {31}, (30, 'z'): {31}, (30, 'n'): {31}, (30, '7'): {31}, (30, 'V'): {31}, (30, 'C'): {31}, (30, 'H'): {31}, (30, 'Y'): {31}, (30, 'p'): {31}, (30, 'I'): {31}, (30, 'J'): {31}, (30, 'O'): {31}, (30, '4'): {31}, (30, 'd'): {31}, (30, 'b'): {31}, (30, '1'): {31}, (30, 'q'): {31}, (30, '0'): {31}, (30, 'B'): {31}, (30, 'F'): {31}, (31, ''): {32}, (32, 'j'): {33}, (32, '8'): {33}, (32, 'Q'): {33}, (32, 'E'): {33}, (32, 'i'): {33}, (32, 'x'): {33}, (32, 'X'): {33}, (32, 'e'): {33}, (32, 'D'): {33}, (32, 'G'): {33}, (32, 'U'): {33}, (32, 'A'): {33}, (32, 'r'): {33}, (32, 'a'): {33}, (32, '3'): {33}, (32, 'N'): {33}, (32, 's'): {33}, (32, 'h'): {33}, (32, 'c'): {33}, (32, 'L'): {33}, (32, 'y'): {33}, (32, 'l'): {33}, (32, '9'): {33}, (32, 'u'): {33}, (32, 'o'): {33}, (32, 'f'): {33}, (32, 'R'): {33}, (32, 'Z'): {33}, (32, '5'): {33}, (32, '2'): {33}, (32, 'm'): {33}, (32, 'P'): {33}, (32, 'k'): {33}, (32, 'S'): {33}, (32, 'K'): {33}, (32, 't'): {33}, (32, 'W'): {33}, (32, 'g'): {33}, (32, '6'): {33}, (32, 'w'): {33}, (32, 'M'): {33}, (32, 'v'): {33}, (32, 'T'): {33}, (32, 'z'): {33}, (32, 'n'): {33}, (32, '7'): {33}, (32, 'V'): {33}, (32, 'C'): {33}, (32, 'H'): {33}, (32, 'Y'): {33}, (32, 'p'): {33}, (32, 'I'): {33}, (32, 'J'): {33}, (32, 'O'): {33}, (32, '4'): {33}, (32, 'd'): {33}, (32, 'b'): {33}, (32, '1'): {33}, (32, 'q'): {33}, (32, '0'): {33}, (32, 'B'): {33}, (32, 'F'): {33}, (33, ''): {35}, (35, ''): {41}, (41, ''): {38, 39}, (38, ''): {42}, (39, '?'): {40}, (39, '!'): {40}, (39, ','): {40}, (39, '.'): {40}, (40, ''): {42}, (42, ''): {43}, (43, '\\r'): {44}, (43, ' '): {44}, (43, '\\x0c'): {44}, (43, '\\t'): {44}, (43, '\\n'): {44}, (43, '\\x0b'): {44}, (44, ''): {34, 46}, (45, ''): {34, 46}}\n",
      "defaultdict(<class 'set'>, {(0, ' '): {1}, (0, '\\t'): {1}, (0, '\\n'): {1}, (0, '\\x0c'): {1}, (0, '\\r'): {1}, (0, '\\x0b'): {1}, (1, '8'): {2}, (1, 'Q'): {2}, (1, 'E'): {2}, (1, 'e'): {2}, (1, 'D'): {2}, (1, 'U'): {2}, (1, 'A'): {2}, (1, 'a'): {2}, (1, 'N'): {2}, (1, 's'): {2}, (1, 'h'): {2}, (1, 'c'): {2}, (1, 'L'): {2}, (1, 'l'): {2}, (1, '9'): {2}, (1, 'u'): {2}, (1, 'o'): {2}, (1, 'm'): {2}, (1, 'P'): {2}, (1, 'K'): {2}, (1, 't'): {2}, (1, 'w'): {2}, (1, 'T'): {2}, (1, '7'): {2}, (1, 'V'): {2}, (1, 'C'): {2}, (1, 'H'): {2}, (1, 'I'): {2}, (1, 'J'): {2}, (1, 'd'): {2}, (1, 'b'): {2}, (1, '1'): {2}, (1, '0'): {2}, (1, 'j'): {2}, (1, 'F'): {2}, (1, 'i'): {2}, (1, 'x'): {2}, (1, 'X'): {2}, (1, 'G'): {2}, (1, '3'): {2}, (1, 'r'): {2}, (1, 'y'): {2}, (1, 'f'): {2}, (1, 'R'): {2}, (1, 'Z'): {2}, (1, '5'): {2}, (1, '2'): {2}, (1, 'k'): {2}, (1, 'S'): {2}, (1, 'W'): {2}, (1, 'g'): {2}, (1, '6'): {2}, (1, 'M'): {2}, (1, 'v'): {2}, (1, 'z'): {2}, (1, 'n'): {2}, (1, 'p'): {2}, (1, 'Y'): {2}, (1, 'O'): {2}, (1, '4'): {2}, (1, 'q'): {2}, (1, 'B'): {2}, (2, '8'): {3}, (2, 'Q'): {3}, (2, 'E'): {3}, (2, 'e'): {3}, (2, 'D'): {3}, (2, 'U'): {3}, (2, 'A'): {3}, (2, 'a'): {3}, (2, 'N'): {3}, (2, 's'): {3}, (2, 'h'): {3}, (2, 'c'): {3}, (2, '.'): {4}, (2, 'L'): {3}, (2, 'l'): {3}, (2, '9'): {3}, (2, 'u'): {3}, (2, '?'): {4}, (2, 'o'): {3}, (2, 'm'): {3}, (2, 'P'): {3}, (2, 'K'): {3}, (2, 't'): {3}, (2, 'w'): {3}, (2, 'T'): {3}, (2, ' '): {5}, (2, '7'): {3}, (2, 'V'): {3}, (2, '\\t'): {5}, (2, 'C'): {3}, (2, 'H'): {3}, (2, 'I'): {3}, (2, 'J'): {3}, (2, 'd'): {3}, (2, 'b'): {3}, (2, '1'): {3}, (2, '0'): {3}, (2, 'j'): {3}, (2, 'F'): {3}, (2, 'i'): {3}, (2, 'x'): {3}, (2, 'X'): {3}, (2, '!'): {4}, (2, 'G'): {3}, (2, '3'): {3}, (2, 'r'): {3}, (2, '\\n'): {5}, (2, '\\x0c'): {5}, (2, 'y'): {3}, (2, 'f'): {3}, (2, 'R'): {3}, (2, 'Z'): {3}, (2, '5'): {3}, (2, '2'): {3}, (2, 'k'): {3}, (2, 'S'): {3}, (2, 'W'): {3}, (2, ','): {4}, (2, 'g'): {3}, (2, '6'): {3}, (2, 'M'): {3}, (2, 'v'): {3}, (2, 'z'): {3}, (2, 'n'): {3}, (2, 'p'): {3}, (2, 'Y'): {3}, (2, '\\r'): {5}, (2, 'O'): {3}, (2, '4'): {3}, (2, 'q'): {3}, (2, 'B'): {3}, (2, '\\x0b'): {5}, (3, '8'): {6}, (3, 'Q'): {6}, (3, 'E'): {6}, (3, 'e'): {6}, (3, 'D'): {6}, (3, 'U'): {6}, (3, 'A'): {6}, (3, 'a'): {6}, (3, 'N'): {6}, (3, 's'): {6}, (3, 'h'): {6}, (3, 'c'): {6}, (3, '.'): {4}, (3, 'L'): {6}, (3, 'l'): {6}, (3, '9'): {6}, (3, 'u'): {6}, (3, '?'): {4}, (3, 'o'): {6}, (3, 'm'): {6}, (3, 'P'): {6}, (3, 'K'): {6}, (3, 't'): {6}, (3, 'w'): {6}, (3, 'T'): {6}, (3, ' '): {5}, (3, '7'): {6}, (3, 'V'): {6}, (3, '\\t'): {5}, (3, 'C'): {6}, (3, 'H'): {6}, (3, 'I'): {6}, (3, 'J'): {6}, (3, 'd'): {6}, (3, 'b'): {6}, (3, '1'): {6}, (3, '0'): {6}, (3, 'j'): {6}, (3, 'F'): {6}, (3, 'i'): {6}, (3, 'x'): {6}, (3, 'X'): {6}, (3, '!'): {4}, (3, 'G'): {6}, (3, '3'): {6}, (3, 'r'): {6}, (3, '\\n'): {5}, (3, '\\x0c'): {5}, (3, 'y'): {6}, (3, 'f'): {6}, (3, 'R'): {6}, (3, 'Z'): {6}, (3, '5'): {6}, (3, '2'): {6}, (3, 'k'): {6}, (3, 'S'): {6}, (3, 'W'): {6}, (3, ','): {4}, (3, 'g'): {6}, (3, '6'): {6}, (3, 'M'): {6}, (3, 'v'): {6}, (3, 'z'): {6}, (3, 'n'): {6}, (3, 'p'): {6}, (3, 'Y'): {6}, (3, '\\r'): {5}, (3, 'O'): {6}, (3, '4'): {6}, (3, 'q'): {6}, (3, 'B'): {6}, (3, '\\x0b'): {5}, (4, ' '): {5}, (4, '\\t'): {5}, (4, '\\n'): {5}, (4, '\\x0c'): {5}, (4, '\\r'): {5}, (4, '\\x0b'): {5}, (5, '8'): {2}, (5, 'Q'): {2}, (5, 'E'): {2}, (5, 'e'): {2}, (5, 'D'): {2}, (5, 'U'): {2}, (5, 'A'): {2}, (5, 'a'): {2}, (5, 'N'): {2}, (5, 's'): {2}, (5, 'h'): {2}, (5, 'c'): {2}, (5, 'L'): {2}, (5, 'l'): {2}, (5, '9'): {2}, (5, 'u'): {2}, (5, 'o'): {2}, (5, 'm'): {2}, (5, 'P'): {2}, (5, 'K'): {2}, (5, 't'): {2}, (5, 'w'): {2}, (5, 'T'): {2}, (5, '7'): {2}, (5, 'V'): {2}, (5, 'C'): {2}, (5, 'H'): {2}, (5, 'I'): {2}, (5, 'J'): {2}, (5, 'd'): {2}, (5, 'b'): {2}, (5, '1'): {2}, (5, '0'): {2}, (5, 'j'): {2}, (5, 'F'): {2}, (5, 'i'): {2}, (5, 'x'): {2}, (5, 'X'): {2}, (5, 'G'): {2}, (5, '3'): {2}, (5, 'r'): {2}, (5, 'y'): {2}, (5, 'f'): {2}, (5, 'R'): {2}, (5, 'Z'): {2}, (5, '5'): {2}, (5, '2'): {2}, (5, 'k'): {2}, (5, 'S'): {2}, (5, 'W'): {2}, (5, 'g'): {2}, (5, '6'): {2}, (5, 'M'): {2}, (5, 'v'): {2}, (5, 'z'): {2}, (5, 'n'): {2}, (5, 'p'): {2}, (5, 'Y'): {2}, (5, 'O'): {2}, (5, '4'): {2}, (5, 'q'): {2}, (5, 'B'): {2}, (6, '8'): {7}, (6, 'Q'): {7}, (6, 'E'): {7}, (6, 'e'): {7}, (6, 'D'): {7}, (6, 'U'): {7}, (6, 'A'): {7}, (6, 'a'): {7}, (6, 'N'): {7}, (6, 's'): {7}, (6, 'h'): {7}, (6, 'c'): {7}, (6, '.'): {4}, (6, 'L'): {7}, (6, 'l'): {7}, (6, '9'): {7}, (6, 'u'): {7}, (6, '?'): {4}, (6, 'o'): {7}, (6, 'm'): {7}, (6, 'P'): {7}, (6, 'K'): {7}, (6, 't'): {7}, (6, 'w'): {7}, (6, 'T'): {7}, (6, ' '): {5}, (6, '7'): {7}, (6, 'V'): {7}, (6, '\\t'): {5}, (6, 'C'): {7}, (6, 'H'): {7}, (6, 'I'): {7}, (6, 'J'): {7}, (6, 'd'): {7}, (6, 'b'): {7}, (6, '1'): {7}, (6, '0'): {7}, (6, 'j'): {7}, (6, 'F'): {7}, (6, 'i'): {7}, (6, 'x'): {7}, (6, 'X'): {7}, (6, '!'): {4}, (6, 'G'): {7}, (6, '3'): {7}, (6, 'r'): {7}, (6, '\\n'): {5}, (6, '\\x0c'): {5}, (6, 'y'): {7}, (6, 'f'): {7}, (6, 'R'): {7}, (6, 'Z'): {7}, (6, '5'): {7}, (6, '2'): {7}, (6, 'k'): {7}, (6, 'S'): {7}, (6, 'W'): {7}, (6, ','): {4}, (6, 'g'): {7}, (6, '6'): {7}, (6, 'M'): {7}, (6, 'v'): {7}, (6, 'z'): {7}, (6, 'n'): {7}, (6, 'p'): {7}, (6, 'Y'): {7}, (6, '\\r'): {5}, (6, 'O'): {7}, (6, '4'): {7}, (6, 'q'): {7}, (6, 'B'): {7}, (6, '\\x0b'): {5}, (7, '8'): {8}, (7, 'Q'): {8}, (7, 'E'): {8}, (7, 'e'): {8}, (7, 'D'): {8}, (7, 'U'): {8}, (7, 'A'): {8}, (7, 'a'): {8}, (7, 'N'): {8}, (7, 's'): {8}, (7, 'h'): {8}, (7, 'c'): {8}, (7, '.'): {4}, (7, 'L'): {8}, (7, 'l'): {8}, (7, '9'): {8}, (7, 'u'): {8}, (7, '?'): {4}, (7, 'o'): {8}, (7, 'm'): {8}, (7, 'P'): {8}, (7, 'K'): {8}, (7, 't'): {8}, (7, 'w'): {8}, (7, 'T'): {8}, (7, ' '): {5}, (7, '7'): {8}, (7, 'V'): {8}, (7, '\\t'): {5}, (7, 'C'): {8}, (7, 'H'): {8}, (7, 'I'): {8}, (7, 'J'): {8}, (7, 'd'): {8}, (7, 'b'): {8}, (7, '1'): {8}, (7, '0'): {8}, (7, 'j'): {8}, (7, 'F'): {8}, (7, 'i'): {8}, (7, 'x'): {8}, (7, 'X'): {8}, (7, '!'): {4}, (7, 'G'): {8}, (7, '3'): {8}, (7, 'r'): {8}, (7, '\\n'): {5}, (7, '\\x0c'): {5}, (7, 'y'): {8}, (7, 'f'): {8}, (7, 'R'): {8}, (7, 'Z'): {8}, (7, '5'): {8}, (7, '2'): {8}, (7, 'k'): {8}, (7, 'S'): {8}, (7, 'W'): {8}, (7, ','): {4}, (7, 'g'): {8}, (7, '6'): {8}, (7, 'M'): {8}, (7, 'v'): {8}, (7, 'z'): {8}, (7, 'n'): {8}, (7, 'p'): {8}, (7, 'Y'): {8}, (7, '\\r'): {5}, (7, 'O'): {8}, (7, '4'): {8}, (7, 'q'): {8}, (7, 'B'): {8}, (7, '\\x0b'): {5}, (8, '.'): {4}, (8, '?'): {4}, (8, ' '): {5}, (8, '\\t'): {5}, (8, '!'): {4}, (8, '\\n'): {5}, (8, '\\x0c'): {5}, (8, ','): {4}, (8, '\\r'): {5}, (8, '\\x0b'): {5}})\n"
     ]
    }
   ],
   "source": [
    "wordlen5 = LenWord(qwenllm, clm.FiniteStateMachine.from_regex(\"\\s([A-Za-z0-9]{1,5}[.!?,]?\\s)+\").to_dfa())\n",
    "wordlen5.create_hash_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows you to apply a constraint on the logits outputted by your LM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(logits, dim=-1)\n",
    "constrained_probs, _ = wordlen5.apply(input_ids=torch.tensor([]), probs=probs) # for the 1st generated token, set input_ids as an empty tensor or None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and sample accordingly to this constraint (the generated tokens don't exceed 5 letters)... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Prime', ' Snow', ' Find']\n",
      "['In July 1789 the French Prime', 'The best basketball player of all time is Michael Snow', 'Ludwig Wittgenstein was a<|endoftext|> Find']\n"
     ]
    }
   ],
   "source": [
    "next_const_token_ids = qwenllm.sample(constrained_probs)\n",
    "print(qwenllm.tokenizer.batch_decode(next_const_token_ids))\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, next_const_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or to sample whole sequences according to this constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "['In July 1789 the French and the other West Asian ports of the world were', 'The best basketball player of all time is Michael Redd.\\nHe was born in 19', 'Ludwig Wittgenstein was a<|endoftext|> Given the above text, we can infer that the']\n"
     ]
    }
   ],
   "source": [
    "cons_multinomial = clm.MultinomialSeqSampler(qwenllm, constraint=wordlen5)\n",
    "cons_generated_token_ids = cons_multinomial.sample(batch.input_ids, max_length=10, top_k=5)\n",
    "print(qwenllm.tokenizer.batch_decode(torch.cat([batch.input_ids, cons_generated_token_ids], dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESS always is >= 1, particles won't be resampled.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "wordlen5.current_state = wordlen5.fsm.start_state       # We need to re-initialize the current state after a generation\n",
    "smc_sampler_fsm = clm.SMCSampler(qwenllm, wordlen5)\n",
    "smc_generated_token_ids_fsm = smc_sampler_fsm.sample(batch.input_ids, max_length=10, num_particles=3, ess_threshold=1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In July 1789 the French were on the verge of rela Iraq of the', 'In July 1789 the French were fired upon by the Hugue Borde', 'In July 1789 the French clerk Mr. Ferry, a Spitz von', 'The best basketball player of all time is Michael J. Midk flex, and one of them', 'The best basketball player of all time is Michael Scott of the NBA.\\nA. Both B.', 'The best basketball player of all time is Michael Cox. Cox is the man who has made most', 'Ludwig Wittgenstein was a<|endoftext|> Parts of Pi. He had one of the great', 'Ludwig Wittgenstein was a<|endoftext|>\\tDream of a While\\nA. Lucky\\n', 'Ludwig Wittgenstein was a<|endoftext|> Let. He was quite uncon. Can you give']\n"
     ]
    }
   ],
   "source": [
    "print([a+b for a,b in zip(qwenllm.tokenizer.batch_decode(batch.input_ids.repeat_interleave(3, dim=0)), qwenllm.tokenizer.batch_decode(smc_generated_token_ids_fsm[0].reshape(3*3,10)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add validate() function ------------------- !!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Digits(clm.FSMConstraint):\n",
    "    def __init__(self, llm, fsm):\n",
    "        super().__init__(llm, fsm)\n",
    "\n",
    "    def prefix():\n",
    "        pass\n",
    "    def complete():\n",
    "        pass\n",
    "\n",
    "    def score(self, input_ids):\n",
    "        \"\"\"\n",
    "        Given the token IDs generated so far (input_ids), \n",
    "        return the score associated by the constraint.\n",
    "        \"\"\"\n",
    "        if input_ids is None or input_ids.numel() == 0:\n",
    "            return 1\n",
    "        # Flatten batch dimensions\n",
    "        batch_shape = input_ids.shape[:-1]\n",
    "        seq_length = input_ids.shape[-1]\n",
    "        flat_ids = input_ids.view(-1, seq_length)\n",
    "\n",
    "        texts = self.model.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        allowed = set(\"0123456789+-*/ \")\n",
    "\n",
    "        scores = []\n",
    "        for text in texts:\n",
    "            if all(c in allowed for c in text):\n",
    "                scores.append(1.0)\n",
    "            else:\n",
    "                scores.append(0.0)\n",
    "        scores = torch.tensor(scores, dtype=torch.float, device=input_ids.device)\n",
    "\n",
    "        return scores.view(batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(MAX_REPEAT, (1, MAXREPEAT, [(SUBPATTERN, (1, 0, 0, [(BRANCH, (None, [[(MAX_REPEAT, (1, MAXREPEAT, [(IN, [(CATEGORY, CATEGORY_DIGIT)])]))], [(IN, [(LITERAL, 32), (LITERAL, 43), (LITERAL, 45), (LITERAL, 42), (LITERAL, 47)])]]))]))]))]\n",
      "{(6, ''): {0, 4}, (0, '8'): {1}, (0, '3'): {1}, (0, '5'): {1}, (0, '7'): {1}, (0, '2'): {1}, (0, '4'): {1}, (0, '1'): {1}, (0, '6'): {1}, (0, '0'): {1}, (0, '9'): {1}, (1, ''): {0, 3}, (2, ''): {0, 3}, (3, ''): {7}, (4, '*'): {5}, (4, ' '): {5}, (4, '+'): {5}, (4, '-'): {5}, (4, '/'): {5}, (5, ''): {7}, (7, ''): {9, 6}, (8, ''): {9, 6}}\n",
      "defaultdict(<class 'set'>, {(0, '8'): {1}, (0, '3'): {1}, (0, '*'): {2}, (0, ' '): {2}, (0, '5'): {1}, (0, '7'): {1}, (0, '2'): {1}, (0, '+'): {2}, (0, '4'): {1}, (0, '-'): {2}, (0, '1'): {1}, (0, '6'): {1}, (0, '0'): {1}, (0, '/'): {2}, (0, '9'): {1}, (1, '8'): {1}, (1, '3'): {1}, (1, '*'): {2}, (1, ' '): {2}, (1, '5'): {1}, (1, '7'): {1}, (1, '2'): {1}, (1, '+'): {2}, (1, '4'): {1}, (1, '-'): {2}, (1, '1'): {1}, (1, '6'): {1}, (1, '0'): {1}, (1, '/'): {2}, (1, '9'): {1}, (2, '8'): {1}, (2, '3'): {1}, (2, '*'): {2}, (2, ' '): {2}, (2, '5'): {1}, (2, '7'): {1}, (2, '2'): {1}, (2, '+'): {2}, (2, '4'): {1}, (2, '-'): {2}, (2, '1'): {1}, (2, '6'): {1}, (2, '0'): {1}, (2, '/'): {2}, (2, '9'): {1}})\n"
     ]
    }
   ],
   "source": [
    "digits = Digits(qwenllm, clm.FiniteStateMachine.from_regex(\"(\\d+|[ +\\-*/])+\").to_dfa())\n",
    "digits.create_hash_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpnc = clm.RPNConstraint(qwenllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_rpn = [\n",
    "    \"Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\\n\\nExample 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: 7 - (2 + 3) * 4\\nOutput: 7 2 3 + 4 * -\\n\\nExample 3:\\nInput: (8 / 2) + (3 * (4 - 1))\\nOutput:\", \n",
    "    \"Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\\n\\nExample 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: 7 - (2 + 3) * 4\\nOutput: 7 2 3 + 4 * -\\n\\nExample 3:\\nInput: (3 + 4) * 5 − 6 / (1 + 2)\\nOutput:\", \n",
    "    \"Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\\n\\nExample 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: 7 - (2 + 3) * 4\\nOutput: 7 2 3 + 4 * -\\n\\nExample 3:\\nInput: (6 + 2) * 3 − 4\\nOutput:\", \n",
    "]\n",
    "batch_rpn = qwenllm.tokenizer(prompts_rpn, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1\n",
      "tensor([[21],\n",
      "        [17],\n",
      "        [16],\n",
      "        [18],\n",
      "        [18],\n",
      "        [18],\n",
      "        [19],\n",
      "        [16],\n",
      "        [16]])\n",
      "2\n",
      "tensor([[ 21, 220],\n",
      "        [ 17, 220],\n",
      "        [ 16, 220],\n",
      "        [ 18, 220],\n",
      "        [ 18, 220],\n",
      "        [ 18, 220],\n",
      "        [ 19,  21],\n",
      "        [ 16, 220],\n",
      "        [ 16, 220]])\n",
      "3\n",
      "tensor([[ 21, 220,  23],\n",
      "        [ 17, 220,  18],\n",
      "        [ 16, 220,  17],\n",
      "        [ 18, 220,  19],\n",
      "        [ 18, 220,  19],\n",
      "        [ 18, 220,  19],\n",
      "        [ 19,  21,  15],\n",
      "        [ 16, 220,  18],\n",
      "        [ 16, 220,  20]])\n",
      "4\n",
      "tensor([[ 21, 220,  23, 481],\n",
      "        [ 17, 220,  18, 220],\n",
      "        [ 16, 220,  17, 353],\n",
      "        [ 18, 220,  19, 488],\n",
      "        [ 18, 220,  19, 488],\n",
      "        [ 18, 220,  19, 488],\n",
      "        [ 19,  21,  15,  20],\n",
      "        [ 16, 220,  18, 220],\n",
      "        [ 16, 220,  20, 488]])\n",
      "5\n",
      "tensor([[ 21, 220,  23, 481, 220],\n",
      "        [ 17, 220,  18, 220,  19],\n",
      "        [ 16, 220,  17, 353, 220],\n",
      "        [ 18, 220,  19, 488, 220],\n",
      "        [ 18, 220,  19, 488, 220],\n",
      "        [ 18, 220,  19, 488, 220],\n",
      "        [ 19,  21,  15,  20,  16],\n",
      "        [ 16, 220,  18, 220,  17],\n",
      "        [ 16, 220,  20, 488, 220]])\n",
      "6\n",
      "tensor([[ 21, 220,  23, 481, 220,  17],\n",
      "        [ 17, 220,  18, 220,  19, 481],\n",
      "        [ 16, 220,  17, 353, 220,  18],\n",
      "        [ 18, 220,  19, 488, 220,  20],\n",
      "        [ 18, 220,  19, 488, 220,  20],\n",
      "        [ 18, 220,  19, 488, 220,  20],\n",
      "        [ 19,  21,  15,  20,  16,  16],\n",
      "        [ 16, 220,  18, 220,  17, 353],\n",
      "        [ 16, 220,  20, 488, 220,  21]])\n",
      "7\n",
      "tensor([[ 21, 220,  23, 481, 220,  17, 353],\n",
      "        [ 17, 220,  18, 220,  19, 481, 220],\n",
      "        [ 16, 220,  17, 353, 220,  18, 488],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353],\n",
      "        [ 19,  21,  15,  20,  16,  16,  22],\n",
      "        [ 16, 220,  18, 220,  17, 353, 488],\n",
      "        [ 16, 220,  20, 488, 220,  21, 353]])\n",
      "8\n",
      "tensor([[ 21, 220,  23, 481, 220,  17, 353, 220],\n",
      "        [ 17, 220,  18, 220,  19, 481, 220,  16],\n",
      "        [ 16, 220,  17, 353, 220,  18, 488, 220],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353, 220],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353, 220],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353, 220],\n",
      "        [ 19,  21,  15,  20,  16,  16,  22,  21],\n",
      "        [ 16, 220,  18, 220,  17, 353, 488, 220],\n",
      "        [ 16, 220,  20, 488, 220,  21, 353, 220]])\n",
      "9\n",
      "tensor([[ 21, 220,  23, 481, 220,  17, 353, 220,  18],\n",
      "        [ 17, 220,  18, 220,  19, 481, 220,  16, 488],\n",
      "        [ 16, 220,  17, 353, 220,  18, 488, 220,  16],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353, 220,  21],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353, 220,  21],\n",
      "        [ 18, 220,  19, 488, 220,  20, 353, 220,  21],\n",
      "        [ 19,  21,  15,  20,  16,  16,  22,  21,  22],\n",
      "        [ 16, 220,  18, 220,  17, 353, 488, 220,  19],\n",
      "        [ 16, 220,  20, 488, 220,  21, 353, 220,  18]])\n"
     ]
    }
   ],
   "source": [
    "smc_sampler_chain = clm.SMCSampler(qwenllm, digits, rpnc)\n",
    "smc_generated_token_ids_chain = smc_sampler_chain.sample(batch_rpn.input_ids, max_length=10, num_particles=3, ess_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (8 / 2) + (3 * (4 - 1))\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|>6 8 - 2 * 3 *\n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (8 / 2) + (3 * (4 - 1))\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|>2 3 4 - 1 + -\n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (8 / 2) + (3 * (4 - 1))\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|>1 2 * 3 + 1 \n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (3 + 4) * 5 − 6 / (1 + 2)\n",
      "Output:3 4 + 5 * 6 /\n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (3 + 4) * 5 − 6 / (1 + 2)\n",
      "Output:3 4 + 5 * 6 /\n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (3 + 4) * 5 − 6 / (1 + 2)\n",
      "Output:3 4 + 5 * 6 /\n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (6 + 2) * 3 − 4\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>4605117678\n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (6 + 2) * 3 − 4\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>1 3 2 * + 4 -\n",
      "------------------------\n",
      "Given an arithmetic expression in standard infix notation, it is possible to convert it to Reverse Polish Notation (RPN).\n",
      "\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: 7 - (2 + 3) * 4\n",
      "Output: 7 2 3 + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: (6 + 2) * 3 − 4\n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>1 5 + 6 * 3 -\n"
     ]
    }
   ],
   "source": [
    "for output in [a+b for a,b in zip(qwenllm.tokenizer.batch_decode(batch_rpn.input_ids.repeat_interleave(3, dim=0)), qwenllm.tokenizer.batch_decode(smc_generated_token_ids_chain[0].reshape(3*3,10)))]:\n",
    "    print(\"------------------------\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 \\\\\n",
    "Model from scratch \\\\\n",
    "Generateur aléatoire \\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot \\\\\n",
    "SFT \\\\\n",
    "\n",
    "\n",
    "(SFT might be enough for translation of RPN expressions, but if the task is: given a number int, find an RPN expression whose result is this number, we need RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not(e) -> len<20 : problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer proba des phrases générées pour LenWord5 avec et sans SMC (greedy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regarder temps exec smc w/ num_particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN : input: an int. We want the model to create a RPN expression that is equal to input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des C et des L : input: int, list(int). output: expression using list(int) equal to int."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b ? e : if b then e \\\n",
    "b: bool\n",
    "e: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN: \\d+-*/ T F et ou ?    ->    stack     -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0th step:\n",
    "\\d+-*/ =  (with fixed name of variable)  ->     stack (we need to modify the FSM)      ->     typing     -> (end of SMC)    eval\n",
    "\n",
    "\n",
    "\n",
    "then same, But I am not allowed to use a variable if it was not initialized before "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st step:\n",
    "\\d+-*/ =    ->     stack (we need to modify the FSM)      ->     typing     -> (end of SMC)    eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPN Typed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"(\\d+|[ +\\-\\*/]|[a-zA-Z][a-zA-Z0-9]*)+\" (lexical constraint)\n",
    "\n",
    " -------> \n",
    " \n",
    " number of digit/variables >= 2 (syntactic constraint),    \n",
    " type-checking in the stack (semantic constraint: type checking, declaration checking)    \n",
    " \n",
    " -------->     end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsVars(clm.FSMConstraint):\n",
    "    def __init__(self, llm, fsm):\n",
    "        super().__init__(llm, fsm)\n",
    "\n",
    "    def prefix():\n",
    "        pass\n",
    "    def complete():\n",
    "        pass\n",
    "\n",
    "    def score(self, input_ids):\n",
    "        \"\"\"\n",
    "        Given the token IDs generated so far (input_ids), \n",
    "        return the score associated by the constraint.\n",
    "        \"\"\"\n",
    "        if input_ids is None or input_ids.numel() == 0:\n",
    "            return 1\n",
    "        # Flatten batch dimensions\n",
    "        batch_shape = input_ids.shape[:-1]\n",
    "        seq_length = input_ids.shape[-1]\n",
    "        flat_ids = input_ids.view(-1, seq_length)\n",
    "\n",
    "        texts = self.model.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        allowed = set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789+-*/ \")\n",
    "\n",
    "        scores = []\n",
    "        for text in texts:\n",
    "            if all(c in allowed for c in text):\n",
    "                scores.append(1.0)\n",
    "            else:\n",
    "                scores.append(0.0)\n",
    "        scores = torch.tensor(scores, dtype=torch.float, device=input_ids.device)\n",
    "\n",
    "        return scores.view(batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(MAX_REPEAT, (1, MAXREPEAT, [(SUBPATTERN, (1, 0, 0, [(BRANCH, (None, [[(MAX_REPEAT, (1, MAXREPEAT, [(IN, [(CATEGORY, CATEGORY_DIGIT)])]))], [(IN, [(LITERAL, 32), (LITERAL, 43), (LITERAL, 45), (LITERAL, 42), (LITERAL, 47)])], [(IN, [(RANGE, (97, 122)), (RANGE, (65, 90))]), (MAX_REPEAT, (0, MAXREPEAT, [(IN, [(RANGE, (97, 122)), (RANGE, (65, 90)), (RANGE, (48, 57))])]))]]))]))]))]\n",
      "{(12, ''): {0, 4, 6}, (0, '8'): {1}, (0, '3'): {1}, (0, '5'): {1}, (0, '7'): {1}, (0, '2'): {1}, (0, '4'): {1}, (0, '1'): {1}, (0, '6'): {1}, (0, '0'): {1}, (0, '9'): {1}, (1, ''): {0, 3}, (2, ''): {0, 3}, (3, ''): {13}, (4, '*'): {5}, (4, ' '): {5}, (4, '+'): {5}, (4, '-'): {5}, (4, '/'): {5}, (5, ''): {13}, (6, 'Q'): {7}, (6, 'E'): {7}, (6, 'i'): {7}, (6, 'x'): {7}, (6, 'X'): {7}, (6, 'e'): {7}, (6, 'D'): {7}, (6, 'G'): {7}, (6, 'U'): {7}, (6, 'A'): {7}, (6, 'r'): {7}, (6, 'a'): {7}, (6, 'N'): {7}, (6, 's'): {7}, (6, 'h'): {7}, (6, 'c'): {7}, (6, 'L'): {7}, (6, 'y'): {7}, (6, 'l'): {7}, (6, 'u'): {7}, (6, 'o'): {7}, (6, 'f'): {7}, (6, 'R'): {7}, (6, 'Z'): {7}, (6, 'm'): {7}, (6, 'k'): {7}, (6, 'P'): {7}, (6, 'S'): {7}, (6, 't'): {7}, (6, 'K'): {7}, (6, 'W'): {7}, (6, 'g'): {7}, (6, 'w'): {7}, (6, 'B'): {7}, (6, 'M'): {7}, (6, 'v'): {7}, (6, 'T'): {7}, (6, 'z'): {7}, (6, 'n'): {7}, (6, 'V'): {7}, (6, 'p'): {7}, (6, 'C'): {7}, (6, 'H'): {7}, (6, 'Y'): {7}, (6, 'I'): {7}, (6, 'J'): {7}, (6, 'O'): {7}, (6, 'd'): {7}, (6, 'b'): {7}, (6, 'q'): {7}, (6, 'j'): {7}, (6, 'F'): {7}, (7, ''): {10}, (8, '8'): {9}, (8, 'Q'): {9}, (8, 'E'): {9}, (8, 'i'): {9}, (8, 'x'): {9}, (8, 'X'): {9}, (8, 'e'): {9}, (8, 'D'): {9}, (8, 'G'): {9}, (8, 'U'): {9}, (8, 'A'): {9}, (8, 'r'): {9}, (8, 'a'): {9}, (8, '3'): {9}, (8, 'N'): {9}, (8, 's'): {9}, (8, 'h'): {9}, (8, 'c'): {9}, (8, 'L'): {9}, (8, 'y'): {9}, (8, 'l'): {9}, (8, '9'): {9}, (8, 'u'): {9}, (8, 'o'): {9}, (8, 'f'): {9}, (8, 'R'): {9}, (8, 'Z'): {9}, (8, '5'): {9}, (8, '2'): {9}, (8, 'm'): {9}, (8, 'k'): {9}, (8, 'P'): {9}, (8, 'S'): {9}, (8, 't'): {9}, (8, 'K'): {9}, (8, 'W'): {9}, (8, 'g'): {9}, (8, '6'): {9}, (8, 'w'): {9}, (8, 'B'): {9}, (8, 'M'): {9}, (8, 'v'): {9}, (8, 'T'): {9}, (8, 'z'): {9}, (8, 'n'): {9}, (8, '7'): {9}, (8, 'V'): {9}, (8, 'p'): {9}, (8, 'C'): {9}, (8, 'H'): {9}, (8, 'Y'): {9}, (8, 'I'): {9}, (8, 'J'): {9}, (8, 'O'): {9}, (8, '4'): {9}, (8, 'd'): {9}, (8, 'b'): {9}, (8, '1'): {9}, (8, 'q'): {9}, (8, '0'): {9}, (8, 'j'): {9}, (8, 'F'): {9}, (10, ''): {8, 11}, (9, ''): {8, 11}, (11, ''): {13}, (13, ''): {12, 15}, (14, ''): {12, 15}}\n",
      "defaultdict(<class 'set'>, {(0, '8'): {1}, (0, 'Q'): {2}, (0, 'E'): {2}, (0, 'e'): {2}, (0, 'D'): {2}, (0, 'U'): {2}, (0, 'A'): {2}, (0, 'a'): {2}, (0, 'N'): {2}, (0, 's'): {2}, (0, 'h'): {2}, (0, 'c'): {2}, (0, 'L'): {2}, (0, 'l'): {2}, (0, '9'): {1}, (0, 'u'): {2}, (0, 'o'): {2}, (0, 'm'): {2}, (0, 'P'): {2}, (0, 't'): {2}, (0, 'K'): {2}, (0, 'w'): {2}, (0, 'T'): {2}, (0, ' '): {3}, (0, '7'): {1}, (0, 'V'): {2}, (0, 'C'): {2}, (0, 'H'): {2}, (0, 'I'): {2}, (0, 'J'): {2}, (0, 'd'): {2}, (0, '1'): {1}, (0, 'b'): {2}, (0, '0'): {1}, (0, '/'): {3}, (0, 'j'): {2}, (0, 'F'): {2}, (0, 'i'): {2}, (0, 'x'): {2}, (0, 'X'): {2}, (0, 'G'): {2}, (0, '3'): {1}, (0, 'r'): {2}, (0, 'y'): {2}, (0, '*'): {3}, (0, 'f'): {2}, (0, '5'): {1}, (0, 'R'): {2}, (0, 'Z'): {2}, (0, '2'): {1}, (0, 'k'): {2}, (0, 'S'): {2}, (0, '-'): {3}, (0, 'W'): {2}, (0, 'g'): {2}, (0, '6'): {1}, (0, 'M'): {2}, (0, 'v'): {2}, (0, 'z'): {2}, (0, 'n'): {2}, (0, '+'): {3}, (0, 'p'): {2}, (0, 'Y'): {2}, (0, 'O'): {2}, (0, '4'): {1}, (0, 'q'): {2}, (0, 'B'): {2}, (1, '8'): {1}, (1, 'Q'): {2}, (1, 'E'): {2}, (1, 'e'): {2}, (1, 'D'): {2}, (1, 'U'): {2}, (1, 'A'): {2}, (1, 'a'): {2}, (1, 'N'): {2}, (1, 's'): {2}, (1, 'h'): {2}, (1, 'c'): {2}, (1, 'L'): {2}, (1, 'l'): {2}, (1, '9'): {1}, (1, 'u'): {2}, (1, 'o'): {2}, (1, 'm'): {2}, (1, 'P'): {2}, (1, 't'): {2}, (1, 'K'): {2}, (1, 'w'): {2}, (1, 'T'): {2}, (1, ' '): {3}, (1, '7'): {1}, (1, 'V'): {2}, (1, 'C'): {2}, (1, 'H'): {2}, (1, 'I'): {2}, (1, 'J'): {2}, (1, 'd'): {2}, (1, '1'): {1}, (1, 'b'): {2}, (1, '0'): {1}, (1, '/'): {3}, (1, 'j'): {2}, (1, 'F'): {2}, (1, 'i'): {2}, (1, 'x'): {2}, (1, 'X'): {2}, (1, 'G'): {2}, (1, '3'): {1}, (1, 'r'): {2}, (1, 'y'): {2}, (1, '*'): {3}, (1, 'f'): {2}, (1, '5'): {1}, (1, 'R'): {2}, (1, 'Z'): {2}, (1, '2'): {1}, (1, 'k'): {2}, (1, 'S'): {2}, (1, '-'): {3}, (1, 'W'): {2}, (1, 'g'): {2}, (1, '6'): {1}, (1, 'M'): {2}, (1, 'v'): {2}, (1, 'z'): {2}, (1, 'n'): {2}, (1, '+'): {3}, (1, 'p'): {2}, (1, 'Y'): {2}, (1, 'O'): {2}, (1, '4'): {1}, (1, 'q'): {2}, (1, 'B'): {2}, (2, '8'): {4}, (2, 'Q'): {5}, (2, 'E'): {5}, (2, 'e'): {5}, (2, 'D'): {5}, (2, 'U'): {5}, (2, 'A'): {5}, (2, 'a'): {5}, (2, 'N'): {5}, (2, 's'): {5}, (2, 'h'): {5}, (2, 'c'): {5}, (2, 'L'): {5}, (2, 'l'): {5}, (2, '9'): {4}, (2, 'u'): {5}, (2, 'o'): {5}, (2, 'm'): {5}, (2, 'P'): {5}, (2, 't'): {5}, (2, 'K'): {5}, (2, 'w'): {5}, (2, 'T'): {5}, (2, ' '): {3}, (2, '7'): {4}, (2, 'V'): {5}, (2, 'C'): {5}, (2, 'H'): {5}, (2, 'I'): {5}, (2, 'J'): {5}, (2, 'd'): {5}, (2, '1'): {4}, (2, 'b'): {5}, (2, '0'): {4}, (2, '/'): {3}, (2, 'j'): {5}, (2, 'F'): {5}, (2, 'i'): {5}, (2, 'x'): {5}, (2, 'X'): {5}, (2, 'G'): {5}, (2, '3'): {4}, (2, 'r'): {5}, (2, 'y'): {5}, (2, '*'): {3}, (2, 'f'): {5}, (2, '5'): {4}, (2, 'R'): {5}, (2, 'Z'): {5}, (2, '2'): {4}, (2, 'k'): {5}, (2, 'S'): {5}, (2, '-'): {3}, (2, 'W'): {5}, (2, 'g'): {5}, (2, '6'): {4}, (2, 'M'): {5}, (2, 'v'): {5}, (2, 'z'): {5}, (2, 'n'): {5}, (2, '+'): {3}, (2, 'p'): {5}, (2, 'Y'): {5}, (2, 'O'): {5}, (2, '4'): {4}, (2, 'q'): {5}, (2, 'B'): {5}, (3, '8'): {1}, (3, 'Q'): {2}, (3, 'E'): {2}, (3, 'e'): {2}, (3, 'D'): {2}, (3, 'U'): {2}, (3, 'A'): {2}, (3, 'a'): {2}, (3, 'N'): {2}, (3, 's'): {2}, (3, 'h'): {2}, (3, 'c'): {2}, (3, 'L'): {2}, (3, 'l'): {2}, (3, '9'): {1}, (3, 'u'): {2}, (3, 'o'): {2}, (3, 'm'): {2}, (3, 'P'): {2}, (3, 't'): {2}, (3, 'K'): {2}, (3, 'w'): {2}, (3, 'T'): {2}, (3, ' '): {3}, (3, '7'): {1}, (3, 'V'): {2}, (3, 'C'): {2}, (3, 'H'): {2}, (3, 'I'): {2}, (3, 'J'): {2}, (3, 'd'): {2}, (3, '1'): {1}, (3, 'b'): {2}, (3, '0'): {1}, (3, '/'): {3}, (3, 'j'): {2}, (3, 'F'): {2}, (3, 'i'): {2}, (3, 'x'): {2}, (3, 'X'): {2}, (3, 'G'): {2}, (3, '3'): {1}, (3, 'r'): {2}, (3, 'y'): {2}, (3, '*'): {3}, (3, 'f'): {2}, (3, '5'): {1}, (3, 'R'): {2}, (3, 'Z'): {2}, (3, '2'): {1}, (3, 'k'): {2}, (3, 'S'): {2}, (3, '-'): {3}, (3, 'W'): {2}, (3, 'g'): {2}, (3, '6'): {1}, (3, 'M'): {2}, (3, 'v'): {2}, (3, 'z'): {2}, (3, 'n'): {2}, (3, '+'): {3}, (3, 'p'): {2}, (3, 'Y'): {2}, (3, 'O'): {2}, (3, '4'): {1}, (3, 'q'): {2}, (3, 'B'): {2}, (4, '8'): {4}, (4, 'Q'): {5}, (4, 'E'): {5}, (4, 'e'): {5}, (4, 'D'): {5}, (4, 'U'): {5}, (4, 'A'): {5}, (4, 'a'): {5}, (4, 'N'): {5}, (4, 's'): {5}, (4, 'h'): {5}, (4, 'c'): {5}, (4, 'L'): {5}, (4, 'l'): {5}, (4, '9'): {4}, (4, 'u'): {5}, (4, 'o'): {5}, (4, 'm'): {5}, (4, 'P'): {5}, (4, 't'): {5}, (4, 'K'): {5}, (4, 'w'): {5}, (4, 'T'): {5}, (4, ' '): {3}, (4, '7'): {4}, (4, 'V'): {5}, (4, 'C'): {5}, (4, 'H'): {5}, (4, 'I'): {5}, (4, 'J'): {5}, (4, 'd'): {5}, (4, '1'): {4}, (4, 'b'): {5}, (4, '0'): {4}, (4, '/'): {3}, (4, 'j'): {5}, (4, 'F'): {5}, (4, 'i'): {5}, (4, 'x'): {5}, (4, 'X'): {5}, (4, 'G'): {5}, (4, '3'): {4}, (4, 'r'): {5}, (4, 'y'): {5}, (4, '*'): {3}, (4, 'f'): {5}, (4, '5'): {4}, (4, 'R'): {5}, (4, 'Z'): {5}, (4, '2'): {4}, (4, 'k'): {5}, (4, 'S'): {5}, (4, '-'): {3}, (4, 'W'): {5}, (4, 'g'): {5}, (4, '6'): {4}, (4, 'M'): {5}, (4, 'v'): {5}, (4, 'z'): {5}, (4, 'n'): {5}, (4, '+'): {3}, (4, 'p'): {5}, (4, 'Y'): {5}, (4, 'O'): {5}, (4, '4'): {4}, (4, 'q'): {5}, (4, 'B'): {5}, (5, '8'): {4}, (5, 'Q'): {5}, (5, 'E'): {5}, (5, 'e'): {5}, (5, 'D'): {5}, (5, 'U'): {5}, (5, 'A'): {5}, (5, 'a'): {5}, (5, 'N'): {5}, (5, 's'): {5}, (5, 'h'): {5}, (5, 'c'): {5}, (5, 'L'): {5}, (5, 'l'): {5}, (5, '9'): {4}, (5, 'u'): {5}, (5, 'o'): {5}, (5, 'm'): {5}, (5, 'P'): {5}, (5, 't'): {5}, (5, 'K'): {5}, (5, 'w'): {5}, (5, 'T'): {5}, (5, ' '): {3}, (5, '7'): {4}, (5, 'V'): {5}, (5, 'C'): {5}, (5, 'H'): {5}, (5, 'I'): {5}, (5, 'J'): {5}, (5, 'd'): {5}, (5, '1'): {4}, (5, 'b'): {5}, (5, '0'): {4}, (5, '/'): {3}, (5, 'j'): {5}, (5, 'F'): {5}, (5, 'i'): {5}, (5, 'x'): {5}, (5, 'X'): {5}, (5, 'G'): {5}, (5, '3'): {4}, (5, 'r'): {5}, (5, 'y'): {5}, (5, '*'): {3}, (5, 'f'): {5}, (5, '5'): {4}, (5, 'R'): {5}, (5, 'Z'): {5}, (5, '2'): {4}, (5, 'k'): {5}, (5, 'S'): {5}, (5, '-'): {3}, (5, 'W'): {5}, (5, 'g'): {5}, (5, '6'): {4}, (5, 'M'): {5}, (5, 'v'): {5}, (5, 'z'): {5}, (5, 'n'): {5}, (5, '+'): {3}, (5, 'p'): {5}, (5, 'Y'): {5}, (5, 'O'): {5}, (5, '4'): {4}, (5, 'q'): {5}, (5, 'B'): {5}})\n"
     ]
    }
   ],
   "source": [
    "digitsvars = DigitsVars(qwenllm, clm.FiniteStateMachine.from_regex(\"(\\d+|[ +\\-*/]|[a-zA-Z][a-zA-Z0-9]*)+\").to_dfa())\n",
    "digitsvars.create_hash_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpntypedc = clm.RPNTypeConstraint(qwenllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_rpntyped = [\n",
    "    \"Example 1:\\nInput: foo = 4, (3 + foo) * 5\\nOutput: 3 foo 4 = + 5 *\\n\\nExample 2:\\nInput: bar = 3, 7 - (2 + bar) * 4\\nOutput: 7 2 bar 3 = + 4 * -\\n\\nExample 3:\\nInput: foofoo = 2, 8 + (foofoo * (4 - 1))\\nOutput:\", \n",
    "    \"Example 1:\\nInput: (3 + 4) * 5\\nOutput: 3 4 + 5 *\\n\\nExample 2:\\nInput: bar = 3,  7 - (2 + bar) * 4\\nOutput: 7 2 bar 3 = + 4 * -\\n\\nExample 3:\\nInput: foofoo = 5, (3 + 4) * foofoo − 6 \\nOutput:\", \n",
    "]\n",
    "batch_rpntyped = qwenllm.tokenizer(prompts_rpntyped, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1\n",
      "tensor([[  526],\n",
      "        [18084],\n",
      "        [53552],\n",
      "        [33975],\n",
      "        [33975],\n",
      "        [   32]])\n",
      "2\n",
      "tensor([[  526,   220],\n",
      "        [18084,   979],\n",
      "        [53552,  6679],\n",
      "        [33975, 17257],\n",
      "        [33975,  3749],\n",
      "        [   32,   425]])\n",
      "3\n",
      "tensor([[  526,   220,    16],\n",
      "        [18084,   979, 15229],\n",
      "        [53552,  6679,   279],\n",
      "        [33975, 17257,    57],\n",
      "        [33975,  3749, 50894],\n",
      "        [   32,   425,  6066]])\n",
      "4\n",
      "tensor([[  526,   220,    16,    15],\n",
      "        [18084,   979, 15229,  7975],\n",
      "        [53552,  6679,   279,  3403],\n",
      "        [33975, 17257,    57, 10981],\n",
      "        [33975,  3749, 50894, 33975],\n",
      "        [   32,   425,  6066,   356]])\n",
      "5\n",
      "tensor([[  526,   220,    16,    15,   220],\n",
      "        [18084,   979, 15229,  7975,   374],\n",
      "        [53552,  6679,   279,  3403,  1714],\n",
      "        [33975, 17257,    57, 10981,  6291],\n",
      "        [33975,  3749, 50894, 33975,  2239],\n",
      "        [   32,   425,  6066,   356,   638]])\n",
      "6\n",
      "tensor([[  526,   220,    16,    15,   220,    23],\n",
      "        [18084,   979, 15229,  7975,   374,   845],\n",
      "        [53552,  6679,   279,  3403,  1714,   323],\n",
      "        [33975, 17257,    57, 10981,  6291,   374],\n",
      "        [33975,  3749, 50894, 33975,  2239, 11551],\n",
      "        [   32,   425,  6066,   356,   638,   576]])\n",
      "7\n",
      "tensor([[  526,   220,    16,    15,   220,    23, 15229],\n",
      "        [18084,   979, 15229,  7975,   374,   845,   476],\n",
      "        [53552,  6679,   279,  3403,  1714,   323,  1410],\n",
      "        [33975, 17257,    57, 10981,  6291,   374, 10865],\n",
      "        [33975,  3749, 50894, 33975,  2239, 11551,  2624],\n",
      "        [   32,   425,  6066,   356,   638,   576,  1102]])\n",
      "8\n",
      "tensor([[  526,   220,    16,    15,   220,    23, 15229,  7975],\n",
      "        [18084,   979, 15229,  7975,   374,   845,   476,   220],\n",
      "        [53552,  6679,   279,  3403,  1714,   323,  1410,   537],\n",
      "        [33975, 17257,    57, 10981,  6291,   374, 10865,   304],\n",
      "        [33975,  3749, 50894, 33975,  2239, 11551,  2624,  2461],\n",
      "        [   32,   425,  6066,   356,   638,   576,  1102,   315]])\n",
      "9\n",
      "tensor([[  526,   220,    16,    15,   220,    23, 15229,  7975,  1308],\n",
      "        [18084,   979, 15229,  7975,   374,   845,   476,   220,    15],\n",
      "        [53552,  6679,   279,  3403,  1714,   323,  1410,   537,  8830],\n",
      "        [33975, 17257,    57, 10981,  6291,   374, 10865,   304,   279],\n",
      "        [33975,  3749, 50894, 33975,  2239, 11551,  2624,  2461, 94568],\n",
      "        [   32,   425,  6066,   356,   638,   576,  1102,   315,   419]])\n"
     ]
    }
   ],
   "source": [
    "smc_sampler_chain2 = clm.SMCSampler(qwenllm, digitsvars, rpntypedc)\n",
    "smc_generated_token_ids_chain2 = smc_sampler_chain2.sample(batch_rpntyped.input_ids, max_length=10, num_particles=3, ess_threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Example 1:\n",
      "Input: foo = 4, (3 + foo) * 5\n",
      "Output: 3 foo 4 = + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3, 7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 2, 8 + (foofoo * (4 - 1))\n",
      "Output: int 10 8 foofoo min \n",
      "------------------------\n",
      "Example 1:\n",
      "Input: foo = 4, (3 + foo) * 5\n",
      "Output: 3 foo 4 = + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3, 7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 2, 8 + (foofoo * (4 - 1))\n",
      "Output: Null when foofoo is null or 0 In\n",
      "------------------------\n",
      "Example 1:\n",
      "Input: foo = 4, (3 + foo) * 5\n",
      "Output: 3 foo 4 = + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3, 7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 2, 8 + (foofoo * (4 - 1))\n",
      "Output: Quit tried the above method and could not resolve the\n",
      "------------------------\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3,  7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 5, (3 + 4) * foofoo − 6 \n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>HumanlcZ proposed solution is broken in the following\n",
      "------------------------\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3,  7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 5, (3 + 4) * foofoo − 6 \n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Human interface IUserHuman extends IFooForHumansHuman\n",
      "------------------------\n",
      "Example 1:\n",
      "Input: (3 + 4) * 5\n",
      "Output: 3 4 + 5 *\n",
      "\n",
      "Example 2:\n",
      "Input: bar = 3,  7 - (2 + bar) * 4\n",
      "Output: 7 2 bar 3 = + 4 * -\n",
      "\n",
      "Example 3:\n",
      "Input: foofoo = 5, (3 + 4) * foofoo − 6 \n",
      "Output:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>A Bbb Ccc The result of this operation\n"
     ]
    }
   ],
   "source": [
    "for output in [a+b for a,b in zip(qwenllm.tokenizer.batch_decode(batch_rpntyped.input_ids.repeat_interleave(3, dim=0)), qwenllm.tokenizer.batch_decode(smc_generated_token_ids_chain2[0].reshape(2*3,10)))]:\n",
    "    print(\"------------------------\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: the expression \"The cat is on the mat 4 = = = = = = = \" is legal, (mat = 4, the = mat, on = the, is = on, cat = is, The = cat), thus \"The cat is on the mat\" is a legal unfinished RPN++ expression.\n",
    "\n",
    "Thus a model with a weak few-shot learning capacity will have a tendency to write sentences in natural language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imp, while, p11-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN : boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencer expériences sur: SFT / RL / Constraint Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st step : utiliser outlines avec SMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd step : faire expérience sur RPN où: \n",
    "- compute log-prob of generated sequences (faire augmenter le nombre de particules (from 1 to 20) and see the log-probs increase / see the accuracy for the task increase)\n",
    "- on compare avec SFT / RL / Constraint Decoding / Random generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd step : Imp avec outlines (scope checking, typing à intégrer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
